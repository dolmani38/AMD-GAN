{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Adaptive Multi-Discriminator GAN (AMD-GAN).ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a685ada720b644b280841f0590376e52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae1abdf3fffe46e8ae92b9229695e41a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bd14da4840cc4dc39428abd0761dd1d4",
              "IPY_MODEL_641368523a33461a9d04c916c7d5136a"
            ]
          }
        },
        "ae1abdf3fffe46e8ae92b9229695e41a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd14da4840cc4dc39428abd0761dd1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90755ae77e87447999b2d90ea3adcafe",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ccad5a8f8bc49f5b116ad82467e89b2"
          }
        },
        "641368523a33461a9d04c916c7d5136a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d81e495c3900474b86cffbdac341912d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:01&lt;00:00, 72.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0335acc1438948589dee555dfaf66820"
          }
        },
        "90755ae77e87447999b2d90ea3adcafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ccad5a8f8bc49f5b116ad82467e89b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d81e495c3900474b86cffbdac341912d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0335acc1438948589dee555dfaf66820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQCxNatSIOk"
      },
      "source": [
        "# Multi-Discriminator GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZjIW9VwyjDf"
      },
      "source": [
        "ABSTRACT\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c20I_OEErmP9"
      },
      "source": [
        "# 3. Related works\n",
        "\n",
        "두개 이상의 discriminator를 사용하는 GAN 연구에 대하여 알아본다.\n",
        "\n",
        "어떤 목적으로 복수의 discriminator를 사용하고 그 효과는 무엇인지 알아본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uFOIbzcFagq"
      },
      "source": [
        "## 1) N개의 discriminator를 활용한 연구 \n",
        "\n",
        "Generative adversarial networks (Goodfellow et al. (2014))\n",
        "\n",
        "Generator를 multi로 한 연구들...\n",
        "\n",
        "(1) Q. Hoang, T. Dinh Nguyen, T. Le, and D. Phung, “Multi-Generator Generative Adversarial Nets,” ArXiv e-prints, Aug. 2017.\n",
        "\n",
        "(2) Multi-Agent Diverse Generative Adversarial Networks\n",
        "\n",
        "Federated learning의 한 지류가 될수도 있을 것...\n",
        "\n",
        "(1) H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas, “Federated learning of deep networks using model averaging,” CoRR, vol. abs/1602.05629, 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa-0-i84rqoX"
      },
      "source": [
        "## 2) Automatic Image Colorization based on Multi-Discriminators Generative Adversarial Networks [품질향상]\n",
        "\n",
        "GAN은 흑백의 이미지를 입력하여 Color화 된 이미지를 생성해 낼 수 있다.\n",
        "본 논문은 두개의 discriminator를 이용하여 더 produces\n",
        "more realistic quality results.\n",
        "\n",
        "Different from conventional GAN network architecture,\n",
        "Park et al. [13] (S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “Srfeat: Single image super-resolution with feature discrimination,” in Proceedings of the European Conference on Computer Vision, 2018, pp. 439–455.) introduce architecture based on combination of one generator associated with two discriminators. For colorization task, we propose an extended model, illustrated in Fig.1, which uses two discriminators: <font color='red'><b>an image discriminator Di and a feature discriminator Df</b> </font>. The first one discriminates real images (RGB) from colorized images by inspecting their pixel values, while the second discriminates real images from colorized ones by inspecting their feature maps, noted respectively\n",
        "VGG(y) and VGG(G(x)) .\n",
        "\n",
        "본 논문의 Proposed Loss functions 중 GAN에 대한 Loss은 다음과 같다.\n",
        "\n",
        "$$ L_{M-dis}(G,D_i,D_f) = \\lambda_iL_{GAN}(G,D_i) + \\lambda_fL_{GAN}(G,D_f)  $$\n",
        "where lambda_i and lambda_f denote a defined weighting factors\n",
        "\n",
        "실험에 있어서도 lambda_i and lambda_f 의 값을 특정 값을 설정하여 실험 하였다.\n",
        "그 값은 최적의 값이 였을까??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfz7tKoDcZQ"
      },
      "source": [
        "## 3) UMLE: Unsupervised Multi-discriminator Network for Low Light Enhancement [품질 향상]\n",
        "\n",
        "Low-light image enhancement, such as recovering color and texture details from low-light images, is a complex and vital task. For automated driving, low-light scenarios will have serious implications for vision-based applications. To address this problem, we propose a real-time unsupervised generative adversarial network (GAN) containing  <font color='red'><b>multiple discriminators, i.e. a multi-scale discriminator, a texture discriminator, and a color discriminator.</b></font>\n",
        "\n",
        "본 논문에서 loss function 은 Adversarial loss + Cycle loss + Color loss + Preserving Loss + Reconstruction loss 로 구성된다.\n",
        "\n",
        "$$ L_{all} = \\omega_1L_{adv}+\\omega_2L_{cyc}+\\omega_3L_{color}+\\omega_4L_{pre}+\\omega_5L_{idt}$$ \n",
        "\n",
        "하지만 각각의 omega는 huristic하게 특정 지었다. 최적화된 값인가??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbt0ApbS8V_Q"
      },
      "source": [
        "## 4) GENERATIVE MULTI-ADVERSARIAL NETWORKS [품질 향상+mode collapse]\n",
        "\n",
        "N개의 복수 discriminator를 사용하여 안정적으로 더 빠르게 GAN 학습을 할 수 있다. 또한 mode collapse에도 robust 한 특성을 보인다.\n",
        "\n",
        "본 논문에서는 loss function을 three classical Pythagorean means 을 응용하여 정의하였다.\n",
        "\n",
        "$$ AM_{soft}(V, \\lambda) = \\sum_{i}^N \\omega_iV_i $$\n",
        "$$ GM_{soft}(V, \\lambda) = -exp(\\sum_{i}^N \\omega_ilog(-V_i)) $$\n",
        "$$ HM_{soft}(V, \\lambda) = (\\sum_{i}^N \\omega_iV_i^{-1})^{-1} $$\n",
        "\n",
        "하지만, 논문에서는 omega에 대하여 다루지 않았다.\n",
        "저 omega는 어떻게 최적화 할 수 있겠는가?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5qpgtDUySyC"
      },
      "source": [
        "## 5) Dual Discriminator Generative Adversarial Nets [mode collapse]\n",
        "\n",
        "GAN에서 발생하는 치명적인 mode collapse (https://developers.google.com/machine-learning/gan/problems) 현상을 개선하기 위해 두개의 discriminator를 사용한다. - dual discriminator generative adversarial network (D2GAN)\n",
        "\n",
        "it combines <font color='red'><b>the Kullback-Leibler (KL) and reverse KL divergences</b></font> into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes.\n",
        "\n",
        "본 논문에서 제안하는 D2GAN의 목적함수는 다음과 같다.\n",
        "\n",
        "$$ \\min_{G} \\max_{D_1,D_2} J (G,D_1,D_2) = \\alpha \\times E_{x \\sim P_{data}} [logD_1 (x)] + E_{z \\sim P_z} [-D_1 (G(z))] + E_{x \\sim P_{data}}[-D_2 (x)] + \\beta \\times E_{z \\sim P_z} [logD_2 (G(z))] $$\n",
        "\n",
        "여기서 alpha, beta는 hyperparameter로서, 본 논문의 실험에서는 다양한 값을 대입하여 각각의 성능을 확인하였다.\n",
        "\n",
        "이렇게 값을 찾아야만 하는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBQHO9hOBho-"
      },
      "source": [
        "## 6) MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets [성능 향상]\n",
        "\n",
        "we address the problem of distributing GANs so that they are able to train over datasets that are spread on multiple workers. MD-GAN is exposed as the first solution for this problem: we propose a novel learning procedure for GANs\n",
        "so that they fit this distributed setup. We then compare the performance of MD-GAN to an adapted version of Federated Learning to GANs, using the MNIST and CIFAR10 datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUM8QvQCWzi"
      },
      "source": [
        "## 7) ParallelWasserstein Generative Adversarial Nets with Multiple Discriminators [성능 향상]\n",
        "\n",
        "In this paper, we solve the computation cost problem by speeding up the Wasserstein GANs from a welldesigned communication efficient parallel architecture. 그리고 이것을 Multiple Discriminators 로 구성하였다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjztFvs812EB"
      },
      "source": [
        "# 4. Proposed methods\n",
        "\n",
        "ref : https://realpython.com/python-ai-neural-network/\n",
        "\n",
        "\n",
        "\n",
        "colab 수식입력 : \n",
        "\n",
        "https://wikidocs.net/1679\n",
        "\n",
        "https://en.wikipedia.org/wiki/Help:Displaying_a_formula#Formatting_using_TeX\n",
        "\n",
        "Original GAN의 목적함수\n",
        "$$ \\min_{G}\\max_{D} V(D,G) = E_{x\\sim p_{data}(x)}[logD(x)] + E_{z\\sim p_{z}(z)}[log(1-D(G(z)))] $$\n",
        "\n",
        "Multi-Discriminator GAN은 discriminator가 각 목적에 의하여 여러개 (N개) 있다.\n",
        "MDGAN의 목적함수\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N \\{E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))]\\} $$\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ L(D_i,G) =  E_{x\\sim p_{data}(x)}[logD_i(x)] + E_{z\\sim p_{z}(z)}[log(1-D_i(G(z)))] $$\n",
        "\n",
        "이라하고 단순화 하면\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) $$\n",
        "\n",
        "와 같이 된다.\n",
        "\n",
        "문제점은 GAN의 특성상, 특정 Discriminator가 학습에 있어 지배적으로 loss 함수에 영향을 미치게 되어 각각의 Discriminator가 골고루 학습에 참여하지 못하고 의도하지 않은 결과를 만들게 된다. 이러한 문제점을 극복하기 위해 다음의 두가지 제안을 한다.\n",
        "\n",
        "1) 목적함수에 각 Loss 에 대한 표준편차 (standard-deviation) 를 반영하여 각 Discriminator에 대한 Loss가 상호 유사한 수준을 유지하면 학습이 진행되도록 한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(D_{i\\sim N},G) = \\sum_{i=1}^N L(D_i,G) + \\sigma(L(D_i,G))$$\n",
        "\n",
        "2) 각 discriminator에 의한 loss를 제어하기 위해, adaptive discriminant factor (ADF) 를 적용하고, 학습의 진행 과정에서 이를 최적화 한다. \n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(\\lambda_{i\\sim N},D_{i\\sim N},G) = \\sum_{i=1}^N \\lambda_iL(D_i,G)$$\n",
        "\n",
        "3) 1)의 제안에 2)의 제안을 추가한다.\n",
        "\n",
        "$$ \\min_{G}\\max_{D_{i\\sim N}} V(\\lambda_{i\\sim N},D_{i\\sim N},G) = \\sum_{i=1}^N \\lambda_iL(D_i,G) + \\sigma(L(D_i,G))$$\n",
        "\n",
        "\n",
        "여기서\n",
        "\n",
        "$$ \\lambda_i = adaptive\\ discriminant \\ factor \\ for \\ discriminator \\ i  $$\n",
        "\n",
        "중요한 것은, 학습과정에서 L_i을 작게 (학습의 방향)하기 위해서는 lambda_i는 역으로 커져야 한다. 그래야, 전체 Loss function에서 비중이 증대되어 더 적극적인 학습이 이루어 지게 된다. 따라서, lambda_i의 최적화 방향은 기존의 gradient decent와 반대 방향이 되어야 한다.\n",
        "\n",
        "$$ \\lambda_i^{t+1} = \\lambda_i^t + \\gamma \\nabla V(\\lambda_{i\\sim N}^t,D_{i\\sim N},G)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87VNBbeRLFF"
      },
      "source": [
        "#5. Experiment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGovNR_uVJFB"
      },
      "source": [
        "실험은 CIFAR-10의 GAN 생성에서, N개의 Discriminator를 사용하며, 위의 3가지 제안을 각각 적용하여 Inception score 를 측정한다\n",
        "\n",
        "https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/\n",
        "\n",
        "https://github.com/hvy/chainer-inception-score\n",
        "\n",
        "https://colab.research.google.com/github/ssundar6087/vision-and-words/blob/master/_notebooks/2020-05-01-DCGAN-CIFAR10.ipynb\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJelZOUbelwt"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQhaYsGRCxhZ",
        "outputId": "a2dc938c-6683-41c2-e4f4-3970ac742fd7"
      },
      "source": [
        "\n",
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlSzlVm_eqjZ",
        "outputId": "1c6b3b43-f41c-4193-e7d7-0097c8ceb978"
      },
      "source": [
        "#collapse-hide\n",
        "#Author: Sairam Sundaresan\n",
        "#Version: 1.0\n",
        "#Date May 1, 2020\n",
        "# Preliminaries\n",
        "# WandB – Install the W&B library\n",
        "%pip install wandb -q"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 15.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 54.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 53.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "PuCPVP4wetwa",
        "outputId": "5fc4b504-860b-440c-f089-57eab54bafb2"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import random # to set the python random seed\n",
        "%matplotlib inline\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "import torchvision.utils as vutils\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "# Ignore excessive warnings\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 42\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(project=\"dcgan\") # Change the project name based on your W & B account"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdolmani38\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.31<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">hopeful-glade-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/dolmani38/dcgan\" target=\"_blank\">https://wandb.ai/dolmani38/dcgan</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/dolmani38/dcgan/runs/2jz6kxga\" target=\"_blank\">https://wandb.ai/dolmani38/dcgan/runs/2jz6kxga</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210529_163833-2jz6kxga</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6ea5747350>"
            ],
            "text/html": [
              "<h1>Run(2jz6kxga)</h1><iframe src=\"https://wandb.ai/dolmani38/dcgan/runs/2jz6kxga\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88tiJ_fGezW5"
      },
      "source": [
        "## Parameters of Interest\n",
        "Note that the Pytorch tutorial [referenced below](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) is designed for the **Celebrity faces** dataset and produces `64 x 64` images. I've tweaked the network architecture to produce `32 x 32` images as corresponding to the **CIFAR-10** dataset. The parameters below reflect the same. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH2Syb9yewfR"
      },
      "source": [
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 32\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xLvwOGe3Me"
      },
      "source": [
        "## Model Definition\n",
        "Let's define a generator and discriminator first. Weight initialization is a key factor in being able to produce a decent GAN and as per the paper, the weights are drawn from a _normal_ distribution with `0` mean and a standard-deviation of `0.02`. Also note that unlike in the original pytorch tutorial, I've removed one layer from the generator (at the end) and from the discriminator (at the beginning) to accomodate the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48qcc0Mke4-C"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIdITkPLe78S"
      },
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS-V8MgMe8wu"
      },
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgG3zeQhe-5e"
      },
      "source": [
        "## Defining the Training Function\n",
        "The training function first trains the discriminator and then the generator as shown below. Note that by setting the real label value to `0.9` and the fake label value to `0.1`, I've applied label smoothing which has been shown to improve the results produced by the GAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYj9LfvufAuK"
      },
      "source": [
        "def train(args, gen, disc, device, dataloader, optimizerG, optimizerD, criterion, epoch, iters):\n",
        "  gen.train()\n",
        "  disc.train()\n",
        "  img_list = []\n",
        "  fixed_noise = torch.randn(64, config.nz, 1, 1, device=device)\n",
        "\n",
        "  # Establish convention for real and fake labels during training (with label smoothing)\n",
        "  real_label = 0.9\n",
        "  fake_label = 0.1\n",
        "  for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "      #*****\n",
        "      # Update Discriminator\n",
        "      #*****\n",
        "      ## Train with all-real batch\n",
        "      disc.zero_grad()\n",
        "      # Format batch\n",
        "      real_cpu = data[0].to(device)\n",
        "      b_size = real_cpu.size(0)\n",
        "      label = torch.full((b_size,), real_label, device=device)\n",
        "      # Forward pass real batch through D\n",
        "      output = disc(real_cpu).view(-1)\n",
        "      # Calculate loss on all-real batch\n",
        "      errD_real = criterion(output, label)\n",
        "      # Calculate gradients for D in backward pass\n",
        "      errD_real.backward()\n",
        "      D_x = output.mean().item()\n",
        "\n",
        "      ## Train with all-fake batch\n",
        "      # Generate batch of latent vectors\n",
        "      noise = torch.randn(b_size, config.nz, 1, 1, device=device)\n",
        "      # Generate fake image batch with G\n",
        "      fake = gen(noise)\n",
        "      label.fill_(fake_label)\n",
        "      # Classify all fake batch with D\n",
        "      output = disc(fake.detach()).view(-1)\n",
        "      # Calculate D's loss on the all-fake batch\n",
        "      errD_fake = criterion(output, label)\n",
        "      # Calculate the gradients for this batch\n",
        "      errD_fake.backward()\n",
        "      D_G_z1 = output.mean().item()\n",
        "      # Add the gradients from the all-real and all-fake batches\n",
        "      errD = errD_real + errD_fake\n",
        "      # Update D\n",
        "      optimizerD.step()\n",
        "\n",
        "      #*****\n",
        "      # Update Generator\n",
        "      #*****\n",
        "      gen.zero_grad()\n",
        "      label.fill_(real_label)  # fake labels are real for generator cost\n",
        "      # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "      output = disc(fake).view(-1)\n",
        "      # Calculate G's loss based on this output\n",
        "      errG = criterion(output, label)\n",
        "      # Calculate gradients for G\n",
        "      errG.backward()\n",
        "      D_G_z2 = output.mean().item()\n",
        "      # Update G\n",
        "      optimizerG.step()\n",
        "\n",
        "      # Output training stats\n",
        "      if i % 50 == 0:\n",
        "          print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                % (epoch, args.epochs, i, len(dataloader),\n",
        "                    errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "          wandb.log({\n",
        "              \"Gen Loss\": errG.item(),\n",
        "              \"Disc Loss\": errD.item()})\n",
        "\n",
        "      # Check how the generator is doing by saving G's output on fixed_noise\n",
        "      if (iters % 500 == 0) or ((epoch == args.epochs-1) and (i == len(dataloader)-1)):\n",
        "          with torch.no_grad():\n",
        "              fake = gen(fixed_noise).detach().cpu()\n",
        "          img_list.append(wandb.Image(vutils.make_grid(fake, padding=2, normalize=True)))\n",
        "          wandb.log({\n",
        "              \"Generated Images\": img_list})\n",
        "      iters += 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZAHrsFMfI2R"
      },
      "source": [
        "## Monitoring the Run\n",
        "Once we have all the pieces in place, all we need to do is train the model and watch it learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkUXUZebfCYo",
        "outputId": "be453e2d-b240-4fd7-d754-17385b35fe49"
      },
      "source": [
        "#hide-collapse\n",
        "wandb.watch_called = False \n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = batch_size \n",
        "config.epochs = num_epochs         \n",
        "config.lr = lr              \n",
        "config.beta1 = beta1\n",
        "config.nz = nz          \n",
        "config.no_cuda = False         \n",
        "config.seed = manualSeed # random seed (default: 42)\n",
        "config.log_interval = 10 # how many batches to wait before logging training status\n",
        "\n",
        "use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "random.seed(config.seed)       # python random seed\n",
        "torch.manual_seed(config.seed) # pytorch random seed\n",
        "np.random.seed(config.seed) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Scale(32),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size,\n",
        "                                            shuffle=True, num_workers=workers)\n",
        "\n",
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "do_train = True\n",
        "\n",
        "if do_train:\n",
        "\n",
        "    # Handle multi-gpu if desired\n",
        "    if (device.type == 'cuda') and (ngpu > 1):\n",
        "        netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "    # Apply the weights_init function to randomly initialize all weights\n",
        "    #  to mean=0, stdev=0.2.\n",
        "    netG.apply(weights_init)\n",
        "\n",
        "    # Create the Discriminator\n",
        "    netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "    # Handle multi-gpu if desired\n",
        "    if (device.type == 'cuda') and (ngpu > 1):\n",
        "        netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "    # Apply the weights_init function to randomly initialize all weights\n",
        "    #  to mean=0, stdev=0.2.\n",
        "    netD.apply(weights_init)\n",
        "\n",
        "    # Initialize BCELoss function\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Setup Adam optimizers for both G and D\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=config.lr, betas=(config.beta1, 0.999))\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=config.lr, betas=(config.beta1, 0.999))\n",
        "\n",
        "    # WandB – wandb.watch() automatically fetches all layer dimensions, gradients, model parameters and logs them automatically to your dashboard.\n",
        "    # Using log=\"all\" log histograms of parameter values in addition to gradients\n",
        "    wandb.watch(netG, log=\"all\")\n",
        "    wandb.watch(netD, log=\"all\")\n",
        "    iters = 0\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        train(config, netG, netD, device, trainloader, optimizerG, optimizerD, criterion, epoch, iters)\n",
        "        \n",
        "    # WandB – Save the model checkpoint. This automatically saves a file to the cloud and associates it with the current run.\n",
        "    torch.save(netG.state_dict(), \"/content/drive/MyDrive/AMD-GAN/CIFAR10_GAN_model/model.h5\")\n",
        "    #wandb.save('/content/drive/MyDrive/AMD-GAN/CIFAR10_GAN_model/model.h5')\n",
        "\n",
        "    gen = netG\n",
        "    gen.eval()\n",
        "else:\n",
        "    # Create the generator\n",
        "    gen = Generator(ngpu).to(device)\n",
        "    gen.load_state_dict(torch.load('/content/drive/MyDrive/AMD-GAN/CIFAR10_GAN_model/model.h5'))\n",
        "    gen.eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "[1/100][0/391]\tLoss_D: 1.8011\tLoss_G: 1.2655\tD(x): 0.3113\tD(G(z)): 0.3837 / 0.2868\n",
            "[1/100][50/391]\tLoss_D: 0.9177\tLoss_G: 4.3246\tD(x): 0.7232\tD(G(z)): 0.2691 / 0.0098\n",
            "[1/100][100/391]\tLoss_D: 0.6980\tLoss_G: 2.0994\tD(x): 0.8635\tD(G(z)): 0.0985 / 0.1032\n",
            "[1/100][150/391]\tLoss_D: 0.9581\tLoss_G: 2.3060\tD(x): 0.6782\tD(G(z)): 0.2138 / 0.0864\n",
            "[1/100][200/391]\tLoss_D: 0.9465\tLoss_G: 1.7895\tD(x): 0.6808\tD(G(z)): 0.2728 / 0.1521\n",
            "[1/100][250/391]\tLoss_D: 0.8297\tLoss_G: 2.4752\tD(x): 0.7061\tD(G(z)): 0.1625 / 0.0717\n",
            "[1/100][300/391]\tLoss_D: 0.8131\tLoss_G: 2.5545\tD(x): 0.8225\tD(G(z)): 0.2451 / 0.0636\n",
            "[1/100][350/391]\tLoss_D: 1.0724\tLoss_G: 1.3907\tD(x): 0.5900\tD(G(z)): 0.2899 / 0.2412\n",
            "[2/100][0/391]\tLoss_D: 1.3085\tLoss_G: 1.6221\tD(x): 0.6621\tD(G(z)): 0.5089 / 0.2035\n",
            "[2/100][50/391]\tLoss_D: 1.0224\tLoss_G: 1.5641\tD(x): 0.5870\tD(G(z)): 0.2385 / 0.1950\n",
            "[2/100][100/391]\tLoss_D: 1.3698\tLoss_G: 1.6569\tD(x): 0.4601\tD(G(z)): 0.3528 / 0.1786\n",
            "[2/100][150/391]\tLoss_D: 0.9231\tLoss_G: 1.6582\tD(x): 0.6672\tD(G(z)): 0.2373 / 0.1816\n",
            "[2/100][200/391]\tLoss_D: 1.5554\tLoss_G: 1.6853\tD(x): 0.3568\tD(G(z)): 0.0317 / 0.1789\n",
            "[2/100][250/391]\tLoss_D: 1.0014\tLoss_G: 2.5649\tD(x): 0.7113\tD(G(z)): 0.3470 / 0.0711\n",
            "[2/100][300/391]\tLoss_D: 0.7955\tLoss_G: 2.0864\tD(x): 0.7956\tD(G(z)): 0.2306 / 0.1116\n",
            "[2/100][350/391]\tLoss_D: 0.8541\tLoss_G: 1.8908\tD(x): 0.7069\tD(G(z)): 0.1919 / 0.1409\n",
            "[3/100][0/391]\tLoss_D: 0.8275\tLoss_G: 2.0123\tD(x): 0.7882\tD(G(z)): 0.2425 / 0.1300\n",
            "[3/100][50/391]\tLoss_D: 0.8505\tLoss_G: 2.3419\tD(x): 0.7909\tD(G(z)): 0.2842 / 0.0908\n",
            "[3/100][100/391]\tLoss_D: 0.8909\tLoss_G: 2.0730\tD(x): 0.7598\tD(G(z)): 0.2726 / 0.1200\n",
            "[3/100][150/391]\tLoss_D: 0.8979\tLoss_G: 2.1610\tD(x): 0.7742\tD(G(z)): 0.3108 / 0.1090\n",
            "[3/100][200/391]\tLoss_D: 1.0675\tLoss_G: 1.2670\tD(x): 0.5651\tD(G(z)): 0.2380 / 0.2847\n",
            "[3/100][250/391]\tLoss_D: 0.9541\tLoss_G: 1.4582\tD(x): 0.6650\tD(G(z)): 0.2472 / 0.2286\n",
            "[3/100][300/391]\tLoss_D: 1.0031\tLoss_G: 1.8166\tD(x): 0.6147\tD(G(z)): 0.2427 / 0.1576\n",
            "[3/100][350/391]\tLoss_D: 1.4292\tLoss_G: 1.3234\tD(x): 0.4821\tD(G(z)): 0.3927 / 0.2717\n",
            "[4/100][0/391]\tLoss_D: 1.0548\tLoss_G: 1.6513\tD(x): 0.7833\tD(G(z)): 0.4573 / 0.1784\n",
            "[4/100][50/391]\tLoss_D: 1.1021\tLoss_G: 1.1452\tD(x): 0.6105\tD(G(z)): 0.3560 / 0.3162\n",
            "[4/100][100/391]\tLoss_D: 0.9093\tLoss_G: 2.0265\tD(x): 0.7347\tD(G(z)): 0.2937 / 0.1226\n",
            "[4/100][150/391]\tLoss_D: 0.9019\tLoss_G: 1.5047\tD(x): 0.7097\tD(G(z)): 0.2745 / 0.2124\n",
            "[4/100][200/391]\tLoss_D: 1.1108\tLoss_G: 2.4617\tD(x): 0.8056\tD(G(z)): 0.4776 / 0.0792\n",
            "[4/100][250/391]\tLoss_D: 0.8815\tLoss_G: 1.8761\tD(x): 0.7731\tD(G(z)): 0.2741 / 0.1441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGiD4A051QWH"
      },
      "source": [
        "## Original CIFAR10의 Inception Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qoIjDUf38Sz",
        "outputId": "facf64e2-e1d3-4b93-a67f-56a098f1f610"
      },
      "source": [
        "!git clone https://github.com/sbarratt/inception-score-pytorch.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'inception-score-pytorch'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 43 (delta 0), reused 1 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "a685ada720b644b280841f0590376e52",
            "ae1abdf3fffe46e8ae92b9229695e41a",
            "bd14da4840cc4dc39428abd0761dd1d4",
            "641368523a33461a9d04c916c7d5136a",
            "90755ae77e87447999b2d90ea3adcafe",
            "7ccad5a8f8bc49f5b116ad82467e89b2",
            "d81e495c3900474b86cffbdac341912d",
            "0335acc1438948589dee555dfaf66820"
          ]
        },
        "id": "XXNVB1Lm3iYT",
        "outputId": "a316d1ad-ee7a-436d-8920-1913f3639915"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/inception-score-pytorch')\n",
        "from inception_score import inception_score\n",
        "\n",
        "x = np.rollaxis(trainset.data, 3, 1)  \n",
        "\n",
        "class IgnoreLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, orig):\n",
        "        self.orig = orig\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.orig[index][0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.orig)\n",
        "\n",
        "print(inception_score(IgnoreLabelDataset(trainset), cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a685ada720b644b280841f0590376e52",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/content/inception-score-pytorch/inception_score.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x).data.cpu().numpy()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(9.672782457317373, 0.14991608790468258)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed4NvlH_CFeA",
        "outputId": "1756412c-275b-474b-c033-fef07726cbea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "id = IgnoreLabelDataset(trainset)\n",
        "id[100].shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig0vtfTYRfPv"
      },
      "source": [
        "## 생성한 image의 inception score 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPC0MAIjQElY"
      },
      "source": [
        "\n",
        "eval_images = []\n",
        "for i in range(1000):\n",
        "    gen_noise = torch.randn(64, config.nz, 1, 1, device=device)\n",
        "    with torch.no_grad():\n",
        "        fake = gen(gen_noise).detach().cpu()\n",
        "    eval_images += fake\n",
        "\n",
        "print(len(eval_images))\n",
        "print(eval_images[0].shape)\n",
        "print(inception_score(eval_images, cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}