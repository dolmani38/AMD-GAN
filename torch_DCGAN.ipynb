{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOa8j35h1GIE4sGV8HIHuCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/AMD-GAN/blob/main/torch_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAjWEM2bxbj8"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yrkf_2fx9OY",
        "outputId": "fe5da1cb-5855-4c9a-9d0e-a07fec5eff5e"
      },
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "#set manual seed to a constant get a consistent output\n",
        "manualSeed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "#loading the dataset\n",
        "dataset = dset.CIFAR10(root=\"./data\", download=True, train=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  8691\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tltjWyJWyIn3"
      },
      "source": [
        "nc=3\n",
        "\n",
        "# number of gpu's available\n",
        "ngpu = 1\n",
        "# input noise dimension\n",
        "nz = 100\n",
        "# number of generator filters\n",
        "ngf = 64\n",
        "#number of discriminator filters\n",
        "ndf = 64\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPChsX8JyNks",
        "outputId": "1d525970-c381-40b1-8435-35cf69417eb2"
      },
      "source": [
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "            return output\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "#load weights to test the model\n",
        "#netG.load_state_dict(torch.load('weights/netG_epoch_24.pth'))\n",
        "print(netG)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8PJwrPIyVgk",
        "outputId": "dc774d3d-a004-4b28-f44f-528dd69070de"
      },
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)\n",
        "#load weights to test the model \n",
        "#netD.load_state_dict(torch.load('weights/netD_epoch_24.pth'))\n",
        "print(netD)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs5F_k5yyZM2"
      },
      "source": [
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(128, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "niter = 25\n",
        "g_loss = []\n",
        "d_loss = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9myBC-vfygnz",
        "outputId": "5e424580-68c0-4c41-b0a9-1419b21d617a"
      },
      "source": [
        "\n",
        "for epoch in range(niter):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        label = torch.full((batch_size,), real_label, dtype=torch.float32, device=device)\n",
        "\n",
        "        output = netD(real_cpu)\n",
        "\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # train with fake\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        output = netD(fake)        \n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "        \n",
        "        #save the output\n",
        "        if i % 100 == 0:\n",
        "            print('saving the output')\n",
        "            vutils.save_image(real_cpu,'output/real_samples.png',normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),'output/fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n",
        "    \n",
        "    # Check pointing for every epoch\n",
        "    torch.save(netG.state_dict(), 'weights/netG_epoch_%d.pth' % (epoch))\n",
        "    torch.save(netD.state_dict(), 'weights/netD_epoch_%d.pth' % (epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/25][0/391] Loss_D: 0.3578 Loss_G: 4.0066 D(x): 0.8605 D(G(z)): 0.1600 / 0.0276\n",
            "saving the output\n",
            "[0/25][1/391] Loss_D: 0.1990 Loss_G: 3.2531 D(x): 0.8976 D(G(z)): 0.0785 / 0.0554\n",
            "[0/25][2/391] Loss_D: 0.2745 Loss_G: 4.3603 D(x): 0.9392 D(G(z)): 0.1735 / 0.0190\n",
            "[0/25][3/391] Loss_D: 0.1335 Loss_G: 3.5473 D(x): 0.9084 D(G(z)): 0.0310 / 0.0481\n",
            "[0/25][4/391] Loss_D: 0.1448 Loss_G: 3.8649 D(x): 0.9690 D(G(z)): 0.1017 / 0.0316\n",
            "[0/25][5/391] Loss_D: 0.1396 Loss_G: 3.9506 D(x): 0.9187 D(G(z)): 0.0476 / 0.0308\n",
            "[0/25][6/391] Loss_D: 0.0312 Loss_G: 5.3697 D(x): 0.9763 D(G(z)): 0.0068 / 0.0077\n",
            "[0/25][7/391] Loss_D: 0.0498 Loss_G: 4.7516 D(x): 0.9585 D(G(z)): 0.0065 / 0.0145\n",
            "[0/25][8/391] Loss_D: 0.3189 Loss_G: 7.2514 D(x): 0.9971 D(G(z)): 0.2500 / 0.0011\n",
            "[0/25][9/391] Loss_D: 0.9920 Loss_G: 2.4271 D(x): 0.4325 D(G(z)): 0.0044 / 0.1274\n",
            "[0/25][10/391] Loss_D: 0.5600 Loss_G: 3.4354 D(x): 0.8903 D(G(z)): 0.3217 / 0.0417\n",
            "[0/25][11/391] Loss_D: 0.7862 Loss_G: 0.8391 D(x): 0.5589 D(G(z)): 0.0994 / 0.4797\n",
            "[0/25][12/391] Loss_D: 0.9689 Loss_G: 4.8840 D(x): 0.9819 D(G(z)): 0.5564 / 0.0112\n",
            "[0/25][13/391] Loss_D: 0.7090 Loss_G: 1.9502 D(x): 0.5578 D(G(z)): 0.0523 / 0.1890\n",
            "[0/25][14/391] Loss_D: 0.3153 Loss_G: 3.0686 D(x): 0.9439 D(G(z)): 0.2086 / 0.0625\n",
            "[0/25][15/391] Loss_D: 0.2603 Loss_G: 4.5715 D(x): 0.9679 D(G(z)): 0.1842 / 0.0158\n",
            "[0/25][16/391] Loss_D: 0.3170 Loss_G: 1.3780 D(x): 0.7768 D(G(z)): 0.0383 / 0.3358\n",
            "[0/25][17/391] Loss_D: 0.6322 Loss_G: 7.5852 D(x): 0.9916 D(G(z)): 0.4104 / 0.0008\n",
            "[0/25][18/391] Loss_D: 1.6728 Loss_G: 1.3831 D(x): 0.2467 D(G(z)): 0.0030 / 0.3210\n",
            "[0/25][19/391] Loss_D: 1.3269 Loss_G: 5.6095 D(x): 0.9711 D(G(z)): 0.6635 / 0.0056\n",
            "[0/25][20/391] Loss_D: 2.3118 Loss_G: 0.3226 D(x): 0.1364 D(G(z)): 0.0188 / 0.7528\n",
            "[0/25][21/391] Loss_D: 1.8715 Loss_G: 4.5836 D(x): 0.9502 D(G(z)): 0.7818 / 0.0201\n",
            "[0/25][22/391] Loss_D: 2.0406 Loss_G: 0.4654 D(x): 0.1854 D(G(z)): 0.0449 / 0.6686\n",
            "[0/25][23/391] Loss_D: 1.8751 Loss_G: 2.7906 D(x): 0.9329 D(G(z)): 0.7746 / 0.0894\n",
            "[0/25][24/391] Loss_D: 0.9449 Loss_G: 1.9712 D(x): 0.5258 D(G(z)): 0.1521 / 0.1818\n",
            "[0/25][25/391] Loss_D: 0.6952 Loss_G: 1.4713 D(x): 0.6875 D(G(z)): 0.2108 / 0.2940\n",
            "[0/25][26/391] Loss_D: 0.8912 Loss_G: 3.4139 D(x): 0.8780 D(G(z)): 0.4675 / 0.0503\n",
            "[0/25][27/391] Loss_D: 0.9341 Loss_G: 1.3221 D(x): 0.4913 D(G(z)): 0.0941 / 0.3311\n",
            "[0/25][28/391] Loss_D: 0.7865 Loss_G: 2.3726 D(x): 0.8620 D(G(z)): 0.4155 / 0.1310\n",
            "[0/25][29/391] Loss_D: 0.4856 Loss_G: 2.6825 D(x): 0.7955 D(G(z)): 0.1848 / 0.0988\n",
            "[0/25][30/391] Loss_D: 0.6858 Loss_G: 1.9336 D(x): 0.7342 D(G(z)): 0.2670 / 0.1795\n",
            "[0/25][31/391] Loss_D: 0.5481 Loss_G: 2.1348 D(x): 0.7658 D(G(z)): 0.2083 / 0.1517\n",
            "[0/25][32/391] Loss_D: 0.7362 Loss_G: 2.2928 D(x): 0.7478 D(G(z)): 0.3143 / 0.1342\n",
            "[0/25][33/391] Loss_D: 0.6607 Loss_G: 1.8547 D(x): 0.7136 D(G(z)): 0.2254 / 0.1911\n",
            "[0/25][34/391] Loss_D: 0.5643 Loss_G: 2.4913 D(x): 0.8241 D(G(z)): 0.2745 / 0.1086\n",
            "[0/25][35/391] Loss_D: 0.6360 Loss_G: 2.1491 D(x): 0.7354 D(G(z)): 0.2409 / 0.1453\n",
            "[0/25][36/391] Loss_D: 0.5527 Loss_G: 1.8931 D(x): 0.7404 D(G(z)): 0.1882 / 0.1811\n",
            "[0/25][37/391] Loss_D: 0.5801 Loss_G: 2.6685 D(x): 0.8347 D(G(z)): 0.2938 / 0.0976\n",
            "[0/25][38/391] Loss_D: 0.6138 Loss_G: 2.1582 D(x): 0.7309 D(G(z)): 0.2111 / 0.1516\n",
            "[0/25][39/391] Loss_D: 0.4494 Loss_G: 2.6082 D(x): 0.8462 D(G(z)): 0.2221 / 0.0984\n",
            "[0/25][40/391] Loss_D: 0.5069 Loss_G: 1.9397 D(x): 0.7607 D(G(z)): 0.1621 / 0.1824\n",
            "[0/25][41/391] Loss_D: 0.5756 Loss_G: 3.7483 D(x): 0.9045 D(G(z)): 0.3463 / 0.0311\n",
            "[0/25][42/391] Loss_D: 0.7346 Loss_G: 1.7592 D(x): 0.5545 D(G(z)): 0.0409 / 0.2240\n",
            "[0/25][43/391] Loss_D: 0.7871 Loss_G: 4.4658 D(x): 0.9591 D(G(z)): 0.4808 / 0.0166\n",
            "[0/25][44/391] Loss_D: 0.4025 Loss_G: 2.1470 D(x): 0.7287 D(G(z)): 0.0509 / 0.1568\n",
            "[0/25][45/391] Loss_D: 0.2563 Loss_G: 3.9067 D(x): 0.9540 D(G(z)): 0.1750 / 0.0297\n",
            "[0/25][46/391] Loss_D: 0.2068 Loss_G: 3.7367 D(x): 0.9158 D(G(z)): 0.0992 / 0.0360\n",
            "[0/25][47/391] Loss_D: 0.2091 Loss_G: 3.6159 D(x): 0.8915 D(G(z)): 0.0776 / 0.0421\n",
            "[0/25][48/391] Loss_D: 0.0860 Loss_G: 4.5837 D(x): 0.9743 D(G(z)): 0.0552 / 0.0174\n",
            "[0/25][49/391] Loss_D: 0.0520 Loss_G: 4.5578 D(x): 0.9804 D(G(z)): 0.0311 / 0.0160\n",
            "[0/25][50/391] Loss_D: 0.0414 Loss_G: 4.5793 D(x): 0.9826 D(G(z)): 0.0231 / 0.0160\n",
            "[0/25][51/391] Loss_D: 0.0548 Loss_G: 4.3498 D(x): 0.9817 D(G(z)): 0.0345 / 0.0205\n",
            "[0/25][52/391] Loss_D: 0.0766 Loss_G: 4.4778 D(x): 0.9746 D(G(z)): 0.0475 / 0.0182\n",
            "[0/25][53/391] Loss_D: 0.0496 Loss_G: 4.5031 D(x): 0.9761 D(G(z)): 0.0244 / 0.0164\n",
            "[0/25][54/391] Loss_D: 0.0989 Loss_G: 4.6404 D(x): 0.9767 D(G(z)): 0.0705 / 0.0141\n",
            "[0/25][55/391] Loss_D: 0.1986 Loss_G: 2.6785 D(x): 0.8769 D(G(z)): 0.0534 / 0.1016\n",
            "[0/25][56/391] Loss_D: 0.3130 Loss_G: 2.9497 D(x): 0.8870 D(G(z)): 0.1580 / 0.0712\n",
            "[0/25][57/391] Loss_D: 0.2048 Loss_G: 4.9865 D(x): 0.9622 D(G(z)): 0.1417 / 0.0105\n",
            "[0/25][58/391] Loss_D: 0.1265 Loss_G: 4.0121 D(x): 0.8981 D(G(z)): 0.0108 / 0.0277\n",
            "[0/25][59/391] Loss_D: 0.0637 Loss_G: 4.5306 D(x): 0.9477 D(G(z)): 0.0083 / 0.0199\n",
            "[0/25][60/391] Loss_D: 0.0260 Loss_G: 4.9655 D(x): 0.9830 D(G(z)): 0.0086 / 0.0125\n",
            "[0/25][61/391] Loss_D: 0.1480 Loss_G: 4.7711 D(x): 0.9910 D(G(z)): 0.1232 / 0.0131\n",
            "[0/25][62/391] Loss_D: 0.1590 Loss_G: 4.3206 D(x): 0.9365 D(G(z)): 0.0843 / 0.0193\n",
            "[0/25][63/391] Loss_D: 0.0465 Loss_G: 4.8249 D(x): 0.9730 D(G(z)): 0.0184 / 0.0125\n",
            "[0/25][64/391] Loss_D: 0.0707 Loss_G: 5.1059 D(x): 0.9369 D(G(z)): 0.0037 / 0.0115\n",
            "[0/25][65/391] Loss_D: 0.0117 Loss_G: 5.1582 D(x): 0.9956 D(G(z)): 0.0072 / 0.0096\n",
            "[0/25][66/391] Loss_D: 0.1414 Loss_G: 4.6030 D(x): 0.9752 D(G(z)): 0.1028 / 0.0163\n",
            "[0/25][67/391] Loss_D: 0.0472 Loss_G: 5.0481 D(x): 0.9807 D(G(z)): 0.0269 / 0.0105\n",
            "[0/25][68/391] Loss_D: 0.0721 Loss_G: 4.4638 D(x): 0.9450 D(G(z)): 0.0111 / 0.0180\n",
            "[0/25][69/391] Loss_D: 0.0323 Loss_G: 4.7367 D(x): 0.9791 D(G(z)): 0.0108 / 0.0139\n",
            "[0/25][70/391] Loss_D: 0.0399 Loss_G: 4.3769 D(x): 0.9930 D(G(z)): 0.0318 / 0.0185\n",
            "[0/25][71/391] Loss_D: 0.0234 Loss_G: 5.3736 D(x): 0.9855 D(G(z)): 0.0085 / 0.0074\n",
            "[0/25][72/391] Loss_D: 0.0605 Loss_G: 4.6334 D(x): 0.9797 D(G(z)): 0.0377 / 0.0165\n",
            "[0/25][73/391] Loss_D: 0.0427 Loss_G: 4.7503 D(x): 0.9762 D(G(z)): 0.0181 / 0.0130\n",
            "[0/25][74/391] Loss_D: 0.0111 Loss_G: 5.8413 D(x): 0.9932 D(G(z)): 0.0042 / 0.0042\n",
            "[0/25][75/391] Loss_D: 0.0533 Loss_G: 4.3303 D(x): 0.9720 D(G(z)): 0.0237 / 0.0182\n",
            "[0/25][76/391] Loss_D: 0.0167 Loss_G: 5.2431 D(x): 0.9929 D(G(z)): 0.0095 / 0.0086\n",
            "[0/25][77/391] Loss_D: 0.0201 Loss_G: 5.2152 D(x): 0.9909 D(G(z)): 0.0106 / 0.0089\n",
            "[0/25][78/391] Loss_D: 0.0205 Loss_G: 5.7097 D(x): 0.9844 D(G(z)): 0.0046 / 0.0058\n",
            "[0/25][79/391] Loss_D: 0.0460 Loss_G: 4.5176 D(x): 0.9929 D(G(z)): 0.0374 / 0.0172\n",
            "[0/25][80/391] Loss_D: 0.0341 Loss_G: 5.0029 D(x): 0.9793 D(G(z)): 0.0127 / 0.0106\n",
            "[0/25][81/391] Loss_D: 0.0262 Loss_G: 5.1516 D(x): 0.9837 D(G(z)): 0.0095 / 0.0092\n",
            "[0/25][82/391] Loss_D: 0.0186 Loss_G: 5.0393 D(x): 0.9941 D(G(z)): 0.0125 / 0.0097\n",
            "[0/25][83/391] Loss_D: 0.0186 Loss_G: 5.1190 D(x): 0.9933 D(G(z)): 0.0117 / 0.0095\n",
            "[0/25][84/391] Loss_D: 0.0828 Loss_G: 4.5900 D(x): 0.9789 D(G(z)): 0.0571 / 0.0152\n",
            "[0/25][85/391] Loss_D: 0.0212 Loss_G: 5.1658 D(x): 0.9937 D(G(z)): 0.0147 / 0.0085\n",
            "[0/25][86/391] Loss_D: 0.0532 Loss_G: 4.4050 D(x): 0.9660 D(G(z)): 0.0173 / 0.0172\n",
            "[0/25][87/391] Loss_D: 0.0320 Loss_G: 4.9534 D(x): 0.9806 D(G(z)): 0.0118 / 0.0111\n",
            "[0/25][88/391] Loss_D: 0.0667 Loss_G: 4.4639 D(x): 0.9799 D(G(z)): 0.0444 / 0.0163\n",
            "[0/25][89/391] Loss_D: 0.1082 Loss_G: 4.4083 D(x): 0.9094 D(G(z)): 0.0065 / 0.0195\n",
            "[0/25][90/391] Loss_D: 0.0360 Loss_G: 3.9449 D(x): 0.9891 D(G(z)): 0.0242 / 0.0275\n",
            "[0/25][91/391] Loss_D: 0.1810 Loss_G: 5.7322 D(x): 0.9962 D(G(z)): 0.1531 / 0.0046\n",
            "[0/25][92/391] Loss_D: 0.3345 Loss_G: 3.7769 D(x): 0.7543 D(G(z)): 0.0241 / 0.0382\n",
            "[0/25][93/391] Loss_D: 0.4397 Loss_G: 1.0693 D(x): 0.7013 D(G(z)): 0.0487 / 0.4011\n",
            "[0/25][94/391] Loss_D: 0.6434 Loss_G: 4.7047 D(x): 0.9726 D(G(z)): 0.4210 / 0.0122\n",
            "[0/25][95/391] Loss_D: 0.7550 Loss_G: 1.5886 D(x): 0.5291 D(G(z)): 0.0352 / 0.2609\n",
            "[0/25][96/391] Loss_D: 0.5372 Loss_G: 4.5474 D(x): 0.9870 D(G(z)): 0.3706 / 0.0156\n",
            "[0/25][97/391] Loss_D: 0.5637 Loss_G: 1.8399 D(x): 0.6532 D(G(z)): 0.0685 / 0.2105\n",
            "[0/25][98/391] Loss_D: 0.2554 Loss_G: 3.7526 D(x): 0.9628 D(G(z)): 0.1816 / 0.0337\n",
            "[0/25][99/391] Loss_D: 0.1715 Loss_G: 3.7773 D(x): 0.8970 D(G(z)): 0.0550 / 0.0348\n",
            "[0/25][100/391] Loss_D: 0.1084 Loss_G: 3.5792 D(x): 0.9521 D(G(z)): 0.0543 / 0.0439\n",
            "saving the output\n",
            "[0/25][101/391] Loss_D: 0.0872 Loss_G: 4.4628 D(x): 0.9809 D(G(z)): 0.0640 / 0.0162\n",
            "[0/25][102/391] Loss_D: 0.1215 Loss_G: 4.7061 D(x): 0.9642 D(G(z)): 0.0773 / 0.0140\n",
            "[0/25][103/391] Loss_D: 0.1027 Loss_G: 3.7107 D(x): 0.9228 D(G(z)): 0.0175 / 0.0370\n",
            "[0/25][104/391] Loss_D: 0.0640 Loss_G: 4.4841 D(x): 0.9944 D(G(z)): 0.0547 / 0.0176\n",
            "[0/25][105/391] Loss_D: 0.0941 Loss_G: 4.7627 D(x): 0.9709 D(G(z)): 0.0598 / 0.0135\n",
            "[0/25][106/391] Loss_D: 0.0552 Loss_G: 4.4899 D(x): 0.9698 D(G(z)): 0.0233 / 0.0187\n",
            "[0/25][107/391] Loss_D: 0.0174 Loss_G: 5.2691 D(x): 0.9931 D(G(z)): 0.0103 / 0.0082\n",
            "[0/25][108/391] Loss_D: 0.0685 Loss_G: 4.6273 D(x): 0.9834 D(G(z)): 0.0492 / 0.0141\n",
            "[0/25][109/391] Loss_D: 0.0353 Loss_G: 4.9854 D(x): 0.9771 D(G(z)): 0.0118 / 0.0115\n",
            "[0/25][110/391] Loss_D: 0.0394 Loss_G: 4.5651 D(x): 0.9757 D(G(z)): 0.0142 / 0.0174\n",
            "[0/25][111/391] Loss_D: 0.0108 Loss_G: 5.6830 D(x): 0.9941 D(G(z)): 0.0049 / 0.0053\n",
            "[0/25][112/391] Loss_D: 0.0582 Loss_G: 5.0757 D(x): 0.9967 D(G(z)): 0.0516 / 0.0095\n",
            "[0/25][113/391] Loss_D: 0.0423 Loss_G: 4.9355 D(x): 0.9692 D(G(z)): 0.0105 / 0.0108\n",
            "[0/25][114/391] Loss_D: 0.0189 Loss_G: 4.9254 D(x): 0.9946 D(G(z)): 0.0134 / 0.0109\n",
            "[0/25][115/391] Loss_D: 0.0370 Loss_G: 4.7903 D(x): 0.9769 D(G(z)): 0.0132 / 0.0130\n",
            "[0/25][116/391] Loss_D: 0.0370 Loss_G: 4.6260 D(x): 0.9805 D(G(z)): 0.0167 / 0.0146\n",
            "[0/25][117/391] Loss_D: 0.0165 Loss_G: 6.3013 D(x): 0.9860 D(G(z)): 0.0023 / 0.0031\n",
            "[0/25][118/391] Loss_D: 0.1995 Loss_G: 7.4251 D(x): 0.9964 D(G(z)): 0.1680 / 0.0008\n",
            "[0/25][119/391] Loss_D: 0.8659 Loss_G: 2.6396 D(x): 0.4741 D(G(z)): 0.0042 / 0.0970\n",
            "[0/25][120/391] Loss_D: 0.5488 Loss_G: 1.4409 D(x): 0.7832 D(G(z)): 0.2313 / 0.2763\n",
            "[0/25][121/391] Loss_D: 0.9000 Loss_G: 2.0188 D(x): 0.6758 D(G(z)): 0.3433 / 0.1713\n",
            "[0/25][122/391] Loss_D: 0.7001 Loss_G: 3.8417 D(x): 0.8170 D(G(z)): 0.3496 / 0.0295\n",
            "[0/25][123/391] Loss_D: 1.4169 Loss_G: 0.1688 D(x): 0.3306 D(G(z)): 0.1366 / 0.8536\n",
            "[0/25][124/391] Loss_D: 2.2103 Loss_G: 6.3302 D(x): 0.9848 D(G(z)): 0.8407 / 0.0037\n",
            "[0/25][125/391] Loss_D: 1.3498 Loss_G: 1.9513 D(x): 0.3266 D(G(z)): 0.0153 / 0.2165\n",
            "[0/25][126/391] Loss_D: 0.5108 Loss_G: 2.0884 D(x): 0.8610 D(G(z)): 0.2565 / 0.1710\n",
            "[0/25][127/391] Loss_D: 0.6459 Loss_G: 4.8831 D(x): 0.9309 D(G(z)): 0.3773 / 0.0121\n",
            "[0/25][128/391] Loss_D: 1.1131 Loss_G: 1.1198 D(x): 0.4203 D(G(z)): 0.0449 / 0.3845\n",
            "[0/25][129/391] Loss_D: 1.1290 Loss_G: 4.8904 D(x): 0.9464 D(G(z)): 0.5944 / 0.0138\n",
            "[0/25][130/391] Loss_D: 0.7652 Loss_G: 2.3989 D(x): 0.5460 D(G(z)): 0.0409 / 0.1392\n",
            "[0/25][131/391] Loss_D: 0.5827 Loss_G: 2.7384 D(x): 0.8903 D(G(z)): 0.3353 / 0.0830\n",
            "[0/25][132/391] Loss_D: 0.3145 Loss_G: 3.7394 D(x): 0.8976 D(G(z)): 0.1615 / 0.0381\n",
            "[0/25][133/391] Loss_D: 0.6986 Loss_G: 1.6814 D(x): 0.5758 D(G(z)): 0.0577 / 0.2570\n",
            "[0/25][134/391] Loss_D: 1.1617 Loss_G: 5.4439 D(x): 0.9664 D(G(z)): 0.6212 / 0.0071\n",
            "[0/25][135/391] Loss_D: 1.1943 Loss_G: 1.6810 D(x): 0.3773 D(G(z)): 0.0320 / 0.2476\n",
            "[0/25][136/391] Loss_D: 0.7586 Loss_G: 3.3419 D(x): 0.8751 D(G(z)): 0.4144 / 0.0476\n",
            "[0/25][137/391] Loss_D: 0.7100 Loss_G: 1.9374 D(x): 0.6267 D(G(z)): 0.1654 / 0.1913\n",
            "[0/25][138/391] Loss_D: 1.0390 Loss_G: 6.0246 D(x): 0.9532 D(G(z)): 0.5669 / 0.0039\n",
            "[0/25][139/391] Loss_D: 1.6397 Loss_G: 1.0920 D(x): 0.2564 D(G(z)): 0.0169 / 0.4093\n",
            "[0/25][140/391] Loss_D: 0.6735 Loss_G: 3.0374 D(x): 0.9167 D(G(z)): 0.3903 / 0.0706\n",
            "[0/25][141/391] Loss_D: 0.5310 Loss_G: 3.6442 D(x): 0.8584 D(G(z)): 0.2853 / 0.0369\n",
            "[0/25][142/391] Loss_D: 0.8314 Loss_G: 1.5413 D(x): 0.5044 D(G(z)): 0.0513 / 0.2713\n",
            "[0/25][143/391] Loss_D: 0.9144 Loss_G: 5.6470 D(x): 0.9792 D(G(z)): 0.5308 / 0.0055\n",
            "[0/25][144/391] Loss_D: 0.5578 Loss_G: 1.7720 D(x): 0.6613 D(G(z)): 0.0767 / 0.2186\n",
            "[0/25][145/391] Loss_D: 0.5585 Loss_G: 3.6313 D(x): 0.8951 D(G(z)): 0.3065 / 0.0385\n",
            "[0/25][146/391] Loss_D: 0.6401 Loss_G: 1.9529 D(x): 0.6583 D(G(z)): 0.1416 / 0.1938\n",
            "[0/25][147/391] Loss_D: 0.5059 Loss_G: 2.9976 D(x): 0.8755 D(G(z)): 0.2827 / 0.0679\n",
            "[0/25][148/391] Loss_D: 0.3856 Loss_G: 3.6669 D(x): 0.8898 D(G(z)): 0.2130 / 0.0358\n",
            "[0/25][149/391] Loss_D: 0.4327 Loss_G: 2.1285 D(x): 0.7289 D(G(z)): 0.0724 / 0.1594\n",
            "[0/25][150/391] Loss_D: 0.4851 Loss_G: 2.7540 D(x): 0.8719 D(G(z)): 0.2652 / 0.0821\n",
            "[0/25][151/391] Loss_D: 0.4189 Loss_G: 3.2386 D(x): 0.8508 D(G(z)): 0.1957 / 0.0551\n",
            "[0/25][152/391] Loss_D: 0.2945 Loss_G: 3.1356 D(x): 0.8684 D(G(z)): 0.1258 / 0.0575\n",
            "[0/25][153/391] Loss_D: 0.4598 Loss_G: 1.8026 D(x): 0.7509 D(G(z)): 0.1306 / 0.2195\n",
            "[0/25][154/391] Loss_D: 0.3789 Loss_G: 4.6930 D(x): 0.9716 D(G(z)): 0.2580 / 0.0138\n",
            "[0/25][155/391] Loss_D: 0.3399 Loss_G: 2.4396 D(x): 0.7693 D(G(z)): 0.0480 / 0.1261\n",
            "[0/25][156/391] Loss_D: 0.2804 Loss_G: 5.4558 D(x): 0.9831 D(G(z)): 0.2134 / 0.0067\n",
            "[0/25][157/391] Loss_D: 0.5163 Loss_G: 1.9223 D(x): 0.6443 D(G(z)): 0.0168 / 0.1938\n",
            "[0/25][158/391] Loss_D: 0.6378 Loss_G: 4.8561 D(x): 0.9794 D(G(z)): 0.4052 / 0.0117\n",
            "[0/25][159/391] Loss_D: 0.8063 Loss_G: 2.3006 D(x): 0.5211 D(G(z)): 0.0238 / 0.1539\n",
            "[0/25][160/391] Loss_D: 0.3258 Loss_G: 2.5967 D(x): 0.9248 D(G(z)): 0.1957 / 0.1135\n",
            "[0/25][161/391] Loss_D: 0.3389 Loss_G: 3.6441 D(x): 0.9249 D(G(z)): 0.2030 / 0.0397\n",
            "[0/25][162/391] Loss_D: 0.3143 Loss_G: 3.3105 D(x): 0.8702 D(G(z)): 0.1435 / 0.0529\n",
            "[0/25][163/391] Loss_D: 0.3493 Loss_G: 2.4884 D(x): 0.7904 D(G(z)): 0.0798 / 0.1092\n",
            "[0/25][164/391] Loss_D: 0.3286 Loss_G: 2.5196 D(x): 0.8714 D(G(z)): 0.1507 / 0.1107\n",
            "[0/25][165/391] Loss_D: 0.4705 Loss_G: 4.9376 D(x): 0.9419 D(G(z)): 0.3002 / 0.0112\n",
            "[0/25][166/391] Loss_D: 0.6032 Loss_G: 1.4254 D(x): 0.5953 D(G(z)): 0.0198 / 0.3100\n",
            "[0/25][167/391] Loss_D: 0.8375 Loss_G: 5.7807 D(x): 0.9960 D(G(z)): 0.5088 / 0.0045\n",
            "[0/25][168/391] Loss_D: 0.9644 Loss_G: 1.8049 D(x): 0.4497 D(G(z)): 0.0134 / 0.2157\n",
            "[0/25][169/391] Loss_D: 0.5601 Loss_G: 2.8789 D(x): 0.9427 D(G(z)): 0.3467 / 0.0765\n",
            "[0/25][170/391] Loss_D: 0.4150 Loss_G: 3.3858 D(x): 0.8365 D(G(z)): 0.1738 / 0.0486\n",
            "[0/25][171/391] Loss_D: 0.5755 Loss_G: 1.6572 D(x): 0.6605 D(G(z)): 0.0935 / 0.2355\n",
            "[0/25][172/391] Loss_D: 0.5795 Loss_G: 4.7354 D(x): 0.9388 D(G(z)): 0.3617 / 0.0125\n",
            "[0/25][173/391] Loss_D: 0.2955 Loss_G: 3.0954 D(x): 0.7907 D(G(z)): 0.0325 / 0.0750\n",
            "[0/25][174/391] Loss_D: 0.1611 Loss_G: 3.4013 D(x): 0.9720 D(G(z)): 0.1190 / 0.0472\n",
            "[0/25][175/391] Loss_D: 0.2417 Loss_G: 4.4470 D(x): 0.9445 D(G(z)): 0.1515 / 0.0174\n",
            "[0/25][176/391] Loss_D: 0.3508 Loss_G: 1.9226 D(x): 0.7663 D(G(z)): 0.0503 / 0.1957\n",
            "[0/25][177/391] Loss_D: 0.3051 Loss_G: 5.5211 D(x): 0.9845 D(G(z)): 0.2258 / 0.0064\n",
            "[0/25][178/391] Loss_D: 0.1884 Loss_G: 4.8505 D(x): 0.8482 D(G(z)): 0.0076 / 0.0135\n",
            "[0/25][179/391] Loss_D: 0.0973 Loss_G: 3.4675 D(x): 0.9640 D(G(z)): 0.0564 / 0.0451\n",
            "[0/25][180/391] Loss_D: 0.1837 Loss_G: 4.3225 D(x): 0.9682 D(G(z)): 0.1330 / 0.0185\n",
            "[0/25][181/391] Loss_D: 0.2938 Loss_G: 2.5380 D(x): 0.8029 D(G(z)): 0.0494 / 0.1075\n",
            "[0/25][182/391] Loss_D: 0.3074 Loss_G: 4.5999 D(x): 0.9805 D(G(z)): 0.2257 / 0.0141\n",
            "[0/25][183/391] Loss_D: 0.4717 Loss_G: 1.9945 D(x): 0.6913 D(G(z)): 0.0334 / 0.1783\n",
            "[0/25][184/391] Loss_D: 0.4223 Loss_G: 6.1120 D(x): 0.9863 D(G(z)): 0.2955 / 0.0034\n",
            "[0/25][185/391] Loss_D: 0.7176 Loss_G: 1.7399 D(x): 0.5392 D(G(z)): 0.0195 / 0.2391\n",
            "[0/25][186/391] Loss_D: 0.4488 Loss_G: 4.7054 D(x): 0.9851 D(G(z)): 0.3215 / 0.0123\n",
            "[0/25][187/391] Loss_D: 0.7291 Loss_G: 0.9452 D(x): 0.5541 D(G(z)): 0.0375 / 0.4440\n",
            "[0/25][188/391] Loss_D: 1.0759 Loss_G: 5.9926 D(x): 0.9920 D(G(z)): 0.5962 / 0.0041\n",
            "[0/25][189/391] Loss_D: 1.2045 Loss_G: 1.2036 D(x): 0.3726 D(G(z)): 0.0224 / 0.3763\n",
            "[0/25][190/391] Loss_D: 0.7958 Loss_G: 5.3917 D(x): 0.9699 D(G(z)): 0.4785 / 0.0071\n",
            "[0/25][191/391] Loss_D: 0.5219 Loss_G: 2.5454 D(x): 0.6392 D(G(z)): 0.0306 / 0.1275\n",
            "[0/25][192/391] Loss_D: 0.4442 Loss_G: 3.5397 D(x): 0.9495 D(G(z)): 0.2897 / 0.0407\n",
            "[0/25][193/391] Loss_D: 0.1974 Loss_G: 3.7235 D(x): 0.8927 D(G(z)): 0.0707 / 0.0372\n",
            "[0/25][194/391] Loss_D: 0.1679 Loss_G: 3.8130 D(x): 0.9541 D(G(z)): 0.1084 / 0.0320\n",
            "[0/25][195/391] Loss_D: 0.2288 Loss_G: 3.7453 D(x): 0.9155 D(G(z)): 0.1193 / 0.0365\n",
            "[0/25][196/391] Loss_D: 0.2623 Loss_G: 3.2506 D(x): 0.8761 D(G(z)): 0.1099 / 0.0543\n",
            "[0/25][197/391] Loss_D: 0.1296 Loss_G: 3.6651 D(x): 0.9433 D(G(z)): 0.0645 / 0.0379\n",
            "[0/25][198/391] Loss_D: 0.1711 Loss_G: 4.6232 D(x): 0.9588 D(G(z)): 0.1101 / 0.0169\n",
            "[0/25][199/391] Loss_D: 0.2463 Loss_G: 2.8417 D(x): 0.8394 D(G(z)): 0.0523 / 0.0898\n",
            "[0/25][200/391] Loss_D: 0.2257 Loss_G: 3.6168 D(x): 0.9371 D(G(z)): 0.1377 / 0.0406\n",
            "saving the output\n",
            "[0/25][201/391] Loss_D: 0.1353 Loss_G: 3.6789 D(x): 0.9463 D(G(z)): 0.0727 / 0.0410\n",
            "[0/25][202/391] Loss_D: 0.1116 Loss_G: 3.8851 D(x): 0.9438 D(G(z)): 0.0489 / 0.0331\n",
            "[0/25][203/391] Loss_D: 0.0874 Loss_G: 4.0814 D(x): 0.9672 D(G(z)): 0.0507 / 0.0244\n",
            "[0/25][204/391] Loss_D: 0.0877 Loss_G: 4.1838 D(x): 0.9663 D(G(z)): 0.0497 / 0.0233\n",
            "[0/25][205/391] Loss_D: 0.0610 Loss_G: 4.3864 D(x): 0.9569 D(G(z)): 0.0157 / 0.0209\n",
            "[0/25][206/391] Loss_D: 0.1886 Loss_G: 5.3026 D(x): 0.9902 D(G(z)): 0.1541 / 0.0072\n",
            "[0/25][207/391] Loss_D: 0.3504 Loss_G: 3.5609 D(x): 0.7352 D(G(z)): 0.0088 / 0.0482\n",
            "[0/25][208/391] Loss_D: 0.4670 Loss_G: 1.1033 D(x): 0.7191 D(G(z)): 0.0906 / 0.3943\n",
            "[0/25][209/391] Loss_D: 0.8185 Loss_G: 5.8296 D(x): 0.9898 D(G(z)): 0.4901 / 0.0045\n",
            "[0/25][210/391] Loss_D: 0.6633 Loss_G: 2.8177 D(x): 0.5649 D(G(z)): 0.0130 / 0.0827\n",
            "[0/25][211/391] Loss_D: 0.3476 Loss_G: 1.9434 D(x): 0.8662 D(G(z)): 0.1650 / 0.1829\n",
            "[0/25][212/391] Loss_D: 0.7136 Loss_G: 4.9367 D(x): 0.8870 D(G(z)): 0.3948 / 0.0103\n",
            "[0/25][213/391] Loss_D: 0.4064 Loss_G: 2.7487 D(x): 0.7188 D(G(z)): 0.0373 / 0.0962\n",
            "[0/25][214/391] Loss_D: 0.2401 Loss_G: 2.2203 D(x): 0.8836 D(G(z)): 0.0994 / 0.1563\n",
            "[0/25][215/391] Loss_D: 0.3259 Loss_G: 5.4103 D(x): 0.9895 D(G(z)): 0.2436 / 0.0063\n",
            "[0/25][216/391] Loss_D: 0.4537 Loss_G: 2.0885 D(x): 0.6797 D(G(z)): 0.0151 / 0.1744\n",
            "[0/25][217/391] Loss_D: 0.4939 Loss_G: 4.8597 D(x): 0.9881 D(G(z)): 0.3465 / 0.0113\n",
            "[0/25][218/391] Loss_D: 0.6149 Loss_G: 2.1584 D(x): 0.5987 D(G(z)): 0.0451 / 0.1644\n",
            "[0/25][219/391] Loss_D: 0.3719 Loss_G: 3.3067 D(x): 0.9283 D(G(z)): 0.2310 / 0.0508\n",
            "[0/25][220/391] Loss_D: 0.2639 Loss_G: 3.5056 D(x): 0.8815 D(G(z)): 0.1145 / 0.0418\n",
            "[0/25][221/391] Loss_D: 0.1256 Loss_G: 4.0275 D(x): 0.9728 D(G(z)): 0.0892 / 0.0255\n",
            "[0/25][222/391] Loss_D: 0.1932 Loss_G: 4.1128 D(x): 0.9301 D(G(z)): 0.1015 / 0.0244\n",
            "[0/25][223/391] Loss_D: 0.0856 Loss_G: 4.2404 D(x): 0.9455 D(G(z)): 0.0274 / 0.0242\n",
            "[0/25][224/391] Loss_D: 0.0993 Loss_G: 3.7475 D(x): 0.9597 D(G(z)): 0.0537 / 0.0422\n",
            "[0/25][225/391] Loss_D: 0.0330 Loss_G: 4.7040 D(x): 0.9835 D(G(z)): 0.0160 / 0.0150\n",
            "[0/25][226/391] Loss_D: 0.0753 Loss_G: 4.4585 D(x): 0.9797 D(G(z)): 0.0509 / 0.0185\n",
            "[0/25][227/391] Loss_D: 0.0562 Loss_G: 4.5478 D(x): 0.9713 D(G(z)): 0.0260 / 0.0169\n",
            "[0/25][228/391] Loss_D: 0.1053 Loss_G: 4.3002 D(x): 0.9637 D(G(z)): 0.0622 / 0.0217\n",
            "[0/25][229/391] Loss_D: 0.0162 Loss_G: 5.5312 D(x): 0.9956 D(G(z)): 0.0116 / 0.0071\n",
            "[0/25][230/391] Loss_D: 0.0588 Loss_G: 4.3495 D(x): 0.9704 D(G(z)): 0.0275 / 0.0191\n",
            "[0/25][231/391] Loss_D: 0.1565 Loss_G: 3.0979 D(x): 0.8874 D(G(z)): 0.0304 / 0.0619\n",
            "[0/25][232/391] Loss_D: 0.1300 Loss_G: 4.9106 D(x): 0.9902 D(G(z)): 0.1089 / 0.0097\n",
            "[0/25][233/391] Loss_D: 0.0958 Loss_G: 4.4076 D(x): 0.9540 D(G(z)): 0.0452 / 0.0183\n",
            "[0/25][234/391] Loss_D: 0.1331 Loss_G: 3.2161 D(x): 0.9196 D(G(z)): 0.0445 / 0.0567\n",
            "[0/25][235/391] Loss_D: 0.1017 Loss_G: 3.5983 D(x): 0.9566 D(G(z)): 0.0528 / 0.0442\n",
            "[0/25][236/391] Loss_D: 0.0362 Loss_G: 4.4147 D(x): 0.9874 D(G(z)): 0.0229 / 0.0174\n",
            "[0/25][237/391] Loss_D: 0.0362 Loss_G: 4.7561 D(x): 0.9812 D(G(z)): 0.0166 / 0.0130\n",
            "[0/25][238/391] Loss_D: 0.0359 Loss_G: 4.4024 D(x): 0.9917 D(G(z)): 0.0268 / 0.0167\n",
            "[0/25][239/391] Loss_D: 0.0903 Loss_G: 4.9197 D(x): 0.9886 D(G(z)): 0.0724 / 0.0107\n",
            "[0/25][240/391] Loss_D: 0.0968 Loss_G: 3.8106 D(x): 0.9328 D(G(z)): 0.0244 / 0.0420\n",
            "[0/25][241/391] Loss_D: 0.1117 Loss_G: 3.9019 D(x): 0.9521 D(G(z)): 0.0566 / 0.0336\n",
            "[0/25][242/391] Loss_D: 0.0768 Loss_G: 4.5471 D(x): 0.9420 D(G(z)): 0.0139 / 0.0154\n",
            "[0/25][243/391] Loss_D: 0.0465 Loss_G: 4.2880 D(x): 0.9677 D(G(z)): 0.0126 / 0.0221\n",
            "[0/25][244/391] Loss_D: 0.0558 Loss_G: 4.1862 D(x): 0.9911 D(G(z)): 0.0448 / 0.0231\n",
            "[0/25][245/391] Loss_D: 0.1062 Loss_G: 5.4887 D(x): 0.9915 D(G(z)): 0.0878 / 0.0064\n",
            "[0/25][246/391] Loss_D: 0.0574 Loss_G: 4.8753 D(x): 0.9629 D(G(z)): 0.0185 / 0.0122\n",
            "[0/25][247/391] Loss_D: 0.0512 Loss_G: 5.2787 D(x): 0.9561 D(G(z)): 0.0055 / 0.0077\n",
            "[0/25][248/391] Loss_D: 0.0759 Loss_G: 3.8760 D(x): 0.9619 D(G(z)): 0.0349 / 0.0297\n",
            "[0/25][249/391] Loss_D: 0.0372 Loss_G: 4.4796 D(x): 0.9777 D(G(z)): 0.0141 / 0.0174\n",
            "[0/25][250/391] Loss_D: 0.0268 Loss_G: 4.7496 D(x): 0.9936 D(G(z)): 0.0199 / 0.0139\n",
            "[0/25][251/391] Loss_D: 0.0168 Loss_G: 5.7415 D(x): 0.9884 D(G(z)): 0.0049 / 0.0052\n",
            "[0/25][252/391] Loss_D: 0.0507 Loss_G: 4.3814 D(x): 0.9866 D(G(z)): 0.0358 / 0.0172\n",
            "[0/25][253/391] Loss_D: 0.0495 Loss_G: 4.7344 D(x): 0.9931 D(G(z)): 0.0401 / 0.0129\n",
            "[0/25][254/391] Loss_D: 0.0294 Loss_G: 5.2230 D(x): 0.9853 D(G(z)): 0.0142 / 0.0087\n",
            "[0/25][255/391] Loss_D: 0.0515 Loss_G: 4.8712 D(x): 0.9587 D(G(z)): 0.0078 / 0.0118\n",
            "[0/25][256/391] Loss_D: 0.0287 Loss_G: 4.5253 D(x): 0.9885 D(G(z)): 0.0168 / 0.0171\n",
            "[0/25][257/391] Loss_D: 0.0342 Loss_G: 5.3290 D(x): 0.9723 D(G(z)): 0.0057 / 0.0076\n",
            "[0/25][258/391] Loss_D: 0.1069 Loss_G: 4.8652 D(x): 0.9944 D(G(z)): 0.0918 / 0.0111\n",
            "[0/25][259/391] Loss_D: 0.1700 Loss_G: 3.6837 D(x): 0.8780 D(G(z)): 0.0314 / 0.0372\n",
            "[0/25][260/391] Loss_D: 0.2788 Loss_G: 1.5017 D(x): 0.8021 D(G(z)): 0.0396 / 0.2729\n",
            "[0/25][261/391] Loss_D: 0.4441 Loss_G: 5.4830 D(x): 0.9902 D(G(z)): 0.3262 / 0.0056\n",
            "[0/25][262/391] Loss_D: 0.8441 Loss_G: 1.1846 D(x): 0.4807 D(G(z)): 0.0186 / 0.3771\n",
            "[0/25][263/391] Loss_D: 0.6268 Loss_G: 6.4951 D(x): 0.9964 D(G(z)): 0.4264 / 0.0023\n",
            "[0/25][264/391] Loss_D: 1.2641 Loss_G: 0.6741 D(x): 0.3386 D(G(z)): 0.0066 / 0.5797\n",
            "[0/25][265/391] Loss_D: 1.1389 Loss_G: 5.3390 D(x): 0.9669 D(G(z)): 0.6108 / 0.0071\n",
            "[0/25][266/391] Loss_D: 1.5073 Loss_G: 0.7729 D(x): 0.2841 D(G(z)): 0.0265 / 0.5270\n",
            "[0/25][267/391] Loss_D: 1.1196 Loss_G: 3.9594 D(x): 0.9604 D(G(z)): 0.5828 / 0.0305\n",
            "[0/25][268/391] Loss_D: 0.7189 Loss_G: 1.9994 D(x): 0.6091 D(G(z)): 0.1105 / 0.1951\n",
            "[0/25][269/391] Loss_D: 0.6692 Loss_G: 2.9874 D(x): 0.8566 D(G(z)): 0.3508 / 0.0728\n",
            "[0/25][270/391] Loss_D: 0.5645 Loss_G: 1.8019 D(x): 0.6862 D(G(z)): 0.1176 / 0.2053\n",
            "[0/25][271/391] Loss_D: 0.6599 Loss_G: 3.1560 D(x): 0.8583 D(G(z)): 0.3464 / 0.0584\n",
            "[0/25][272/391] Loss_D: 0.4542 Loss_G: 3.0895 D(x): 0.8012 D(G(z)): 0.1696 / 0.0628\n",
            "[0/25][273/391] Loss_D: 0.3579 Loss_G: 2.5881 D(x): 0.8102 D(G(z)): 0.1160 / 0.0998\n",
            "[0/25][274/391] Loss_D: 0.5466 Loss_G: 3.9703 D(x): 0.8723 D(G(z)): 0.2939 / 0.0264\n",
            "[0/25][275/391] Loss_D: 0.4692 Loss_G: 1.7200 D(x): 0.6973 D(G(z)): 0.0502 / 0.2480\n",
            "[0/25][276/391] Loss_D: 0.6542 Loss_G: 5.6429 D(x): 0.9929 D(G(z)): 0.4226 / 0.0057\n",
            "[0/25][277/391] Loss_D: 0.7733 Loss_G: 2.2037 D(x): 0.5146 D(G(z)): 0.0235 / 0.1507\n",
            "[0/25][278/391] Loss_D: 0.4191 Loss_G: 2.2867 D(x): 0.8686 D(G(z)): 0.2189 / 0.1320\n",
            "[0/25][279/391] Loss_D: 0.6013 Loss_G: 3.4434 D(x): 0.8677 D(G(z)): 0.3287 / 0.0439\n",
            "[0/25][280/391] Loss_D: 0.4325 Loss_G: 2.7847 D(x): 0.7504 D(G(z)): 0.0938 / 0.0871\n",
            "[0/25][281/391] Loss_D: 0.3766 Loss_G: 3.1395 D(x): 0.8945 D(G(z)): 0.2006 / 0.0618\n",
            "[0/25][282/391] Loss_D: 0.2503 Loss_G: 3.5852 D(x): 0.8988 D(G(z)): 0.1188 / 0.0414\n",
            "[0/25][283/391] Loss_D: 0.3291 Loss_G: 2.4212 D(x): 0.7933 D(G(z)): 0.0733 / 0.1283\n",
            "[0/25][284/391] Loss_D: 0.4256 Loss_G: 2.6803 D(x): 0.8549 D(G(z)): 0.2055 / 0.0922\n",
            "[0/25][285/391] Loss_D: 0.2119 Loss_G: 4.1515 D(x): 0.9491 D(G(z)): 0.1334 / 0.0229\n",
            "[0/25][286/391] Loss_D: 0.2481 Loss_G: 3.0520 D(x): 0.8323 D(G(z)): 0.0486 / 0.0703\n",
            "[0/25][287/391] Loss_D: 0.1528 Loss_G: 3.9564 D(x): 0.9808 D(G(z)): 0.1182 / 0.0313\n",
            "[0/25][288/391] Loss_D: 0.1711 Loss_G: 3.9752 D(x): 0.9412 D(G(z)): 0.0995 / 0.0265\n",
            "[0/25][289/391] Loss_D: 0.1296 Loss_G: 3.5235 D(x): 0.9137 D(G(z)): 0.0346 / 0.0466\n",
            "[0/25][290/391] Loss_D: 0.1125 Loss_G: 4.7707 D(x): 0.9674 D(G(z)): 0.0732 / 0.0135\n",
            "[0/25][291/391] Loss_D: 0.1167 Loss_G: 4.1465 D(x): 0.9597 D(G(z)): 0.0687 / 0.0229\n",
            "[0/25][292/391] Loss_D: 0.1154 Loss_G: 3.3387 D(x): 0.9191 D(G(z)): 0.0250 / 0.0544\n",
            "[0/25][293/391] Loss_D: 0.1238 Loss_G: 4.1779 D(x): 0.9757 D(G(z)): 0.0912 / 0.0217\n",
            "[0/25][294/391] Loss_D: 0.0541 Loss_G: 4.9130 D(x): 0.9688 D(G(z)): 0.0214 / 0.0119\n",
            "[0/25][295/391] Loss_D: 0.0812 Loss_G: 4.1454 D(x): 0.9642 D(G(z)): 0.0420 / 0.0246\n",
            "[0/25][296/391] Loss_D: 0.0426 Loss_G: 4.7077 D(x): 0.9864 D(G(z)): 0.0280 / 0.0138\n",
            "[0/25][297/391] Loss_D: 0.1054 Loss_G: 3.9947 D(x): 0.9450 D(G(z)): 0.0455 / 0.0280\n",
            "[0/25][298/391] Loss_D: 0.0589 Loss_G: 4.4579 D(x): 0.9714 D(G(z)): 0.0283 / 0.0182\n",
            "[0/25][299/391] Loss_D: 0.1044 Loss_G: 4.6349 D(x): 0.9873 D(G(z)): 0.0827 / 0.0151\n",
            "[0/25][300/391] Loss_D: 0.0548 Loss_G: 5.2723 D(x): 0.9560 D(G(z)): 0.0081 / 0.0078\n",
            "saving the output\n",
            "[0/25][301/391] Loss_D: 0.0335 Loss_G: 4.7263 D(x): 0.9801 D(G(z)): 0.0131 / 0.0139\n",
            "[0/25][302/391] Loss_D: 0.0508 Loss_G: 4.4227 D(x): 0.9683 D(G(z)): 0.0175 / 0.0197\n",
            "[0/25][303/391] Loss_D: 0.0206 Loss_G: 4.7150 D(x): 0.9923 D(G(z)): 0.0127 / 0.0136\n",
            "[0/25][304/391] Loss_D: 0.0698 Loss_G: 4.7287 D(x): 0.9960 D(G(z)): 0.0622 / 0.0129\n",
            "[0/25][305/391] Loss_D: 0.0505 Loss_G: 4.6872 D(x): 0.9780 D(G(z)): 0.0272 / 0.0142\n",
            "[0/25][306/391] Loss_D: 0.0671 Loss_G: 4.2410 D(x): 0.9809 D(G(z)): 0.0454 / 0.0202\n",
            "[0/25][307/391] Loss_D: 0.0865 Loss_G: 4.1215 D(x): 0.9434 D(G(z)): 0.0249 / 0.0255\n",
            "[0/25][308/391] Loss_D: 0.0512 Loss_G: 4.0845 D(x): 0.9659 D(G(z)): 0.0156 / 0.0251\n",
            "[0/25][309/391] Loss_D: 0.0257 Loss_G: 4.6158 D(x): 0.9888 D(G(z)): 0.0142 / 0.0149\n",
            "[0/25][310/391] Loss_D: 0.0161 Loss_G: 5.4670 D(x): 0.9909 D(G(z)): 0.0068 / 0.0070\n",
            "[0/25][311/391] Loss_D: 0.1022 Loss_G: 4.4941 D(x): 0.9931 D(G(z)): 0.0877 / 0.0170\n",
            "[0/25][312/391] Loss_D: 0.0896 Loss_G: 4.1292 D(x): 0.9422 D(G(z)): 0.0280 / 0.0228\n",
            "[0/25][313/391] Loss_D: 0.0732 Loss_G: 4.1566 D(x): 0.9498 D(G(z)): 0.0199 / 0.0251\n",
            "[0/25][314/391] Loss_D: 0.0376 Loss_G: 4.7948 D(x): 0.9708 D(G(z)): 0.0075 / 0.0135\n",
            "[0/25][315/391] Loss_D: 0.0237 Loss_G: 4.9723 D(x): 0.9863 D(G(z)): 0.0097 / 0.0119\n",
            "[0/25][316/391] Loss_D: 0.1384 Loss_G: 4.5860 D(x): 0.9963 D(G(z)): 0.1192 / 0.0152\n",
            "[0/25][317/391] Loss_D: 0.0298 Loss_G: 5.6568 D(x): 0.9810 D(G(z)): 0.0104 / 0.0052\n",
            "[0/25][318/391] Loss_D: 0.0797 Loss_G: 4.1000 D(x): 0.9455 D(G(z)): 0.0218 / 0.0242\n",
            "[0/25][319/391] Loss_D: 0.1175 Loss_G: 3.2744 D(x): 0.9361 D(G(z)): 0.0466 / 0.0524\n",
            "[0/25][320/391] Loss_D: 0.0788 Loss_G: 4.2624 D(x): 0.9869 D(G(z)): 0.0612 / 0.0204\n",
            "[0/25][321/391] Loss_D: 0.0844 Loss_G: 4.2511 D(x): 0.9328 D(G(z)): 0.0119 / 0.0211\n",
            "[0/25][322/391] Loss_D: 0.0274 Loss_G: 4.5882 D(x): 0.9911 D(G(z)): 0.0180 / 0.0161\n",
            "[0/25][323/391] Loss_D: 0.0760 Loss_G: 4.0239 D(x): 0.9746 D(G(z)): 0.0476 / 0.0269\n",
            "[0/25][324/391] Loss_D: 0.0954 Loss_G: 4.5482 D(x): 0.9857 D(G(z)): 0.0757 / 0.0150\n",
            "[0/25][325/391] Loss_D: 0.0286 Loss_G: 5.2412 D(x): 0.9880 D(G(z)): 0.0161 / 0.0079\n",
            "[0/25][326/391] Loss_D: 0.0270 Loss_G: 5.7773 D(x): 0.9775 D(G(z)): 0.0039 / 0.0043\n",
            "[0/25][327/391] Loss_D: 0.0256 Loss_G: 5.8336 D(x): 0.9782 D(G(z)): 0.0033 / 0.0044\n",
            "[0/25][328/391] Loss_D: 0.0357 Loss_G: 6.8402 D(x): 0.9666 D(G(z)): 0.0009 / 0.0018\n",
            "[0/25][329/391] Loss_D: 0.1236 Loss_G: 4.8170 D(x): 0.9947 D(G(z)): 0.1046 / 0.0126\n",
            "[0/25][330/391] Loss_D: 0.0295 Loss_G: 7.3149 D(x): 0.9729 D(G(z)): 0.0015 / 0.0010\n",
            "[0/25][331/391] Loss_D: 0.0253 Loss_G: 6.0594 D(x): 0.9787 D(G(z)): 0.0033 / 0.0040\n",
            "[0/25][332/391] Loss_D: 0.0494 Loss_G: 4.1512 D(x): 0.9775 D(G(z)): 0.0258 / 0.0232\n",
            "[0/25][333/391] Loss_D: 0.0266 Loss_G: 4.9402 D(x): 0.9864 D(G(z)): 0.0125 / 0.0118\n",
            "[0/25][334/391] Loss_D: 0.0066 Loss_G: 6.6767 D(x): 0.9955 D(G(z)): 0.0020 / 0.0021\n",
            "[0/25][335/391] Loss_D: 0.0217 Loss_G: 5.1707 D(x): 0.9876 D(G(z)): 0.0090 / 0.0089\n",
            "[0/25][336/391] Loss_D: 0.0302 Loss_G: 4.6019 D(x): 0.9954 D(G(z)): 0.0249 / 0.0152\n",
            "[0/25][337/391] Loss_D: 0.0283 Loss_G: 4.8908 D(x): 0.9887 D(G(z)): 0.0166 / 0.0119\n",
            "[0/25][338/391] Loss_D: 0.0492 Loss_G: 4.3835 D(x): 0.9873 D(G(z)): 0.0354 / 0.0176\n",
            "[0/25][339/391] Loss_D: 0.0341 Loss_G: 4.7665 D(x): 0.9855 D(G(z)): 0.0191 / 0.0119\n",
            "[0/25][340/391] Loss_D: 0.0163 Loss_G: 5.8189 D(x): 0.9883 D(G(z)): 0.0044 / 0.0045\n",
            "[0/25][341/391] Loss_D: 0.0205 Loss_G: 5.1511 D(x): 0.9894 D(G(z)): 0.0097 / 0.0091\n",
            "[0/25][342/391] Loss_D: 0.0204 Loss_G: 4.7920 D(x): 0.9950 D(G(z)): 0.0151 / 0.0127\n",
            "[0/25][343/391] Loss_D: 0.0362 Loss_G: 4.5851 D(x): 0.9906 D(G(z)): 0.0259 / 0.0157\n",
            "[0/25][344/391] Loss_D: 0.0357 Loss_G: 5.3608 D(x): 0.9715 D(G(z)): 0.0061 / 0.0072\n",
            "[0/25][345/391] Loss_D: 0.0836 Loss_G: 4.3149 D(x): 0.9863 D(G(z)): 0.0645 / 0.0194\n",
            "[0/25][346/391] Loss_D: 0.0473 Loss_G: 5.5651 D(x): 0.9594 D(G(z)): 0.0050 / 0.0060\n",
            "[0/25][347/391] Loss_D: 0.0200 Loss_G: 5.0673 D(x): 0.9899 D(G(z)): 0.0097 / 0.0108\n",
            "[0/25][348/391] Loss_D: 0.0293 Loss_G: 4.6139 D(x): 0.9914 D(G(z)): 0.0200 / 0.0153\n",
            "[0/25][349/391] Loss_D: 0.0183 Loss_G: 5.3676 D(x): 0.9920 D(G(z)): 0.0102 / 0.0085\n",
            "[0/25][350/391] Loss_D: 0.0071 Loss_G: 7.2403 D(x): 0.9942 D(G(z)): 0.0012 / 0.0012\n",
            "[0/25][351/391] Loss_D: 0.0315 Loss_G: 4.4458 D(x): 0.9924 D(G(z)): 0.0233 / 0.0176\n",
            "[0/25][352/391] Loss_D: 0.0084 Loss_G: 6.8256 D(x): 0.9933 D(G(z)): 0.0016 / 0.0016\n",
            "[0/25][353/391] Loss_D: 0.0838 Loss_G: 4.2242 D(x): 0.9901 D(G(z)): 0.0689 / 0.0203\n",
            "[0/25][354/391] Loss_D: 0.0354 Loss_G: 4.9128 D(x): 0.9844 D(G(z)): 0.0192 / 0.0101\n",
            "[0/25][355/391] Loss_D: 0.0402 Loss_G: 5.0208 D(x): 0.9711 D(G(z)): 0.0105 / 0.0101\n",
            "[0/25][356/391] Loss_D: 0.0313 Loss_G: 4.7643 D(x): 0.9835 D(G(z)): 0.0143 / 0.0134\n",
            "[0/25][357/391] Loss_D: 0.0573 Loss_G: 4.0510 D(x): 0.9769 D(G(z)): 0.0328 / 0.0234\n",
            "[0/25][358/391] Loss_D: 0.0276 Loss_G: 4.9156 D(x): 0.9937 D(G(z)): 0.0208 / 0.0117\n",
            "[0/25][359/391] Loss_D: 0.0333 Loss_G: 4.8083 D(x): 0.9888 D(G(z)): 0.0216 / 0.0125\n",
            "[0/25][360/391] Loss_D: 0.0268 Loss_G: 5.1767 D(x): 0.9833 D(G(z)): 0.0097 / 0.0085\n",
            "[0/25][361/391] Loss_D: 0.0186 Loss_G: 6.1830 D(x): 0.9845 D(G(z)): 0.0027 / 0.0032\n",
            "[0/25][362/391] Loss_D: 0.0607 Loss_G: 4.5154 D(x): 0.9926 D(G(z)): 0.0511 / 0.0149\n",
            "[0/25][363/391] Loss_D: 0.0252 Loss_G: 6.5046 D(x): 0.9786 D(G(z)): 0.0031 / 0.0025\n",
            "[0/25][364/391] Loss_D: 0.0323 Loss_G: 5.5201 D(x): 0.9734 D(G(z)): 0.0048 / 0.0061\n",
            "[0/25][365/391] Loss_D: 0.1263 Loss_G: 5.1868 D(x): 0.9914 D(G(z)): 0.1064 / 0.0082\n",
            "[0/25][366/391] Loss_D: 0.0473 Loss_G: 7.3782 D(x): 0.9560 D(G(z)): 0.0011 / 0.0011\n",
            "[0/25][367/391] Loss_D: 0.1125 Loss_G: 5.3260 D(x): 0.9030 D(G(z)): 0.0035 / 0.0081\n",
            "[0/25][368/391] Loss_D: 0.3697 Loss_G: 1.5877 D(x): 0.7325 D(G(z)): 0.0228 / 0.2750\n",
            "[0/25][369/391] Loss_D: 0.6483 Loss_G: 8.3378 D(x): 0.9940 D(G(z)): 0.4186 / 0.0004\n",
            "[0/25][370/391] Loss_D: 3.5265 Loss_G: 0.2028 D(x): 0.0502 D(G(z)): 0.0023 / 0.8319\n",
            "[0/25][371/391] Loss_D: 2.1842 Loss_G: 6.7981 D(x): 0.9862 D(G(z)): 0.8214 / 0.0022\n",
            "[0/25][372/391] Loss_D: 1.7724 Loss_G: 1.3629 D(x): 0.2290 D(G(z)): 0.0096 / 0.3622\n",
            "[0/25][373/391] Loss_D: 0.8330 Loss_G: 3.9441 D(x): 0.9679 D(G(z)): 0.4736 / 0.0330\n",
            "[0/25][374/391] Loss_D: 0.4410 Loss_G: 3.0276 D(x): 0.7550 D(G(z)): 0.1112 / 0.0743\n",
            "[0/25][375/391] Loss_D: 0.2267 Loss_G: 3.0472 D(x): 0.8950 D(G(z)): 0.0978 / 0.0731\n",
            "[0/25][376/391] Loss_D: 0.3757 Loss_G: 3.9249 D(x): 0.9240 D(G(z)): 0.2315 / 0.0298\n",
            "[0/25][377/391] Loss_D: 0.4445 Loss_G: 2.4526 D(x): 0.7489 D(G(z)): 0.0789 / 0.1245\n",
            "[0/25][378/391] Loss_D: 0.4182 Loss_G: 3.9331 D(x): 0.9175 D(G(z)): 0.2465 / 0.0277\n",
            "[0/25][379/391] Loss_D: 0.5048 Loss_G: 1.7303 D(x): 0.6950 D(G(z)): 0.0762 / 0.2421\n",
            "[0/25][380/391] Loss_D: 0.6376 Loss_G: 5.6742 D(x): 0.9556 D(G(z)): 0.3976 / 0.0052\n",
            "[0/25][381/391] Loss_D: 0.5708 Loss_G: 2.3677 D(x): 0.6260 D(G(z)): 0.0257 / 0.1435\n",
            "[0/25][382/391] Loss_D: 0.3455 Loss_G: 3.5054 D(x): 0.9391 D(G(z)): 0.2188 / 0.0456\n",
            "[0/25][383/391] Loss_D: 0.4685 Loss_G: 4.6711 D(x): 0.8576 D(G(z)): 0.2368 / 0.0151\n",
            "[0/25][384/391] Loss_D: 0.1950 Loss_G: 3.1201 D(x): 0.8668 D(G(z)): 0.0396 / 0.0659\n",
            "[0/25][385/391] Loss_D: 0.1954 Loss_G: 5.9410 D(x): 0.9881 D(G(z)): 0.1544 / 0.0043\n",
            "[0/25][386/391] Loss_D: 0.2433 Loss_G: 2.6432 D(x): 0.8230 D(G(z)): 0.0274 / 0.1097\n",
            "[0/25][387/391] Loss_D: 0.3047 Loss_G: 5.7756 D(x): 0.9863 D(G(z)): 0.2218 / 0.0048\n",
            "[0/25][388/391] Loss_D: 0.7791 Loss_G: 1.5506 D(x): 0.5228 D(G(z)): 0.0310 / 0.2657\n",
            "[0/25][389/391] Loss_D: 1.4768 Loss_G: 8.7952 D(x): 0.9982 D(G(z)): 0.6999 / 0.0003\n",
            "[0/25][390/391] Loss_D: 3.6581 Loss_G: 0.7684 D(x): 0.0465 D(G(z)): 0.0011 / 0.5659\n",
            "[1/25][0/391] Loss_D: 1.9466 Loss_G: 5.6373 D(x): 0.9582 D(G(z)): 0.6867 / 0.0127\n",
            "saving the output\n",
            "[1/25][1/391] Loss_D: 2.3553 Loss_G: 0.2412 D(x): 0.1797 D(G(z)): 0.0320 / 0.8162\n",
            "[1/25][2/391] Loss_D: 2.2937 Loss_G: 5.1402 D(x): 0.9811 D(G(z)): 0.8386 / 0.0118\n",
            "[1/25][3/391] Loss_D: 1.3659 Loss_G: 0.9205 D(x): 0.3425 D(G(z)): 0.0367 / 0.4963\n",
            "[1/25][4/391] Loss_D: 1.2113 Loss_G: 3.9003 D(x): 0.9318 D(G(z)): 0.5870 / 0.0449\n",
            "[1/25][5/391] Loss_D: 0.9831 Loss_G: 1.7272 D(x): 0.5118 D(G(z)): 0.1167 / 0.2602\n",
            "[1/25][6/391] Loss_D: 0.8065 Loss_G: 2.0991 D(x): 0.7953 D(G(z)): 0.3494 / 0.1856\n",
            "[1/25][7/391] Loss_D: 0.7476 Loss_G: 2.8126 D(x): 0.7945 D(G(z)): 0.3206 / 0.0867\n",
            "[1/25][8/391] Loss_D: 0.5895 Loss_G: 2.2374 D(x): 0.7153 D(G(z)): 0.1704 / 0.1447\n",
            "[1/25][9/391] Loss_D: 0.7927 Loss_G: 1.7253 D(x): 0.6957 D(G(z)): 0.2897 / 0.2246\n",
            "[1/25][10/391] Loss_D: 0.8538 Loss_G: 3.4386 D(x): 0.8393 D(G(z)): 0.4296 / 0.0524\n",
            "[1/25][11/391] Loss_D: 0.4445 Loss_G: 2.9889 D(x): 0.7450 D(G(z)): 0.0977 / 0.0773\n",
            "[1/25][12/391] Loss_D: 0.6330 Loss_G: 1.4718 D(x): 0.6782 D(G(z)): 0.1624 / 0.3203\n",
            "[1/25][13/391] Loss_D: 0.6395 Loss_G: 2.6858 D(x): 0.8544 D(G(z)): 0.3366 / 0.0958\n",
            "[1/25][14/391] Loss_D: 0.5982 Loss_G: 2.6766 D(x): 0.7619 D(G(z)): 0.2375 / 0.0967\n",
            "[1/25][15/391] Loss_D: 0.7213 Loss_G: 1.9334 D(x): 0.6881 D(G(z)): 0.2258 / 0.1896\n",
            "[1/25][16/391] Loss_D: 0.4887 Loss_G: 2.9743 D(x): 0.8546 D(G(z)): 0.2508 / 0.0692\n",
            "[1/25][17/391] Loss_D: 0.8820 Loss_G: 1.1315 D(x): 0.5897 D(G(z)): 0.1939 / 0.3768\n",
            "[1/25][18/391] Loss_D: 0.7412 Loss_G: 4.3781 D(x): 0.9475 D(G(z)): 0.4480 / 0.0200\n",
            "[1/25][19/391] Loss_D: 0.6936 Loss_G: 2.1166 D(x): 0.5941 D(G(z)): 0.0731 / 0.1688\n",
            "[1/25][20/391] Loss_D: 0.4601 Loss_G: 2.8758 D(x): 0.9142 D(G(z)): 0.2646 / 0.0881\n",
            "[1/25][21/391] Loss_D: 0.4864 Loss_G: 2.2459 D(x): 0.7469 D(G(z)): 0.1204 / 0.1467\n",
            "[1/25][22/391] Loss_D: 0.5069 Loss_G: 2.8851 D(x): 0.8722 D(G(z)): 0.2636 / 0.0835\n",
            "[1/25][23/391] Loss_D: 0.3345 Loss_G: 2.9136 D(x): 0.8382 D(G(z)): 0.1270 / 0.0809\n",
            "[1/25][24/391] Loss_D: 0.5195 Loss_G: 1.9824 D(x): 0.7588 D(G(z)): 0.1782 / 0.1705\n",
            "[1/25][25/391] Loss_D: 0.8266 Loss_G: 4.3472 D(x): 0.8896 D(G(z)): 0.4494 / 0.0207\n",
            "[1/25][26/391] Loss_D: 1.0033 Loss_G: 1.2341 D(x): 0.4676 D(G(z)): 0.0451 / 0.3508\n",
            "[1/25][27/391] Loss_D: 0.9244 Loss_G: 4.0459 D(x): 0.9590 D(G(z)): 0.5148 / 0.0264\n",
            "[1/25][28/391] Loss_D: 0.5108 Loss_G: 2.9585 D(x): 0.6865 D(G(z)): 0.0677 / 0.0709\n",
            "[1/25][29/391] Loss_D: 0.4907 Loss_G: 1.7039 D(x): 0.7321 D(G(z)): 0.0955 / 0.2406\n",
            "[1/25][30/391] Loss_D: 0.7766 Loss_G: 4.2542 D(x): 0.9477 D(G(z)): 0.4640 / 0.0234\n",
            "[1/25][31/391] Loss_D: 0.6139 Loss_G: 2.5608 D(x): 0.6301 D(G(z)): 0.0587 / 0.1129\n",
            "[1/25][32/391] Loss_D: 0.3742 Loss_G: 3.5413 D(x): 0.9402 D(G(z)): 0.2382 / 0.0438\n",
            "[1/25][33/391] Loss_D: 0.3674 Loss_G: 2.8826 D(x): 0.7708 D(G(z)): 0.0707 / 0.0796\n",
            "[1/25][34/391] Loss_D: 0.5900 Loss_G: 4.5277 D(x): 0.8853 D(G(z)): 0.3297 / 0.0169\n",
            "[1/25][35/391] Loss_D: 0.4374 Loss_G: 2.4457 D(x): 0.7197 D(G(z)): 0.0522 / 0.1297\n",
            "[1/25][36/391] Loss_D: 0.3561 Loss_G: 2.7564 D(x): 0.8974 D(G(z)): 0.2002 / 0.0833\n",
            "[1/25][37/391] Loss_D: 0.1826 Loss_G: 3.8288 D(x): 0.9422 D(G(z)): 0.1057 / 0.0332\n",
            "[1/25][38/391] Loss_D: 0.2072 Loss_G: 3.6010 D(x): 0.9141 D(G(z)): 0.0977 / 0.0436\n",
            "[1/25][39/391] Loss_D: 0.3108 Loss_G: 2.0704 D(x): 0.8103 D(G(z)): 0.0748 / 0.1711\n",
            "[1/25][40/391] Loss_D: 0.5213 Loss_G: 5.8447 D(x): 0.9840 D(G(z)): 0.3302 / 0.0054\n",
            "[1/25][41/391] Loss_D: 0.6934 Loss_G: 1.6480 D(x): 0.5723 D(G(z)): 0.0179 / 0.2802\n",
            "[1/25][42/391] Loss_D: 0.3747 Loss_G: 4.0518 D(x): 0.9607 D(G(z)): 0.2490 / 0.0262\n",
            "[1/25][43/391] Loss_D: 0.2406 Loss_G: 3.4768 D(x): 0.8885 D(G(z)): 0.1025 / 0.0441\n",
            "[1/25][44/391] Loss_D: 0.2123 Loss_G: 3.0226 D(x): 0.8997 D(G(z)): 0.0906 / 0.0747\n",
            "[1/25][45/391] Loss_D: 0.1967 Loss_G: 4.2876 D(x): 0.9545 D(G(z)): 0.1273 / 0.0225\n",
            "[1/25][46/391] Loss_D: 0.1774 Loss_G: 3.4948 D(x): 0.8838 D(G(z)): 0.0431 / 0.0485\n",
            "[1/25][47/391] Loss_D: 0.0916 Loss_G: 4.5794 D(x): 0.9806 D(G(z)): 0.0643 / 0.0178\n",
            "[1/25][48/391] Loss_D: 0.0982 Loss_G: 4.5980 D(x): 0.9671 D(G(z)): 0.0596 / 0.0164\n",
            "[1/25][49/391] Loss_D: 0.1170 Loss_G: 4.0955 D(x): 0.9520 D(G(z)): 0.0613 / 0.0261\n",
            "[1/25][50/391] Loss_D: 0.0975 Loss_G: 4.6138 D(x): 0.9649 D(G(z)): 0.0570 / 0.0164\n",
            "[1/25][51/391] Loss_D: 0.1261 Loss_G: 3.7868 D(x): 0.8954 D(G(z)): 0.0099 / 0.0378\n",
            "[1/25][52/391] Loss_D: 0.2721 Loss_G: 7.1371 D(x): 0.9768 D(G(z)): 0.2033 / 0.0014\n",
            "[1/25][53/391] Loss_D: 0.9109 Loss_G: 2.2665 D(x): 0.4743 D(G(z)): 0.0202 / 0.1472\n",
            "[1/25][54/391] Loss_D: 0.6299 Loss_G: 0.4717 D(x): 0.7142 D(G(z)): 0.1990 / 0.6496\n",
            "[1/25][55/391] Loss_D: 2.2349 Loss_G: 7.2855 D(x): 0.9664 D(G(z)): 0.8400 / 0.0012\n",
            "[1/25][56/391] Loss_D: 4.2969 Loss_G: 0.1691 D(x): 0.0254 D(G(z)): 0.0066 / 0.8582\n",
            "[1/25][57/391] Loss_D: 2.5378 Loss_G: 7.1923 D(x): 0.9621 D(G(z)): 0.8582 / 0.0020\n",
            "[1/25][58/391] Loss_D: 4.7132 Loss_G: 0.1694 D(x): 0.0212 D(G(z)): 0.0049 / 0.8594\n",
            "[1/25][59/391] Loss_D: 2.4287 Loss_G: 4.5002 D(x): 0.9830 D(G(z)): 0.8504 / 0.0235\n",
            "[1/25][60/391] Loss_D: 1.5605 Loss_G: 1.2511 D(x): 0.3194 D(G(z)): 0.0860 / 0.3847\n",
            "[1/25][61/391] Loss_D: 1.0237 Loss_G: 2.1460 D(x): 0.8376 D(G(z)): 0.4537 / 0.1896\n",
            "[1/25][62/391] Loss_D: 0.8437 Loss_G: 2.7370 D(x): 0.7385 D(G(z)): 0.3063 / 0.1423\n",
            "[1/25][63/391] Loss_D: 1.1157 Loss_G: 1.6783 D(x): 0.6009 D(G(z)): 0.2714 / 0.2949\n",
            "[1/25][64/391] Loss_D: 0.8183 Loss_G: 2.2728 D(x): 0.7632 D(G(z)): 0.3141 / 0.1502\n",
            "[1/25][65/391] Loss_D: 0.5814 Loss_G: 2.7999 D(x): 0.8209 D(G(z)): 0.2764 / 0.0941\n",
            "[1/25][66/391] Loss_D: 0.9569 Loss_G: 1.7265 D(x): 0.6157 D(G(z)): 0.2685 / 0.2436\n",
            "[1/25][67/391] Loss_D: 0.6425 Loss_G: 2.4887 D(x): 0.7866 D(G(z)): 0.2751 / 0.1151\n",
            "[1/25][68/391] Loss_D: 0.8102 Loss_G: 1.5178 D(x): 0.6440 D(G(z)): 0.2264 / 0.2889\n",
            "[1/25][69/391] Loss_D: 0.7648 Loss_G: 3.5681 D(x): 0.8958 D(G(z)): 0.4164 / 0.0433\n",
            "[1/25][70/391] Loss_D: 0.7401 Loss_G: 1.4153 D(x): 0.5774 D(G(z)): 0.0964 / 0.2987\n",
            "[1/25][71/391] Loss_D: 0.7594 Loss_G: 3.3670 D(x): 0.8942 D(G(z)): 0.4165 / 0.0498\n",
            "[1/25][72/391] Loss_D: 0.7601 Loss_G: 1.8032 D(x): 0.6157 D(G(z)): 0.1506 / 0.2272\n",
            "[1/25][73/391] Loss_D: 0.6722 Loss_G: 3.3507 D(x): 0.8832 D(G(z)): 0.3645 / 0.0508\n",
            "[1/25][74/391] Loss_D: 0.6974 Loss_G: 1.4459 D(x): 0.6146 D(G(z)): 0.1291 / 0.2957\n",
            "[1/25][75/391] Loss_D: 0.5914 Loss_G: 4.0551 D(x): 0.9508 D(G(z)): 0.3735 / 0.0266\n",
            "[1/25][76/391] Loss_D: 0.4348 Loss_G: 3.0472 D(x): 0.7543 D(G(z)): 0.1003 / 0.0702\n",
            "[1/25][77/391] Loss_D: 0.4752 Loss_G: 2.0738 D(x): 0.7881 D(G(z)): 0.1745 / 0.1673\n",
            "[1/25][78/391] Loss_D: 0.3865 Loss_G: 3.3706 D(x): 0.9102 D(G(z)): 0.2230 / 0.0516\n",
            "[1/25][79/391] Loss_D: 0.3878 Loss_G: 2.6592 D(x): 0.8012 D(G(z)): 0.1142 / 0.1050\n",
            "[1/25][80/391] Loss_D: 0.4809 Loss_G: 3.3754 D(x): 0.8676 D(G(z)): 0.2477 / 0.0509\n",
            "[1/25][81/391] Loss_D: 0.4268 Loss_G: 2.0605 D(x): 0.7610 D(G(z)): 0.0991 / 0.1792\n",
            "[1/25][82/391] Loss_D: 0.4612 Loss_G: 5.0345 D(x): 0.9608 D(G(z)): 0.3087 / 0.0093\n",
            "[1/25][83/391] Loss_D: 0.7743 Loss_G: 0.8905 D(x): 0.5383 D(G(z)): 0.0405 / 0.5009\n",
            "[1/25][84/391] Loss_D: 1.1434 Loss_G: 7.5072 D(x): 0.9765 D(G(z)): 0.6098 / 0.0009\n",
            "[1/25][85/391] Loss_D: 3.3124 Loss_G: 1.5251 D(x): 0.0626 D(G(z)): 0.0039 / 0.2865\n",
            "[1/25][86/391] Loss_D: 1.0834 Loss_G: 3.0444 D(x): 0.8603 D(G(z)): 0.5110 / 0.0941\n",
            "[1/25][87/391] Loss_D: 0.9597 Loss_G: 2.5239 D(x): 0.6515 D(G(z)): 0.2998 / 0.1151\n",
            "[1/25][88/391] Loss_D: 0.5725 Loss_G: 2.6402 D(x): 0.7391 D(G(z)): 0.1862 / 0.1018\n",
            "[1/25][89/391] Loss_D: 0.4453 Loss_G: 3.1965 D(x): 0.8554 D(G(z)): 0.2177 / 0.0580\n",
            "[1/25][90/391] Loss_D: 0.5541 Loss_G: 3.0230 D(x): 0.8266 D(G(z)): 0.2619 / 0.0636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr8WmW4e7XLg",
        "outputId": "7e6a7c60-7733-4672-f416-42a3be09e00f"
      },
      "source": [
        "!git clone https://github.com/sbarratt/inception-score-pytorch.git"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'inception-score-pytorch'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 43 (delta 0), reused 1 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qtoUg-H3ZsQ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('./inception-score-pytorch')\n",
        "from inception_score import inception_score"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JGUUsWu7qMD",
        "outputId": "7c3fa668-a005-4ef4-9ef4-2e2ed0718e2c"
      },
      "source": [
        "class IgnoreLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, orig):\n",
        "        self.orig = orig\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.orig[index][0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.orig)\n",
        "\n",
        "print(inception_score(IgnoreLabelDataset(dataset), cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "./inception-score-pytorch/inception_score.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x).data.cpu().numpy()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(8.698740416061545, 0.14147672235452327)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCQ-tQzJ-HIc",
        "outputId": "78589fd7-e079-451d-d961-6f45e9acb4e0"
      },
      "source": [
        "eval_images = []\n",
        "for i in range(200):\n",
        "    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    with torch.no_grad():\n",
        "        fake = netG(noise).detach().cpu()\n",
        "    eval_images += fake\n",
        "\n",
        "print(len(eval_images))\n",
        "print(eval_images[0].shape)\n",
        "print(inception_score(eval_images, cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16000\n",
            "torch.Size([3, 64, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "./inception-score-pytorch/inception_score.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x).data.cpu().numpy()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4.2092557388624, 0.0962006108358531)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}