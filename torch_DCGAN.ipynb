{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOidYodzS0eaOBdVxsw6Xed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "094299c6c7fe4e58911a9d9e8dad8617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e5e45665b7fa48a2b2b8cd3b2099e5f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_195a7db6d73c44989fd08d2a951059dd",
              "IPY_MODEL_b0acec4a2aea429da4279762018b6fc3"
            ]
          }
        },
        "e5e45665b7fa48a2b2b8cd3b2099e5f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "195a7db6d73c44989fd08d2a951059dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1a50942ed98b4dfba47fc6ba6afabfd0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edd201fbca62432da69f900810b0e6eb"
          }
        },
        "b0acec4a2aea429da4279762018b6fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_65e7eaca6aa14bf2bd84ef0039e9ea18",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [01:01&lt;00:00, 2775620.69it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_529b973b77b349ce8dbd7deccc466511"
          }
        },
        "1a50942ed98b4dfba47fc6ba6afabfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edd201fbca62432da69f900810b0e6eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65e7eaca6aa14bf2bd84ef0039e9ea18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "529b973b77b349ce8dbd7deccc466511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6be1cb7b22184708aeea0b2ab9bf4089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08534094490f455e8016f800e6837ff9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cccea77658ba4b0d85aa43f0780e7267",
              "IPY_MODEL_63090f4d6db14eefa428801589ba1401"
            ]
          }
        },
        "08534094490f455e8016f800e6837ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cccea77658ba4b0d85aa43f0780e7267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0a2124f67552473e9fad3ffef2456124",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108949747,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108949747,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a78009aad69e4d1dbdeec3926a82795c"
          }
        },
        "63090f4d6db14eefa428801589ba1401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9fd673ee68f04698bbe4b195d1a84f3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:01&lt;00:00, 65.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_091aed46cde54a228dcd406a018a733d"
          }
        },
        "0a2124f67552473e9fad3ffef2456124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a78009aad69e4d1dbdeec3926a82795c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fd673ee68f04698bbe4b195d1a84f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "091aed46cde54a228dcd406a018a733d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ede85a895f64e68847cdeee31644224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3048ee5f602d403f94620b4233314bd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f835bced574d4dc59252a0136d53c7f9",
              "IPY_MODEL_060b9239fd9a4a5882f40dd501c9d6d6"
            ]
          }
        },
        "3048ee5f602d403f94620b4233314bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f835bced574d4dc59252a0136d53c7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c39dcec62aab46f3982252bc714ad0a4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108949747,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108949747,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a95023631fa44ecdb89ef2644d9df68d"
          }
        },
        "060b9239fd9a4a5882f40dd501c9d6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0b2b4f590fc470f9e490ea46bb4c6c3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:01&lt;00:00, 71.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_363b8ed2ffe54e3b9df80743115ce655"
          }
        },
        "c39dcec62aab46f3982252bc714ad0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a95023631fa44ecdb89ef2644d9df68d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0b2b4f590fc470f9e490ea46bb4c6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "363b8ed2ffe54e3b9df80743115ce655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dolmani38/AMD-GAN/blob/main/torch_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAjWEM2bxbj8",
        "outputId": "3a818ce3-dad2-4856-a373-fb44989660ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSAX0DwVFmok"
      },
      "source": [
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "094299c6c7fe4e58911a9d9e8dad8617",
            "e5e45665b7fa48a2b2b8cd3b2099e5f3",
            "195a7db6d73c44989fd08d2a951059dd",
            "b0acec4a2aea429da4279762018b6fc3",
            "1a50942ed98b4dfba47fc6ba6afabfd0",
            "edd201fbca62432da69f900810b0e6eb",
            "65e7eaca6aa14bf2bd84ef0039e9ea18",
            "529b973b77b349ce8dbd7deccc466511"
          ]
        },
        "id": "1Yrkf_2fx9OY",
        "outputId": "643e2b6a-e007-4c17-fadf-927b67b75b94"
      },
      "source": [
        "cudnn.benchmark = True\n",
        "\n",
        "#set manual seed to a constant get a consistent output\n",
        "manualSeed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "#loading the dataset\n",
        "dataset = dset.CIFAR10(root=\"./data\", download=True, train=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               #transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "\n",
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  6145\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "094299c6c7fe4e58911a9d9e8dad8617",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usqya5HOHcSM",
        "outputId": "129b95db-f837-4588-eba7-869c026d65a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class IgnoreLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, orig):\n",
        "        self.orig = orig\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.orig[index][0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.orig)\n",
        "\n",
        "ld = IgnoreLabelDataset(dataset)\n",
        "print(len(ld))\n",
        "print(ld[0].shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tltjWyJWyIn3"
      },
      "source": [
        "nc=3\n",
        "\n",
        "# number of gpu's available\n",
        "ngpu = 1\n",
        "# input noise dimension\n",
        "nz = 100\n",
        "# number of generator filters\n",
        "ngf = 64\n",
        "#number of discriminator filters\n",
        "ndf = 64\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPChsX8JyNks",
        "outputId": "4cbaac9d-be87-429b-8515-e410304c03bb"
      },
      "source": [
        "\n",
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (ngf*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. (nc) x 64 x 64\n",
        "                        \n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "            return output\n",
        "\n",
        "netG = Generator(ngpu).to(device)\n",
        "netG.apply(weights_init)\n",
        "#load weights to test the model\n",
        "#netG.load_state_dict(torch.load('weights/netG_epoch_24.pth'))\n",
        "print(netG)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8PJwrPIyVgk",
        "outputId": "81faa941-56d3-47c1-c9b5-721bd4f26eaa"
      },
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            #nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            #nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(nc, ndf * 2, 4, 2, 1, bias=False),\n",
        "            #nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.is_cuda and self.ngpu > 1:\n",
        "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
        "        else:\n",
        "            output = self.main(input)\n",
        "\n",
        "        return output.view(-1, 1).squeeze(1)\n",
        "\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "netD.apply(weights_init)\n",
        "#load weights to test the model \n",
        "#netD.load_state_dict(torch.load('weights/netD_epoch_24.pth'))\n",
        "print(netD)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs5F_k5yyZM2"
      },
      "source": [
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(128, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "niter = 25\n",
        "g_loss = []\n",
        "d_loss = []"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9myBC-vfygnz",
        "outputId": "056f8ca6-60a3-4a14-efe7-67ff6794aee2"
      },
      "source": [
        "\n",
        "for epoch in range(niter):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        rlabel = torch.full((batch_size,), real_label, dtype=torch.float32, device=device)\n",
        "        flabel = torch.full((batch_size,), fake_label, dtype=torch.float32, device=device)\n",
        "\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "\n",
        "        errD = criterion(netD(real_cpu), rlabel) + criterion(netD(fake.detach()), flabel)\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "\n",
        "        errG = criterion(netD(fake), rlabel)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f ' % (epoch, niter, i, len(dataloader), errD.item(), errG.item()))\n",
        "        \n",
        "        #save the output\n",
        "        if i % 100 == 0:\n",
        "            print('saving the output')\n",
        "            vutils.save_image(real_cpu,'./drive/MyDrive/t_DCGAN/output/real_samples.png',normalize=True)\n",
        "            fake = netG(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),'./drive/MyDrive/t_DCGAN/output/fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n",
        "    \n",
        "    # Check pointing for every epoch\n",
        "    torch.save(netG.state_dict(), './drive/MyDrive/t_DCGAN/weights/netG_epoch_%d.pth' % (epoch))\n",
        "    torch.save(netD.state_dict(), './drive/MyDrive/t_DCGAN/weights/netD_epoch_%d.pth' % (epoch))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[12/25][133/391] Loss_D: 0.2569 Loss_G: 2.8886 \n",
            "[12/25][134/391] Loss_D: 0.2436 Loss_G: 2.8783 \n",
            "[12/25][135/391] Loss_D: 0.3297 Loss_G: 2.8382 \n",
            "[12/25][136/391] Loss_D: 0.3373 Loss_G: 2.3662 \n",
            "[12/25][137/391] Loss_D: 0.4544 Loss_G: 3.5773 \n",
            "[12/25][138/391] Loss_D: 0.6984 Loss_G: 1.3458 \n",
            "[12/25][139/391] Loss_D: 0.4733 Loss_G: 3.3451 \n",
            "[12/25][140/391] Loss_D: 0.3019 Loss_G: 3.3028 \n",
            "[12/25][141/391] Loss_D: 0.2932 Loss_G: 2.7603 \n",
            "[12/25][142/391] Loss_D: 0.3355 Loss_G: 2.8879 \n",
            "[12/25][143/391] Loss_D: 0.3733 Loss_G: 2.8200 \n",
            "[12/25][144/391] Loss_D: 0.4129 Loss_G: 1.9700 \n",
            "[12/25][145/391] Loss_D: 0.4087 Loss_G: 3.7173 \n",
            "[12/25][146/391] Loss_D: 0.3036 Loss_G: 3.0100 \n",
            "[12/25][147/391] Loss_D: 0.3060 Loss_G: 2.2205 \n",
            "[12/25][148/391] Loss_D: 0.5184 Loss_G: 3.7026 \n",
            "[12/25][149/391] Loss_D: 0.5328 Loss_G: 1.9803 \n",
            "[12/25][150/391] Loss_D: 0.3997 Loss_G: 2.8787 \n",
            "[12/25][151/391] Loss_D: 0.2740 Loss_G: 3.5979 \n",
            "[12/25][152/391] Loss_D: 0.3780 Loss_G: 2.2734 \n",
            "[12/25][153/391] Loss_D: 0.3356 Loss_G: 2.2212 \n",
            "[12/25][154/391] Loss_D: 0.4005 Loss_G: 3.5254 \n",
            "[12/25][155/391] Loss_D: 0.4447 Loss_G: 2.2760 \n",
            "[12/25][156/391] Loss_D: 0.3304 Loss_G: 3.0247 \n",
            "[12/25][157/391] Loss_D: 0.3782 Loss_G: 2.5803 \n",
            "[12/25][158/391] Loss_D: 0.3503 Loss_G: 2.5169 \n",
            "[12/25][159/391] Loss_D: 0.2717 Loss_G: 2.9977 \n",
            "[12/25][160/391] Loss_D: 0.3363 Loss_G: 3.1451 \n",
            "[12/25][161/391] Loss_D: 0.4854 Loss_G: 1.8647 \n",
            "[12/25][162/391] Loss_D: 0.3552 Loss_G: 3.0535 \n",
            "[12/25][163/391] Loss_D: 0.3445 Loss_G: 3.0066 \n",
            "[12/25][164/391] Loss_D: 0.2863 Loss_G: 2.6860 \n",
            "[12/25][165/391] Loss_D: 0.3125 Loss_G: 2.5230 \n",
            "[12/25][166/391] Loss_D: 0.3404 Loss_G: 3.3830 \n",
            "[12/25][167/391] Loss_D: 0.6634 Loss_G: 1.5241 \n",
            "[12/25][168/391] Loss_D: 0.4345 Loss_G: 3.1977 \n",
            "[12/25][169/391] Loss_D: 0.5115 Loss_G: 2.0599 \n",
            "[12/25][170/391] Loss_D: 0.3693 Loss_G: 3.4190 \n",
            "[12/25][171/391] Loss_D: 0.3617 Loss_G: 2.5460 \n",
            "[12/25][172/391] Loss_D: 0.3454 Loss_G: 2.6575 \n",
            "[12/25][173/391] Loss_D: 0.2708 Loss_G: 3.1092 \n",
            "[12/25][174/391] Loss_D: 0.4309 Loss_G: 2.7769 \n",
            "[12/25][175/391] Loss_D: 0.2850 Loss_G: 2.8373 \n",
            "[12/25][176/391] Loss_D: 0.2062 Loss_G: 3.2887 \n",
            "[12/25][177/391] Loss_D: 0.2076 Loss_G: 3.2398 \n",
            "[12/25][178/391] Loss_D: 0.3203 Loss_G: 2.4277 \n",
            "[12/25][179/391] Loss_D: 0.3429 Loss_G: 3.4655 \n",
            "[12/25][180/391] Loss_D: 0.3614 Loss_G: 2.6345 \n",
            "[12/25][181/391] Loss_D: 0.4849 Loss_G: 1.8464 \n",
            "[12/25][182/391] Loss_D: 0.4095 Loss_G: 4.5070 \n",
            "[12/25][183/391] Loss_D: 0.5625 Loss_G: 1.6199 \n",
            "[12/25][184/391] Loss_D: 0.4282 Loss_G: 3.9708 \n",
            "[12/25][185/391] Loss_D: 0.4369 Loss_G: 2.4791 \n",
            "[12/25][186/391] Loss_D: 0.3544 Loss_G: 3.3019 \n",
            "[12/25][187/391] Loss_D: 0.2768 Loss_G: 2.9008 \n",
            "[12/25][188/391] Loss_D: 0.3703 Loss_G: 2.4326 \n",
            "[12/25][189/391] Loss_D: 0.3941 Loss_G: 2.5442 \n",
            "[12/25][190/391] Loss_D: 0.3826 Loss_G: 3.9486 \n",
            "[12/25][191/391] Loss_D: 0.4389 Loss_G: 2.2980 \n",
            "[12/25][192/391] Loss_D: 0.3790 Loss_G: 2.4441 \n",
            "[12/25][193/391] Loss_D: 0.5468 Loss_G: 2.8691 \n",
            "[12/25][194/391] Loss_D: 0.3103 Loss_G: 2.8984 \n",
            "[12/25][195/391] Loss_D: 0.3556 Loss_G: 2.7634 \n",
            "[12/25][196/391] Loss_D: 0.4573 Loss_G: 2.0728 \n",
            "[12/25][197/391] Loss_D: 0.3739 Loss_G: 3.9258 \n",
            "[12/25][198/391] Loss_D: 0.2903 Loss_G: 2.7583 \n",
            "[12/25][199/391] Loss_D: 0.3317 Loss_G: 2.4184 \n",
            "[12/25][200/391] Loss_D: 0.3370 Loss_G: 3.8735 \n",
            "saving the output\n",
            "[12/25][201/391] Loss_D: 0.2959 Loss_G: 2.9996 \n",
            "[12/25][202/391] Loss_D: 0.3893 Loss_G: 2.0561 \n",
            "[12/25][203/391] Loss_D: 0.3825 Loss_G: 3.7269 \n",
            "[12/25][204/391] Loss_D: 0.3145 Loss_G: 3.0264 \n",
            "[12/25][205/391] Loss_D: 0.3753 Loss_G: 2.8301 \n",
            "[12/25][206/391] Loss_D: 0.2874 Loss_G: 3.7495 \n",
            "[12/25][207/391] Loss_D: 0.2045 Loss_G: 3.4576 \n",
            "[12/25][208/391] Loss_D: 0.4854 Loss_G: 1.0070 \n",
            "[12/25][209/391] Loss_D: 0.9074 Loss_G: 5.3515 \n",
            "[12/25][210/391] Loss_D: 0.8668 Loss_G: 1.3473 \n",
            "[12/25][211/391] Loss_D: 0.7775 Loss_G: 4.4349 \n",
            "[12/25][212/391] Loss_D: 0.6389 Loss_G: 1.9563 \n",
            "[12/25][213/391] Loss_D: 0.4270 Loss_G: 3.0405 \n",
            "[12/25][214/391] Loss_D: 0.4643 Loss_G: 2.6725 \n",
            "[12/25][215/391] Loss_D: 0.5786 Loss_G: 2.9051 \n",
            "[12/25][216/391] Loss_D: 0.6660 Loss_G: 1.9704 \n",
            "[12/25][217/391] Loss_D: 0.3735 Loss_G: 4.5424 \n",
            "[12/25][218/391] Loss_D: 0.4897 Loss_G: 2.2560 \n",
            "[12/25][219/391] Loss_D: 0.5874 Loss_G: 4.3001 \n",
            "[12/25][220/391] Loss_D: 0.4977 Loss_G: 2.6116 \n",
            "[12/25][221/391] Loss_D: 0.3710 Loss_G: 2.7107 \n",
            "[12/25][222/391] Loss_D: 0.3709 Loss_G: 3.5925 \n",
            "[12/25][223/391] Loss_D: 0.5138 Loss_G: 1.9064 \n",
            "[12/25][224/391] Loss_D: 0.6201 Loss_G: 4.7743 \n",
            "[12/25][225/391] Loss_D: 0.9329 Loss_G: 0.8788 \n",
            "[12/25][226/391] Loss_D: 0.9761 Loss_G: 4.5200 \n",
            "[12/25][227/391] Loss_D: 0.7907 Loss_G: 0.2606 \n",
            "[12/25][228/391] Loss_D: 2.2351 Loss_G: 7.5151 \n",
            "[12/25][229/391] Loss_D: 2.7431 Loss_G: 1.3513 \n",
            "[12/25][230/391] Loss_D: 0.8740 Loss_G: 4.0434 \n",
            "[12/25][231/391] Loss_D: 0.4545 Loss_G: 2.7486 \n",
            "[12/25][232/391] Loss_D: 0.4665 Loss_G: 1.8866 \n",
            "[12/25][233/391] Loss_D: 0.6743 Loss_G: 4.7069 \n",
            "[12/25][234/391] Loss_D: 0.9727 Loss_G: 0.7798 \n",
            "[12/25][235/391] Loss_D: 1.1405 Loss_G: 4.8986 \n",
            "[12/25][236/391] Loss_D: 0.8308 Loss_G: 2.2747 \n",
            "[12/25][237/391] Loss_D: 0.5138 Loss_G: 2.1679 \n",
            "[12/25][238/391] Loss_D: 0.6833 Loss_G: 3.7431 \n",
            "[12/25][239/391] Loss_D: 0.4119 Loss_G: 2.7535 \n",
            "[12/25][240/391] Loss_D: 0.4292 Loss_G: 2.6254 \n",
            "[12/25][241/391] Loss_D: 0.5065 Loss_G: 4.6130 \n",
            "[12/25][242/391] Loss_D: 0.7756 Loss_G: 1.4289 \n",
            "[12/25][243/391] Loss_D: 0.6095 Loss_G: 4.0583 \n",
            "[12/25][244/391] Loss_D: 0.6266 Loss_G: 1.9797 \n",
            "[12/25][245/391] Loss_D: 0.4973 Loss_G: 4.1376 \n",
            "[12/25][246/391] Loss_D: 0.4691 Loss_G: 2.8737 \n",
            "[12/25][247/391] Loss_D: 0.3299 Loss_G: 3.0390 \n",
            "[12/25][248/391] Loss_D: 0.3847 Loss_G: 3.3614 \n",
            "[12/25][249/391] Loss_D: 0.2864 Loss_G: 3.4204 \n",
            "[12/25][250/391] Loss_D: 0.3932 Loss_G: 2.9284 \n",
            "[12/25][251/391] Loss_D: 0.4615 Loss_G: 2.2368 \n",
            "[12/25][252/391] Loss_D: 0.4055 Loss_G: 3.9353 \n",
            "[12/25][253/391] Loss_D: 0.5687 Loss_G: 2.1965 \n",
            "[12/25][254/391] Loss_D: 0.4219 Loss_G: 3.4675 \n",
            "[12/25][255/391] Loss_D: 0.2977 Loss_G: 3.2975 \n",
            "[12/25][256/391] Loss_D: 0.3199 Loss_G: 3.1169 \n",
            "[12/25][257/391] Loss_D: 0.3055 Loss_G: 2.8561 \n",
            "[12/25][258/391] Loss_D: 0.3080 Loss_G: 2.7791 \n",
            "[12/25][259/391] Loss_D: 0.3052 Loss_G: 3.5346 \n",
            "[12/25][260/391] Loss_D: 0.3798 Loss_G: 2.6350 \n",
            "[12/25][261/391] Loss_D: 0.3696 Loss_G: 2.5718 \n",
            "[12/25][262/391] Loss_D: 0.3227 Loss_G: 3.5868 \n",
            "[12/25][263/391] Loss_D: 0.2396 Loss_G: 3.3480 \n",
            "[12/25][264/391] Loss_D: 0.2473 Loss_G: 3.1319 \n",
            "[12/25][265/391] Loss_D: 0.3823 Loss_G: 2.4197 \n",
            "[12/25][266/391] Loss_D: 0.3661 Loss_G: 3.4757 \n",
            "[12/25][267/391] Loss_D: 0.4751 Loss_G: 1.9671 \n",
            "[12/25][268/391] Loss_D: 0.4567 Loss_G: 3.2745 \n",
            "[12/25][269/391] Loss_D: 0.3138 Loss_G: 2.9088 \n",
            "[12/25][270/391] Loss_D: 0.4272 Loss_G: 2.1007 \n",
            "[12/25][271/391] Loss_D: 0.4941 Loss_G: 4.0653 \n",
            "[12/25][272/391] Loss_D: 0.4910 Loss_G: 2.0762 \n",
            "[12/25][273/391] Loss_D: 0.4291 Loss_G: 3.6016 \n",
            "[12/25][274/391] Loss_D: 0.4133 Loss_G: 2.9157 \n",
            "[12/25][275/391] Loss_D: 0.3314 Loss_G: 2.2893 \n",
            "[12/25][276/391] Loss_D: 0.6122 Loss_G: 4.4981 \n",
            "[12/25][277/391] Loss_D: 0.3991 Loss_G: 3.0175 \n",
            "[12/25][278/391] Loss_D: 0.3043 Loss_G: 2.7189 \n",
            "[12/25][279/391] Loss_D: 0.3891 Loss_G: 4.3176 \n",
            "[12/25][280/391] Loss_D: 0.4706 Loss_G: 2.7386 \n",
            "[12/25][281/391] Loss_D: 0.3568 Loss_G: 3.9692 \n",
            "[12/25][282/391] Loss_D: 0.1733 Loss_G: 4.0715 \n",
            "[12/25][283/391] Loss_D: 0.1342 Loss_G: 3.8057 \n",
            "[12/25][284/391] Loss_D: 0.2140 Loss_G: 3.4729 \n",
            "[12/25][285/391] Loss_D: 0.3189 Loss_G: 3.4843 \n",
            "[12/25][286/391] Loss_D: 0.4900 Loss_G: 2.7850 \n",
            "[12/25][287/391] Loss_D: 0.6424 Loss_G: 3.3639 \n",
            "[12/25][288/391] Loss_D: 0.4726 Loss_G: 3.0588 \n",
            "[12/25][289/391] Loss_D: 0.4566 Loss_G: 3.3359 \n",
            "[12/25][290/391] Loss_D: 0.3374 Loss_G: 2.9879 \n",
            "[12/25][291/391] Loss_D: 0.3708 Loss_G: 2.9682 \n",
            "[12/25][292/391] Loss_D: 0.3106 Loss_G: 3.1485 \n",
            "[12/25][293/391] Loss_D: 0.2419 Loss_G: 3.5657 \n",
            "[12/25][294/391] Loss_D: 0.2523 Loss_G: 3.2775 \n",
            "[12/25][295/391] Loss_D: 0.3766 Loss_G: 3.7971 \n",
            "[12/25][296/391] Loss_D: 0.3065 Loss_G: 2.9874 \n",
            "[12/25][297/391] Loss_D: 0.5682 Loss_G: 3.6411 \n",
            "[12/25][298/391] Loss_D: 0.5627 Loss_G: 2.3916 \n",
            "[12/25][299/391] Loss_D: 0.5443 Loss_G: 4.3958 \n",
            "[12/25][300/391] Loss_D: 0.3938 Loss_G: 2.8184 \n",
            "saving the output\n",
            "[12/25][301/391] Loss_D: 0.3279 Loss_G: 3.8870 \n",
            "[12/25][302/391] Loss_D: 0.3794 Loss_G: 2.4781 \n",
            "[12/25][303/391] Loss_D: 0.4776 Loss_G: 3.7431 \n",
            "[12/25][304/391] Loss_D: 0.3224 Loss_G: 3.2372 \n",
            "[12/25][305/391] Loss_D: 0.4767 Loss_G: 2.3162 \n",
            "[12/25][306/391] Loss_D: 0.4304 Loss_G: 4.0169 \n",
            "[12/25][307/391] Loss_D: 0.4411 Loss_G: 2.4231 \n",
            "[12/25][308/391] Loss_D: 0.5648 Loss_G: 4.0192 \n",
            "[12/25][309/391] Loss_D: 0.3842 Loss_G: 2.7279 \n",
            "[12/25][310/391] Loss_D: 0.4473 Loss_G: 3.9935 \n",
            "[12/25][311/391] Loss_D: 0.4888 Loss_G: 2.0094 \n",
            "[12/25][312/391] Loss_D: 0.5006 Loss_G: 4.7668 \n",
            "[12/25][313/391] Loss_D: 0.8200 Loss_G: 0.9356 \n",
            "[12/25][314/391] Loss_D: 0.9375 Loss_G: 5.0095 \n",
            "[12/25][315/391] Loss_D: 0.5713 Loss_G: 0.7649 \n",
            "[12/25][316/391] Loss_D: 1.3434 Loss_G: 8.7095 \n",
            "[12/25][317/391] Loss_D: 3.4150 Loss_G: 0.0834 \n",
            "[12/25][318/391] Loss_D: 3.2080 Loss_G: 8.1029 \n",
            "[12/25][319/391] Loss_D: 3.4655 Loss_G: 0.5907 \n",
            "[12/25][320/391] Loss_D: 1.8573 Loss_G: 4.8407 \n",
            "[12/25][321/391] Loss_D: 0.4823 Loss_G: 3.3600 \n",
            "[12/25][322/391] Loss_D: 0.6969 Loss_G: 1.1268 \n",
            "[12/25][323/391] Loss_D: 1.4881 Loss_G: 7.9198 \n",
            "[12/25][324/391] Loss_D: 2.4846 Loss_G: 0.8016 \n",
            "[12/25][325/391] Loss_D: 1.1970 Loss_G: 3.1438 \n",
            "[12/25][326/391] Loss_D: 0.6945 Loss_G: 4.0054 \n",
            "[12/25][327/391] Loss_D: 0.9620 Loss_G: 1.4662 \n",
            "[12/25][328/391] Loss_D: 0.7644 Loss_G: 3.9350 \n",
            "[12/25][329/391] Loss_D: 0.7935 Loss_G: 2.1885 \n",
            "[12/25][330/391] Loss_D: 0.8101 Loss_G: 4.5733 \n",
            "[12/25][331/391] Loss_D: 0.7473 Loss_G: 2.0388 \n",
            "[12/25][332/391] Loss_D: 0.8454 Loss_G: 3.9684 \n",
            "[12/25][333/391] Loss_D: 0.8488 Loss_G: 1.4622 \n",
            "[12/25][334/391] Loss_D: 1.0685 Loss_G: 5.2762 \n",
            "[12/25][335/391] Loss_D: 1.1885 Loss_G: 1.5027 \n",
            "[12/25][336/391] Loss_D: 0.9571 Loss_G: 3.8525 \n",
            "[12/25][337/391] Loss_D: 0.7484 Loss_G: 1.9393 \n",
            "[12/25][338/391] Loss_D: 0.6818 Loss_G: 3.8757 \n",
            "[12/25][339/391] Loss_D: 0.4018 Loss_G: 3.1089 \n",
            "[12/25][340/391] Loss_D: 0.4954 Loss_G: 2.1892 \n",
            "[12/25][341/391] Loss_D: 0.5302 Loss_G: 3.7264 \n",
            "[12/25][342/391] Loss_D: 0.4800 Loss_G: 2.6973 \n",
            "[12/25][343/391] Loss_D: 0.3756 Loss_G: 3.1412 \n",
            "[12/25][344/391] Loss_D: 0.4769 Loss_G: 2.4688 \n",
            "[12/25][345/391] Loss_D: 0.4235 Loss_G: 3.6776 \n",
            "[12/25][346/391] Loss_D: 0.4545 Loss_G: 2.7132 \n",
            "[12/25][347/391] Loss_D: 0.4615 Loss_G: 2.8478 \n",
            "[12/25][348/391] Loss_D: 0.3414 Loss_G: 2.9649 \n",
            "[12/25][349/391] Loss_D: 0.2723 Loss_G: 3.3575 \n",
            "[12/25][350/391] Loss_D: 0.5070 Loss_G: 2.4534 \n",
            "[12/25][351/391] Loss_D: 0.3814 Loss_G: 3.1702 \n",
            "[12/25][352/391] Loss_D: 0.3293 Loss_G: 3.2016 \n",
            "[12/25][353/391] Loss_D: 0.4344 Loss_G: 2.4227 \n",
            "[12/25][354/391] Loss_D: 0.3350 Loss_G: 3.1023 \n",
            "[12/25][355/391] Loss_D: 0.2791 Loss_G: 3.6051 \n",
            "[12/25][356/391] Loss_D: 0.4011 Loss_G: 2.4037 \n",
            "[12/25][357/391] Loss_D: 0.3473 Loss_G: 3.6247 \n",
            "[12/25][358/391] Loss_D: 0.6344 Loss_G: 1.4613 \n",
            "[12/25][359/391] Loss_D: 0.7446 Loss_G: 4.7606 \n",
            "[12/25][360/391] Loss_D: 0.4896 Loss_G: 2.9256 \n",
            "[12/25][361/391] Loss_D: 0.4633 Loss_G: 1.7925 \n",
            "[12/25][362/391] Loss_D: 0.4848 Loss_G: 3.3506 \n",
            "[12/25][363/391] Loss_D: 0.4687 Loss_G: 2.7449 \n",
            "[12/25][364/391] Loss_D: 0.5325 Loss_G: 2.5502 \n",
            "[12/25][365/391] Loss_D: 0.4229 Loss_G: 2.5114 \n",
            "[12/25][366/391] Loss_D: 0.5869 Loss_G: 2.5094 \n",
            "[12/25][367/391] Loss_D: 0.4070 Loss_G: 4.0939 \n",
            "[12/25][368/391] Loss_D: 0.4617 Loss_G: 2.1498 \n",
            "[12/25][369/391] Loss_D: 0.5310 Loss_G: 2.5881 \n",
            "[12/25][370/391] Loss_D: 0.5144 Loss_G: 3.2076 \n",
            "[12/25][371/391] Loss_D: 0.6043 Loss_G: 1.6162 \n",
            "[12/25][372/391] Loss_D: 0.7103 Loss_G: 4.3608 \n",
            "[12/25][373/391] Loss_D: 0.8079 Loss_G: 1.3829 \n",
            "[12/25][374/391] Loss_D: 0.5876 Loss_G: 3.6731 \n",
            "[12/25][375/391] Loss_D: 0.4075 Loss_G: 2.7482 \n",
            "[12/25][376/391] Loss_D: 0.3427 Loss_G: 3.0344 \n",
            "[12/25][377/391] Loss_D: 0.3340 Loss_G: 2.8777 \n",
            "[12/25][378/391] Loss_D: 0.5604 Loss_G: 3.7738 \n",
            "[12/25][379/391] Loss_D: 0.5701 Loss_G: 1.9305 \n",
            "[12/25][380/391] Loss_D: 0.6140 Loss_G: 3.3495 \n",
            "[12/25][381/391] Loss_D: 0.2907 Loss_G: 3.5031 \n",
            "[12/25][382/391] Loss_D: 0.2974 Loss_G: 2.7213 \n",
            "[12/25][383/391] Loss_D: 0.3534 Loss_G: 2.9546 \n",
            "[12/25][384/391] Loss_D: 0.5485 Loss_G: 2.1109 \n",
            "[12/25][385/391] Loss_D: 0.5397 Loss_G: 3.8749 \n",
            "[12/25][386/391] Loss_D: 0.7196 Loss_G: 1.5695 \n",
            "[12/25][387/391] Loss_D: 0.6332 Loss_G: 3.7451 \n",
            "[12/25][388/391] Loss_D: 0.2595 Loss_G: 4.1973 \n",
            "[12/25][389/391] Loss_D: 0.5678 Loss_G: 1.5804 \n",
            "[12/25][390/391] Loss_D: 0.6119 Loss_G: 3.8012 \n",
            "[13/25][0/391] Loss_D: 0.4054 Loss_G: 2.8396 \n",
            "saving the output\n",
            "[13/25][1/391] Loss_D: 0.4469 Loss_G: 3.5007 \n",
            "[13/25][2/391] Loss_D: 0.4997 Loss_G: 2.4288 \n",
            "[13/25][3/391] Loss_D: 0.3462 Loss_G: 3.0177 \n",
            "[13/25][4/391] Loss_D: 0.4604 Loss_G: 2.0120 \n",
            "[13/25][5/391] Loss_D: 0.4845 Loss_G: 3.0448 \n",
            "[13/25][6/391] Loss_D: 0.4171 Loss_G: 2.8441 \n",
            "[13/25][7/391] Loss_D: 0.3284 Loss_G: 3.0293 \n",
            "[13/25][8/391] Loss_D: 0.3870 Loss_G: 3.0078 \n",
            "[13/25][9/391] Loss_D: 0.5473 Loss_G: 2.0708 \n",
            "[13/25][10/391] Loss_D: 0.4645 Loss_G: 3.3456 \n",
            "[13/25][11/391] Loss_D: 0.5410 Loss_G: 1.8401 \n",
            "[13/25][12/391] Loss_D: 0.5950 Loss_G: 3.5952 \n",
            "[13/25][13/391] Loss_D: 0.3468 Loss_G: 3.1871 \n",
            "[13/25][14/391] Loss_D: 0.3787 Loss_G: 2.2180 \n",
            "[13/25][15/391] Loss_D: 0.3410 Loss_G: 3.1711 \n",
            "[13/25][16/391] Loss_D: 0.3644 Loss_G: 3.0730 \n",
            "[13/25][17/391] Loss_D: 0.4366 Loss_G: 2.3059 \n",
            "[13/25][18/391] Loss_D: 0.4014 Loss_G: 3.4686 \n",
            "[13/25][19/391] Loss_D: 0.3665 Loss_G: 2.5117 \n",
            "[13/25][20/391] Loss_D: 0.4425 Loss_G: 2.4905 \n",
            "[13/25][21/391] Loss_D: 0.3407 Loss_G: 2.8876 \n",
            "[13/25][22/391] Loss_D: 0.3742 Loss_G: 3.0532 \n",
            "[13/25][23/391] Loss_D: 0.2198 Loss_G: 3.3328 \n",
            "[13/25][24/391] Loss_D: 0.3669 Loss_G: 2.5820 \n",
            "[13/25][25/391] Loss_D: 0.4806 Loss_G: 3.4488 \n",
            "[13/25][26/391] Loss_D: 0.3332 Loss_G: 2.7707 \n",
            "[13/25][27/391] Loss_D: 0.6159 Loss_G: 1.2959 \n",
            "[13/25][28/391] Loss_D: 0.7734 Loss_G: 4.7988 \n",
            "[13/25][29/391] Loss_D: 0.7285 Loss_G: 2.0064 \n",
            "[13/25][30/391] Loss_D: 0.4032 Loss_G: 3.0680 \n",
            "[13/25][31/391] Loss_D: 0.3189 Loss_G: 3.8170 \n",
            "[13/25][32/391] Loss_D: 0.4000 Loss_G: 2.4050 \n",
            "[13/25][33/391] Loss_D: 0.3849 Loss_G: 2.7191 \n",
            "[13/25][34/391] Loss_D: 0.3622 Loss_G: 3.1795 \n",
            "[13/25][35/391] Loss_D: 0.3803 Loss_G: 2.8659 \n",
            "[13/25][36/391] Loss_D: 0.3299 Loss_G: 2.7152 \n",
            "[13/25][37/391] Loss_D: 0.2466 Loss_G: 3.3280 \n",
            "[13/25][38/391] Loss_D: 0.4469 Loss_G: 2.3036 \n",
            "[13/25][39/391] Loss_D: 0.3744 Loss_G: 3.0616 \n",
            "[13/25][40/391] Loss_D: 0.3785 Loss_G: 3.0737 \n",
            "[13/25][41/391] Loss_D: 0.2628 Loss_G: 2.9336 \n",
            "[13/25][42/391] Loss_D: 0.2630 Loss_G: 2.7750 \n",
            "[13/25][43/391] Loss_D: 0.2832 Loss_G: 2.9233 \n",
            "[13/25][44/391] Loss_D: 0.2394 Loss_G: 3.3310 \n",
            "[13/25][45/391] Loss_D: 0.3055 Loss_G: 3.0043 \n",
            "[13/25][46/391] Loss_D: 0.3327 Loss_G: 2.5691 \n",
            "[13/25][47/391] Loss_D: 0.3522 Loss_G: 2.8733 \n",
            "[13/25][48/391] Loss_D: 0.4181 Loss_G: 2.7812 \n",
            "[13/25][49/391] Loss_D: 0.4302 Loss_G: 2.4183 \n",
            "[13/25][50/391] Loss_D: 0.5553 Loss_G: 4.7857 \n",
            "[13/25][51/391] Loss_D: 0.8461 Loss_G: 1.7296 \n",
            "[13/25][52/391] Loss_D: 0.6194 Loss_G: 3.9082 \n",
            "[13/25][53/391] Loss_D: 0.6001 Loss_G: 1.7131 \n",
            "[13/25][54/391] Loss_D: 0.4692 Loss_G: 3.3920 \n",
            "[13/25][55/391] Loss_D: 0.4163 Loss_G: 2.8906 \n",
            "[13/25][56/391] Loss_D: 0.4123 Loss_G: 1.9964 \n",
            "[13/25][57/391] Loss_D: 0.5001 Loss_G: 4.6430 \n",
            "[13/25][58/391] Loss_D: 0.4745 Loss_G: 2.6593 \n",
            "[13/25][59/391] Loss_D: 0.2557 Loss_G: 3.0222 \n",
            "[13/25][60/391] Loss_D: 0.4147 Loss_G: 3.0044 \n",
            "[13/25][61/391] Loss_D: 0.3589 Loss_G: 2.7274 \n",
            "[13/25][62/391] Loss_D: 0.3181 Loss_G: 3.6834 \n",
            "[13/25][63/391] Loss_D: 0.4393 Loss_G: 2.0203 \n",
            "[13/25][64/391] Loss_D: 0.3458 Loss_G: 3.7698 \n",
            "[13/25][65/391] Loss_D: 0.3003 Loss_G: 3.1778 \n",
            "[13/25][66/391] Loss_D: 0.2422 Loss_G: 2.8392 \n",
            "[13/25][67/391] Loss_D: 0.4178 Loss_G: 2.6854 \n",
            "[13/25][68/391] Loss_D: 0.4694 Loss_G: 2.5139 \n",
            "[13/25][69/391] Loss_D: 0.2599 Loss_G: 3.4144 \n",
            "[13/25][70/391] Loss_D: 0.4275 Loss_G: 2.1699 \n",
            "[13/25][71/391] Loss_D: 0.3261 Loss_G: 2.5320 \n",
            "[13/25][72/391] Loss_D: 0.3046 Loss_G: 3.1852 \n",
            "[13/25][73/391] Loss_D: 0.4363 Loss_G: 2.8549 \n",
            "[13/25][74/391] Loss_D: 0.3612 Loss_G: 2.7997 \n",
            "[13/25][75/391] Loss_D: 0.3226 Loss_G: 2.8840 \n",
            "[13/25][76/391] Loss_D: 0.2203 Loss_G: 3.1046 \n",
            "[13/25][77/391] Loss_D: 0.2824 Loss_G: 2.8212 \n",
            "[13/25][78/391] Loss_D: 0.2898 Loss_G: 2.8143 \n",
            "[13/25][79/391] Loss_D: 0.2936 Loss_G: 3.2498 \n",
            "[13/25][80/391] Loss_D: 0.4600 Loss_G: 1.7581 \n",
            "[13/25][81/391] Loss_D: 0.3779 Loss_G: 3.1855 \n",
            "[13/25][82/391] Loss_D: 0.2414 Loss_G: 3.4732 \n",
            "[13/25][83/391] Loss_D: 0.3909 Loss_G: 2.0103 \n",
            "[13/25][84/391] Loss_D: 0.3521 Loss_G: 3.9806 \n",
            "[13/25][85/391] Loss_D: 0.5190 Loss_G: 1.8316 \n",
            "[13/25][86/391] Loss_D: 0.3953 Loss_G: 2.8440 \n",
            "[13/25][87/391] Loss_D: 0.2694 Loss_G: 4.1392 \n",
            "[13/25][88/391] Loss_D: 0.5748 Loss_G: 1.6449 \n",
            "[13/25][89/391] Loss_D: 0.3762 Loss_G: 2.6840 \n",
            "[13/25][90/391] Loss_D: 0.3426 Loss_G: 4.0084 \n",
            "[13/25][91/391] Loss_D: 0.3879 Loss_G: 2.3704 \n",
            "[13/25][92/391] Loss_D: 0.3291 Loss_G: 2.9554 \n",
            "[13/25][93/391] Loss_D: 0.3085 Loss_G: 2.8002 \n",
            "[13/25][94/391] Loss_D: 0.4108 Loss_G: 2.7471 \n",
            "[13/25][95/391] Loss_D: 0.3950 Loss_G: 2.4585 \n",
            "[13/25][96/391] Loss_D: 0.2849 Loss_G: 3.3581 \n",
            "[13/25][97/391] Loss_D: 0.2304 Loss_G: 3.2659 \n",
            "[13/25][98/391] Loss_D: 0.3282 Loss_G: 2.0140 \n",
            "[13/25][99/391] Loss_D: 0.5041 Loss_G: 3.7518 \n",
            "[13/25][100/391] Loss_D: 0.5436 Loss_G: 1.7688 \n",
            "saving the output\n",
            "[13/25][101/391] Loss_D: 0.4934 Loss_G: 4.2415 \n",
            "[13/25][102/391] Loss_D: 0.4383 Loss_G: 2.4811 \n",
            "[13/25][103/391] Loss_D: 0.3121 Loss_G: 2.4488 \n",
            "[13/25][104/391] Loss_D: 0.3604 Loss_G: 4.4191 \n",
            "[13/25][105/391] Loss_D: 0.4571 Loss_G: 2.2430 \n",
            "[13/25][106/391] Loss_D: 0.3862 Loss_G: 2.9620 \n",
            "[13/25][107/391] Loss_D: 0.3386 Loss_G: 2.8894 \n",
            "[13/25][108/391] Loss_D: 0.5097 Loss_G: 1.8987 \n",
            "[13/25][109/391] Loss_D: 0.4629 Loss_G: 3.8633 \n",
            "[13/25][110/391] Loss_D: 0.4322 Loss_G: 2.5769 \n",
            "[13/25][111/391] Loss_D: 0.4852 Loss_G: 2.7151 \n",
            "[13/25][112/391] Loss_D: 0.2851 Loss_G: 3.1295 \n",
            "[13/25][113/391] Loss_D: 0.3795 Loss_G: 1.8657 \n",
            "[13/25][114/391] Loss_D: 0.4604 Loss_G: 3.6925 \n",
            "[13/25][115/391] Loss_D: 0.4879 Loss_G: 2.3712 \n",
            "[13/25][116/391] Loss_D: 0.2638 Loss_G: 3.4967 \n",
            "[13/25][117/391] Loss_D: 0.4067 Loss_G: 2.0268 \n",
            "[13/25][118/391] Loss_D: 0.6097 Loss_G: 4.0593 \n",
            "[13/25][119/391] Loss_D: 0.3223 Loss_G: 3.1559 \n",
            "[13/25][120/391] Loss_D: 0.3733 Loss_G: 1.8228 \n",
            "[13/25][121/391] Loss_D: 0.5255 Loss_G: 4.4410 \n",
            "[13/25][122/391] Loss_D: 0.4612 Loss_G: 2.4541 \n",
            "[13/25][123/391] Loss_D: 0.4158 Loss_G: 2.0589 \n",
            "[13/25][124/391] Loss_D: 0.4883 Loss_G: 4.2320 \n",
            "[13/25][125/391] Loss_D: 0.5961 Loss_G: 1.6286 \n",
            "[13/25][126/391] Loss_D: 0.4810 Loss_G: 4.0982 \n",
            "[13/25][127/391] Loss_D: 0.5910 Loss_G: 1.8090 \n",
            "[13/25][128/391] Loss_D: 0.5208 Loss_G: 3.5429 \n",
            "[13/25][129/391] Loss_D: 0.2760 Loss_G: 3.2497 \n",
            "[13/25][130/391] Loss_D: 0.3216 Loss_G: 2.5894 \n",
            "[13/25][131/391] Loss_D: 0.4795 Loss_G: 2.8428 \n",
            "[13/25][132/391] Loss_D: 0.2674 Loss_G: 3.1876 \n",
            "[13/25][133/391] Loss_D: 0.3621 Loss_G: 2.7479 \n",
            "[13/25][134/391] Loss_D: 0.4268 Loss_G: 2.9917 \n",
            "[13/25][135/391] Loss_D: 0.4416 Loss_G: 1.9905 \n",
            "[13/25][136/391] Loss_D: 0.4292 Loss_G: 3.9891 \n",
            "[13/25][137/391] Loss_D: 0.4027 Loss_G: 2.1485 \n",
            "[13/25][138/391] Loss_D: 0.2924 Loss_G: 3.1341 \n",
            "[13/25][139/391] Loss_D: 0.4212 Loss_G: 3.0375 \n",
            "[13/25][140/391] Loss_D: 0.3780 Loss_G: 2.3303 \n",
            "[13/25][141/391] Loss_D: 0.3511 Loss_G: 3.2409 \n",
            "[13/25][142/391] Loss_D: 0.3834 Loss_G: 2.4676 \n",
            "[13/25][143/391] Loss_D: 0.3529 Loss_G: 2.5981 \n",
            "[13/25][144/391] Loss_D: 0.4820 Loss_G: 2.9106 \n",
            "[13/25][145/391] Loss_D: 0.3618 Loss_G: 2.6158 \n",
            "[13/25][146/391] Loss_D: 0.3904 Loss_G: 2.8013 \n",
            "[13/25][147/391] Loss_D: 0.3240 Loss_G: 2.6586 \n",
            "[13/25][148/391] Loss_D: 0.3175 Loss_G: 3.0458 \n",
            "[13/25][149/391] Loss_D: 0.3514 Loss_G: 3.1847 \n",
            "[13/25][150/391] Loss_D: 0.4394 Loss_G: 1.7995 \n",
            "[13/25][151/391] Loss_D: 0.4552 Loss_G: 3.6702 \n",
            "[13/25][152/391] Loss_D: 0.2302 Loss_G: 3.6080 \n",
            "[13/25][153/391] Loss_D: 0.3646 Loss_G: 1.9470 \n",
            "[13/25][154/391] Loss_D: 0.5015 Loss_G: 4.3085 \n",
            "[13/25][155/391] Loss_D: 0.9718 Loss_G: 0.9148 \n",
            "[13/25][156/391] Loss_D: 1.1877 Loss_G: 6.0704 \n",
            "[13/25][157/391] Loss_D: 0.9852 Loss_G: 0.5886 \n",
            "[13/25][158/391] Loss_D: 1.7510 Loss_G: 6.7897 \n",
            "[13/25][159/391] Loss_D: 2.4683 Loss_G: 0.5088 \n",
            "[13/25][160/391] Loss_D: 1.4191 Loss_G: 6.1403 \n",
            "[13/25][161/391] Loss_D: 1.2108 Loss_G: 1.2963 \n",
            "[13/25][162/391] Loss_D: 1.4106 Loss_G: 5.1909 \n",
            "[13/25][163/391] Loss_D: 0.8417 Loss_G: 2.1864 \n",
            "[13/25][164/391] Loss_D: 0.6678 Loss_G: 3.7750 \n",
            "[13/25][165/391] Loss_D: 0.5990 Loss_G: 1.9895 \n",
            "[13/25][166/391] Loss_D: 0.8236 Loss_G: 4.2218 \n",
            "[13/25][167/391] Loss_D: 0.5090 Loss_G: 2.8045 \n",
            "[13/25][168/391] Loss_D: 0.5905 Loss_G: 2.0211 \n",
            "[13/25][169/391] Loss_D: 1.1234 Loss_G: 6.6187 \n",
            "[13/25][170/391] Loss_D: 2.4501 Loss_G: 0.8335 \n",
            "[13/25][171/391] Loss_D: 1.2808 Loss_G: 4.6108 \n",
            "[13/25][172/391] Loss_D: 0.5386 Loss_G: 2.1465 \n",
            "[13/25][173/391] Loss_D: 0.5289 Loss_G: 4.9531 \n",
            "[13/25][174/391] Loss_D: 0.8584 Loss_G: 0.7829 \n",
            "[13/25][175/391] Loss_D: 1.2294 Loss_G: 6.5344 \n",
            "[13/25][176/391] Loss_D: 1.1077 Loss_G: 1.8318 \n",
            "[13/25][177/391] Loss_D: 0.7260 Loss_G: 3.7356 \n",
            "[13/25][178/391] Loss_D: 0.3524 Loss_G: 3.6704 \n",
            "[13/25][179/391] Loss_D: 0.8171 Loss_G: 0.7416 \n",
            "[13/25][180/391] Loss_D: 1.6409 Loss_G: 5.1797 \n",
            "[13/25][181/391] Loss_D: 1.1910 Loss_G: 0.3158 \n",
            "[13/25][182/391] Loss_D: 2.0732 Loss_G: 7.5524 \n",
            "[13/25][183/391] Loss_D: 2.5854 Loss_G: 1.7122 \n",
            "[13/25][184/391] Loss_D: 0.7325 Loss_G: 2.5414 \n",
            "[13/25][185/391] Loss_D: 0.5353 Loss_G: 4.1882 \n",
            "[13/25][186/391] Loss_D: 0.8332 Loss_G: 1.5545 \n",
            "[13/25][187/391] Loss_D: 0.6917 Loss_G: 4.0138 \n",
            "[13/25][188/391] Loss_D: 1.0010 Loss_G: 1.4268 \n",
            "[13/25][189/391] Loss_D: 1.0516 Loss_G: 3.6360 \n",
            "[13/25][190/391] Loss_D: 0.6429 Loss_G: 2.9287 \n",
            "[13/25][191/391] Loss_D: 0.4846 Loss_G: 2.3084 \n",
            "[13/25][192/391] Loss_D: 0.6311 Loss_G: 4.0939 \n",
            "[13/25][193/391] Loss_D: 0.6388 Loss_G: 1.6061 \n",
            "[13/25][194/391] Loss_D: 0.6549 Loss_G: 4.2371 \n",
            "[13/25][195/391] Loss_D: 0.8560 Loss_G: 1.7795 \n",
            "[13/25][196/391] Loss_D: 0.8051 Loss_G: 4.3830 \n",
            "[13/25][197/391] Loss_D: 0.7805 Loss_G: 1.3131 \n",
            "[13/25][198/391] Loss_D: 1.0121 Loss_G: 5.5178 \n",
            "[13/25][199/391] Loss_D: 0.8062 Loss_G: 2.5259 \n",
            "[13/25][200/391] Loss_D: 0.4352 Loss_G: 2.1586 \n",
            "saving the output\n",
            "[13/25][201/391] Loss_D: 0.4808 Loss_G: 4.0091 \n",
            "[13/25][202/391] Loss_D: 0.3950 Loss_G: 3.2366 \n",
            "[13/25][203/391] Loss_D: 0.4675 Loss_G: 2.3025 \n",
            "[13/25][204/391] Loss_D: 0.4971 Loss_G: 2.9910 \n",
            "[13/25][205/391] Loss_D: 0.4562 Loss_G: 2.9889 \n",
            "[13/25][206/391] Loss_D: 0.5452 Loss_G: 2.0347 \n",
            "[13/25][207/391] Loss_D: 0.7515 Loss_G: 3.0390 \n",
            "[13/25][208/391] Loss_D: 0.5225 Loss_G: 2.5341 \n",
            "[13/25][209/391] Loss_D: 0.4470 Loss_G: 2.7438 \n",
            "[13/25][210/391] Loss_D: 0.4808 Loss_G: 3.3127 \n",
            "[13/25][211/391] Loss_D: 0.5796 Loss_G: 1.9198 \n",
            "[13/25][212/391] Loss_D: 0.5228 Loss_G: 3.9254 \n",
            "[13/25][213/391] Loss_D: 0.5132 Loss_G: 2.3567 \n",
            "[13/25][214/391] Loss_D: 0.4409 Loss_G: 2.3734 \n",
            "[13/25][215/391] Loss_D: 0.4859 Loss_G: 4.2240 \n",
            "[13/25][216/391] Loss_D: 0.6042 Loss_G: 2.1856 \n",
            "[13/25][217/391] Loss_D: 0.3539 Loss_G: 3.0700 \n",
            "[13/25][218/391] Loss_D: 0.5042 Loss_G: 2.3986 \n",
            "[13/25][219/391] Loss_D: 0.4376 Loss_G: 3.3090 \n",
            "[13/25][220/391] Loss_D: 0.3524 Loss_G: 2.6071 \n",
            "[13/25][221/391] Loss_D: 0.3726 Loss_G: 3.4278 \n",
            "[13/25][222/391] Loss_D: 0.3793 Loss_G: 2.8712 \n",
            "[13/25][223/391] Loss_D: 0.4341 Loss_G: 2.7083 \n",
            "[13/25][224/391] Loss_D: 0.6546 Loss_G: 2.8002 \n",
            "[13/25][225/391] Loss_D: 0.4498 Loss_G: 2.8052 \n",
            "[13/25][226/391] Loss_D: 0.5366 Loss_G: 1.6973 \n",
            "[13/25][227/391] Loss_D: 0.6327 Loss_G: 4.7000 \n",
            "[13/25][228/391] Loss_D: 0.8299 Loss_G: 1.3539 \n",
            "[13/25][229/391] Loss_D: 0.6726 Loss_G: 4.5594 \n",
            "[13/25][230/391] Loss_D: 0.5569 Loss_G: 2.1831 \n",
            "[13/25][231/391] Loss_D: 0.3967 Loss_G: 2.7875 \n",
            "[13/25][232/391] Loss_D: 0.3507 Loss_G: 4.0186 \n",
            "[13/25][233/391] Loss_D: 0.5146 Loss_G: 2.1545 \n",
            "[13/25][234/391] Loss_D: 0.5597 Loss_G: 1.8019 \n",
            "[13/25][235/391] Loss_D: 0.6325 Loss_G: 4.7667 \n",
            "[13/25][236/391] Loss_D: 0.6798 Loss_G: 2.1451 \n",
            "[13/25][237/391] Loss_D: 0.4372 Loss_G: 3.1858 \n",
            "[13/25][238/391] Loss_D: 0.6400 Loss_G: 2.0397 \n",
            "[13/25][239/391] Loss_D: 0.4309 Loss_G: 3.2404 \n",
            "[13/25][240/391] Loss_D: 0.3733 Loss_G: 2.5324 \n",
            "[13/25][241/391] Loss_D: 0.3412 Loss_G: 2.6656 \n",
            "[13/25][242/391] Loss_D: 0.2819 Loss_G: 3.3610 \n",
            "[13/25][243/391] Loss_D: 0.6523 Loss_G: 1.5032 \n",
            "[13/25][244/391] Loss_D: 0.6273 Loss_G: 4.2006 \n",
            "[13/25][245/391] Loss_D: 0.4653 Loss_G: 2.4244 \n",
            "[13/25][246/391] Loss_D: 0.4046 Loss_G: 2.2250 \n",
            "[13/25][247/391] Loss_D: 0.5189 Loss_G: 4.3913 \n",
            "[13/25][248/391] Loss_D: 0.7748 Loss_G: 1.6059 \n",
            "[13/25][249/391] Loss_D: 0.4573 Loss_G: 3.0368 \n",
            "[13/25][250/391] Loss_D: 0.3200 Loss_G: 3.4639 \n",
            "[13/25][251/391] Loss_D: 0.3016 Loss_G: 2.8701 \n",
            "[13/25][252/391] Loss_D: 0.4608 Loss_G: 1.6386 \n",
            "[13/25][253/391] Loss_D: 0.6771 Loss_G: 4.7100 \n",
            "[13/25][254/391] Loss_D: 0.7996 Loss_G: 1.5549 \n",
            "[13/25][255/391] Loss_D: 0.7142 Loss_G: 4.2241 \n",
            "[13/25][256/391] Loss_D: 0.5167 Loss_G: 2.4383 \n",
            "[13/25][257/391] Loss_D: 0.5130 Loss_G: 2.8102 \n",
            "[13/25][258/391] Loss_D: 0.4546 Loss_G: 2.8461 \n",
            "[13/25][259/391] Loss_D: 0.3488 Loss_G: 3.2457 \n",
            "[13/25][260/391] Loss_D: 0.3438 Loss_G: 2.5753 \n",
            "[13/25][261/391] Loss_D: 0.4730 Loss_G: 3.2846 \n",
            "[13/25][262/391] Loss_D: 0.5477 Loss_G: 2.2193 \n",
            "[13/25][263/391] Loss_D: 0.5931 Loss_G: 1.6797 \n",
            "[13/25][264/391] Loss_D: 0.7150 Loss_G: 5.1987 \n",
            "[13/25][265/391] Loss_D: 1.1484 Loss_G: 1.1359 \n",
            "[13/25][266/391] Loss_D: 0.9082 Loss_G: 5.0922 \n",
            "[13/25][267/391] Loss_D: 0.6973 Loss_G: 2.2098 \n",
            "[13/25][268/391] Loss_D: 0.3196 Loss_G: 3.0818 \n",
            "[13/25][269/391] Loss_D: 0.3020 Loss_G: 3.6954 \n",
            "[13/25][270/391] Loss_D: 0.3930 Loss_G: 2.6647 \n",
            "[13/25][271/391] Loss_D: 0.4085 Loss_G: 2.2678 \n",
            "[13/25][272/391] Loss_D: 0.5991 Loss_G: 3.3110 \n",
            "[13/25][273/391] Loss_D: 0.3810 Loss_G: 2.7215 \n",
            "[13/25][274/391] Loss_D: 0.5340 Loss_G: 2.4647 \n",
            "[13/25][275/391] Loss_D: 0.4352 Loss_G: 2.5926 \n",
            "[13/25][276/391] Loss_D: 0.4204 Loss_G: 3.5626 \n",
            "[13/25][277/391] Loss_D: 0.4460 Loss_G: 2.3257 \n",
            "[13/25][278/391] Loss_D: 0.5523 Loss_G: 2.5459 \n",
            "[13/25][279/391] Loss_D: 0.4028 Loss_G: 2.8704 \n",
            "[13/25][280/391] Loss_D: 0.4207 Loss_G: 2.1544 \n",
            "[13/25][281/391] Loss_D: 0.3686 Loss_G: 3.5956 \n",
            "[13/25][282/391] Loss_D: 0.4603 Loss_G: 2.0148 \n",
            "[13/25][283/391] Loss_D: 0.3407 Loss_G: 3.2130 \n",
            "[13/25][284/391] Loss_D: 0.3518 Loss_G: 3.5933 \n",
            "[13/25][285/391] Loss_D: 0.4493 Loss_G: 1.8094 \n",
            "[13/25][286/391] Loss_D: 0.4851 Loss_G: 4.2229 \n",
            "[13/25][287/391] Loss_D: 0.5617 Loss_G: 2.0651 \n",
            "[13/25][288/391] Loss_D: 0.4212 Loss_G: 3.0190 \n",
            "[13/25][289/391] Loss_D: 0.4232 Loss_G: 2.8739 \n",
            "[13/25][290/391] Loss_D: 0.3577 Loss_G: 2.5599 \n",
            "[13/25][291/391] Loss_D: 0.4849 Loss_G: 2.9817 \n",
            "[13/25][292/391] Loss_D: 0.4224 Loss_G: 2.2224 \n",
            "[13/25][293/391] Loss_D: 0.4565 Loss_G: 3.0961 \n",
            "[13/25][294/391] Loss_D: 0.3832 Loss_G: 2.3998 \n",
            "[13/25][295/391] Loss_D: 0.2899 Loss_G: 3.4691 \n",
            "[13/25][296/391] Loss_D: 0.4755 Loss_G: 1.9227 \n",
            "[13/25][297/391] Loss_D: 0.6244 Loss_G: 3.8215 \n",
            "[13/25][298/391] Loss_D: 0.4627 Loss_G: 2.3790 \n",
            "[13/25][299/391] Loss_D: 0.3979 Loss_G: 2.2647 \n",
            "[13/25][300/391] Loss_D: 0.2933 Loss_G: 3.1264 \n",
            "saving the output\n",
            "[13/25][301/391] Loss_D: 0.2633 Loss_G: 3.3702 \n",
            "[13/25][302/391] Loss_D: 0.2580 Loss_G: 3.3617 \n",
            "[13/25][303/391] Loss_D: 0.3511 Loss_G: 2.7625 \n",
            "[13/25][304/391] Loss_D: 0.5568 Loss_G: 1.5648 \n",
            "[13/25][305/391] Loss_D: 0.7866 Loss_G: 4.0557 \n",
            "[13/25][306/391] Loss_D: 0.7050 Loss_G: 1.4900 \n",
            "[13/25][307/391] Loss_D: 0.6214 Loss_G: 4.4810 \n",
            "[13/25][308/391] Loss_D: 0.7891 Loss_G: 1.4773 \n",
            "[13/25][309/391] Loss_D: 0.7730 Loss_G: 4.7245 \n",
            "[13/25][310/391] Loss_D: 0.3043 Loss_G: 4.1461 \n",
            "[13/25][311/391] Loss_D: 0.3583 Loss_G: 1.9560 \n",
            "[13/25][312/391] Loss_D: 0.5259 Loss_G: 3.5915 \n",
            "[13/25][313/391] Loss_D: 0.2231 Loss_G: 3.6720 \n",
            "[13/25][314/391] Loss_D: 0.5890 Loss_G: 1.2417 \n",
            "[13/25][315/391] Loss_D: 0.7066 Loss_G: 4.4111 \n",
            "[13/25][316/391] Loss_D: 0.3903 Loss_G: 3.2521 \n",
            "[13/25][317/391] Loss_D: 0.4038 Loss_G: 1.8489 \n",
            "[13/25][318/391] Loss_D: 0.5199 Loss_G: 3.7858 \n",
            "[13/25][319/391] Loss_D: 0.5918 Loss_G: 1.8027 \n",
            "[13/25][320/391] Loss_D: 0.5025 Loss_G: 3.2480 \n",
            "[13/25][321/391] Loss_D: 0.4660 Loss_G: 2.5931 \n",
            "[13/25][322/391] Loss_D: 0.3343 Loss_G: 2.8251 \n",
            "[13/25][323/391] Loss_D: 0.3860 Loss_G: 2.4132 \n",
            "[13/25][324/391] Loss_D: 0.4141 Loss_G: 3.3402 \n",
            "[13/25][325/391] Loss_D: 0.3454 Loss_G: 2.5638 \n",
            "[13/25][326/391] Loss_D: 0.4897 Loss_G: 3.3651 \n",
            "[13/25][327/391] Loss_D: 0.3714 Loss_G: 2.5852 \n",
            "[13/25][328/391] Loss_D: 0.3755 Loss_G: 2.8339 \n",
            "[13/25][329/391] Loss_D: 0.3504 Loss_G: 3.1647 \n",
            "[13/25][330/391] Loss_D: 0.3346 Loss_G: 2.4893 \n",
            "[13/25][331/391] Loss_D: 0.5638 Loss_G: 1.8767 \n",
            "[13/25][332/391] Loss_D: 0.5396 Loss_G: 3.7932 \n",
            "[13/25][333/391] Loss_D: 0.3397 Loss_G: 2.8561 \n",
            "[13/25][334/391] Loss_D: 0.3803 Loss_G: 2.8301 \n",
            "[13/25][335/391] Loss_D: 0.3041 Loss_G: 2.9313 \n",
            "[13/25][336/391] Loss_D: 0.3139 Loss_G: 2.9334 \n",
            "[13/25][337/391] Loss_D: 0.3317 Loss_G: 2.8809 \n",
            "[13/25][338/391] Loss_D: 0.2562 Loss_G: 3.2418 \n",
            "[13/25][339/391] Loss_D: 0.4364 Loss_G: 2.2052 \n",
            "[13/25][340/391] Loss_D: 0.4009 Loss_G: 3.3610 \n",
            "[13/25][341/391] Loss_D: 0.3928 Loss_G: 2.4639 \n",
            "[13/25][342/391] Loss_D: 0.4241 Loss_G: 3.0007 \n",
            "[13/25][343/391] Loss_D: 0.4133 Loss_G: 2.2813 \n",
            "[13/25][344/391] Loss_D: 0.4158 Loss_G: 2.7353 \n",
            "[13/25][345/391] Loss_D: 0.2893 Loss_G: 3.5136 \n",
            "[13/25][346/391] Loss_D: 0.5283 Loss_G: 1.6036 \n",
            "[13/25][347/391] Loss_D: 0.7152 Loss_G: 4.0372 \n",
            "[13/25][348/391] Loss_D: 0.4962 Loss_G: 2.3538 \n",
            "[13/25][349/391] Loss_D: 0.4275 Loss_G: 3.4346 \n",
            "[13/25][350/391] Loss_D: 0.4648 Loss_G: 2.2791 \n",
            "[13/25][351/391] Loss_D: 0.4338 Loss_G: 2.7906 \n",
            "[13/25][352/391] Loss_D: 0.3054 Loss_G: 3.7000 \n",
            "[13/25][353/391] Loss_D: 0.5827 Loss_G: 1.2836 \n",
            "[13/25][354/391] Loss_D: 1.0497 Loss_G: 4.8452 \n",
            "[13/25][355/391] Loss_D: 0.8188 Loss_G: 1.4895 \n",
            "[13/25][356/391] Loss_D: 0.6940 Loss_G: 4.5882 \n",
            "[13/25][357/391] Loss_D: 0.4546 Loss_G: 2.1165 \n",
            "[13/25][358/391] Loss_D: 0.3900 Loss_G: 3.6801 \n",
            "[13/25][359/391] Loss_D: 0.3632 Loss_G: 2.3580 \n",
            "[13/25][360/391] Loss_D: 0.4179 Loss_G: 3.0380 \n",
            "[13/25][361/391] Loss_D: 0.3976 Loss_G: 2.3859 \n",
            "[13/25][362/391] Loss_D: 0.5027 Loss_G: 3.7475 \n",
            "[13/25][363/391] Loss_D: 0.6734 Loss_G: 1.2280 \n",
            "[13/25][364/391] Loss_D: 0.5923 Loss_G: 4.2870 \n",
            "[13/25][365/391] Loss_D: 0.6790 Loss_G: 1.4659 \n",
            "[13/25][366/391] Loss_D: 0.4921 Loss_G: 4.1300 \n",
            "[13/25][367/391] Loss_D: 0.3341 Loss_G: 2.6744 \n",
            "[13/25][368/391] Loss_D: 0.3628 Loss_G: 2.4750 \n",
            "[13/25][369/391] Loss_D: 0.4797 Loss_G: 2.6461 \n",
            "[13/25][370/391] Loss_D: 0.5840 Loss_G: 2.5620 \n",
            "[13/25][371/391] Loss_D: 0.2341 Loss_G: 3.3405 \n",
            "[13/25][372/391] Loss_D: 0.4863 Loss_G: 1.6479 \n",
            "[13/25][373/391] Loss_D: 0.5044 Loss_G: 4.0295 \n",
            "[13/25][374/391] Loss_D: 0.4607 Loss_G: 2.3313 \n",
            "[13/25][375/391] Loss_D: 0.4095 Loss_G: 2.5411 \n",
            "[13/25][376/391] Loss_D: 0.4302 Loss_G: 2.6134 \n",
            "[13/25][377/391] Loss_D: 0.3953 Loss_G: 2.5725 \n",
            "[13/25][378/391] Loss_D: 0.3906 Loss_G: 3.7573 \n",
            "[13/25][379/391] Loss_D: 0.5396 Loss_G: 1.6861 \n",
            "[13/25][380/391] Loss_D: 0.3814 Loss_G: 3.2237 \n",
            "[13/25][381/391] Loss_D: 0.2275 Loss_G: 3.7639 \n",
            "[13/25][382/391] Loss_D: 0.2864 Loss_G: 2.7911 \n",
            "[13/25][383/391] Loss_D: 0.4408 Loss_G: 2.6055 \n",
            "[13/25][384/391] Loss_D: 0.3225 Loss_G: 2.9288 \n",
            "[13/25][385/391] Loss_D: 0.4029 Loss_G: 2.1393 \n",
            "[13/25][386/391] Loss_D: 0.3936 Loss_G: 3.0849 \n",
            "[13/25][387/391] Loss_D: 0.3825 Loss_G: 2.9746 \n",
            "[13/25][388/391] Loss_D: 0.2868 Loss_G: 2.5362 \n",
            "[13/25][389/391] Loss_D: 0.4837 Loss_G: 3.8568 \n",
            "[13/25][390/391] Loss_D: 0.6140 Loss_G: 1.1972 \n",
            "[14/25][0/391] Loss_D: 1.0091 Loss_G: 6.7815 \n",
            "saving the output\n",
            "[14/25][1/391] Loss_D: 1.4868 Loss_G: 0.8847 \n",
            "[14/25][2/391] Loss_D: 1.0801 Loss_G: 6.1639 \n",
            "[14/25][3/391] Loss_D: 1.7069 Loss_G: 0.1162 \n",
            "[14/25][4/391] Loss_D: 2.9621 Loss_G: 6.0891 \n",
            "[14/25][5/391] Loss_D: 0.7525 Loss_G: 1.1012 \n",
            "[14/25][6/391] Loss_D: 1.3660 Loss_G: 7.5786 \n",
            "[14/25][7/391] Loss_D: 3.2771 Loss_G: 0.8016 \n",
            "[14/25][8/391] Loss_D: 1.5640 Loss_G: 5.0269 \n",
            "[14/25][9/391] Loss_D: 1.0832 Loss_G: 1.4132 \n",
            "[14/25][10/391] Loss_D: 1.0666 Loss_G: 5.3571 \n",
            "[14/25][11/391] Loss_D: 1.9019 Loss_G: 0.4171 \n",
            "[14/25][12/391] Loss_D: 1.5866 Loss_G: 4.9115 \n",
            "[14/25][13/391] Loss_D: 0.5842 Loss_G: 3.4112 \n",
            "[14/25][14/391] Loss_D: 0.4400 Loss_G: 1.9084 \n",
            "[14/25][15/391] Loss_D: 0.9791 Loss_G: 4.2445 \n",
            "[14/25][16/391] Loss_D: 0.9287 Loss_G: 1.4040 \n",
            "[14/25][17/391] Loss_D: 0.9153 Loss_G: 2.9800 \n",
            "[14/25][18/391] Loss_D: 0.6181 Loss_G: 2.7310 \n",
            "[14/25][19/391] Loss_D: 0.5867 Loss_G: 2.2345 \n",
            "[14/25][20/391] Loss_D: 0.7599 Loss_G: 3.1069 \n",
            "[14/25][21/391] Loss_D: 0.7165 Loss_G: 1.6828 \n",
            "[14/25][22/391] Loss_D: 0.6677 Loss_G: 3.9363 \n",
            "[14/25][23/391] Loss_D: 0.6342 Loss_G: 1.9154 \n",
            "[14/25][24/391] Loss_D: 0.4757 Loss_G: 2.7956 \n",
            "[14/25][25/391] Loss_D: 0.4405 Loss_G: 3.4988 \n",
            "[14/25][26/391] Loss_D: 0.6966 Loss_G: 1.5968 \n",
            "[14/25][27/391] Loss_D: 0.6269 Loss_G: 3.1068 \n",
            "[14/25][28/391] Loss_D: 0.3808 Loss_G: 2.8611 \n",
            "[14/25][29/391] Loss_D: 0.3761 Loss_G: 2.9616 \n",
            "[14/25][30/391] Loss_D: 0.3832 Loss_G: 3.0410 \n",
            "[14/25][31/391] Loss_D: 0.3581 Loss_G: 2.2418 \n",
            "[14/25][32/391] Loss_D: 0.5377 Loss_G: 4.0351 \n",
            "[14/25][33/391] Loss_D: 0.7336 Loss_G: 1.2040 \n",
            "[14/25][34/391] Loss_D: 0.8637 Loss_G: 4.4787 \n",
            "[14/25][35/391] Loss_D: 0.5081 Loss_G: 2.8029 \n",
            "[14/25][36/391] Loss_D: 0.4708 Loss_G: 2.3922 \n",
            "[14/25][37/391] Loss_D: 0.4561 Loss_G: 2.1375 \n",
            "[14/25][38/391] Loss_D: 0.6458 Loss_G: 5.1322 \n",
            "[14/25][39/391] Loss_D: 1.2063 Loss_G: 1.2006 \n",
            "[14/25][40/391] Loss_D: 0.8642 Loss_G: 3.8740 \n",
            "[14/25][41/391] Loss_D: 0.4633 Loss_G: 2.6975 \n",
            "[14/25][42/391] Loss_D: 0.4187 Loss_G: 2.4237 \n",
            "[14/25][43/391] Loss_D: 0.4281 Loss_G: 3.4084 \n",
            "[14/25][44/391] Loss_D: 0.3684 Loss_G: 2.6769 \n",
            "[14/25][45/391] Loss_D: 0.4122 Loss_G: 2.7016 \n",
            "[14/25][46/391] Loss_D: 0.5226 Loss_G: 2.3604 \n",
            "[14/25][47/391] Loss_D: 0.4873 Loss_G: 2.7835 \n",
            "[14/25][48/391] Loss_D: 0.5696 Loss_G: 2.1489 \n",
            "[14/25][49/391] Loss_D: 0.4303 Loss_G: 2.5536 \n",
            "[14/25][50/391] Loss_D: 0.3427 Loss_G: 3.3539 \n",
            "[14/25][51/391] Loss_D: 0.4268 Loss_G: 3.0591 \n",
            "[14/25][52/391] Loss_D: 0.4351 Loss_G: 2.1885 \n",
            "[14/25][53/391] Loss_D: 0.6011 Loss_G: 3.0694 \n",
            "[14/25][54/391] Loss_D: 0.2935 Loss_G: 3.3410 \n",
            "[14/25][55/391] Loss_D: 0.5336 Loss_G: 1.7205 \n",
            "[14/25][56/391] Loss_D: 0.5579 Loss_G: 3.5508 \n",
            "[14/25][57/391] Loss_D: 0.5152 Loss_G: 2.5878 \n",
            "[14/25][58/391] Loss_D: 0.5474 Loss_G: 1.5645 \n",
            "[14/25][59/391] Loss_D: 0.7390 Loss_G: 4.0259 \n",
            "[14/25][60/391] Loss_D: 0.4921 Loss_G: 2.6460 \n",
            "[14/25][61/391] Loss_D: 0.3557 Loss_G: 2.2442 \n",
            "[14/25][62/391] Loss_D: 0.3882 Loss_G: 2.7364 \n",
            "[14/25][63/391] Loss_D: 0.4097 Loss_G: 2.3242 \n",
            "[14/25][64/391] Loss_D: 0.4806 Loss_G: 3.2224 \n",
            "[14/25][65/391] Loss_D: 0.5108 Loss_G: 1.7518 \n",
            "[14/25][66/391] Loss_D: 0.4766 Loss_G: 3.1940 \n",
            "[14/25][67/391] Loss_D: 0.3766 Loss_G: 2.6597 \n",
            "[14/25][68/391] Loss_D: 0.4145 Loss_G: 2.2363 \n",
            "[14/25][69/391] Loss_D: 0.5601 Loss_G: 2.3997 \n",
            "[14/25][70/391] Loss_D: 0.3879 Loss_G: 2.6512 \n",
            "[14/25][71/391] Loss_D: 0.3436 Loss_G: 3.3126 \n",
            "[14/25][72/391] Loss_D: 0.4797 Loss_G: 2.0290 \n",
            "[14/25][73/391] Loss_D: 0.3881 Loss_G: 2.8127 \n",
            "[14/25][74/391] Loss_D: 0.4290 Loss_G: 2.9778 \n",
            "[14/25][75/391] Loss_D: 0.4589 Loss_G: 1.8140 \n",
            "[14/25][76/391] Loss_D: 0.5513 Loss_G: 3.7861 \n",
            "[14/25][77/391] Loss_D: 0.5509 Loss_G: 1.6737 \n",
            "[14/25][78/391] Loss_D: 0.5265 Loss_G: 4.0802 \n",
            "[14/25][79/391] Loss_D: 0.5786 Loss_G: 1.6824 \n",
            "[14/25][80/391] Loss_D: 0.5282 Loss_G: 2.9520 \n",
            "[14/25][81/391] Loss_D: 0.3226 Loss_G: 3.1437 \n",
            "[14/25][82/391] Loss_D: 0.3749 Loss_G: 2.5389 \n",
            "[14/25][83/391] Loss_D: 0.4982 Loss_G: 1.7194 \n",
            "[14/25][84/391] Loss_D: 0.5021 Loss_G: 3.6017 \n",
            "[14/25][85/391] Loss_D: 0.4807 Loss_G: 2.4020 \n",
            "[14/25][86/391] Loss_D: 0.4371 Loss_G: 2.2512 \n",
            "[14/25][87/391] Loss_D: 0.3820 Loss_G: 2.6648 \n",
            "[14/25][88/391] Loss_D: 0.3935 Loss_G: 2.2034 \n",
            "[14/25][89/391] Loss_D: 0.4226 Loss_G: 3.2598 \n",
            "[14/25][90/391] Loss_D: 0.3328 Loss_G: 2.7176 \n",
            "[14/25][91/391] Loss_D: 0.2455 Loss_G: 2.8677 \n",
            "[14/25][92/391] Loss_D: 0.3888 Loss_G: 2.6482 \n",
            "[14/25][93/391] Loss_D: 0.3934 Loss_G: 2.5350 \n",
            "[14/25][94/391] Loss_D: 0.4701 Loss_G: 1.8685 \n",
            "[14/25][95/391] Loss_D: 0.5393 Loss_G: 4.1401 \n",
            "[14/25][96/391] Loss_D: 0.5410 Loss_G: 2.2746 \n",
            "[14/25][97/391] Loss_D: 0.3049 Loss_G: 2.5955 \n",
            "[14/25][98/391] Loss_D: 0.4227 Loss_G: 2.8925 \n",
            "[14/25][99/391] Loss_D: 0.3483 Loss_G: 2.9923 \n",
            "[14/25][100/391] Loss_D: 0.4762 Loss_G: 1.9080 \n",
            "saving the output\n",
            "[14/25][101/391] Loss_D: 0.3391 Loss_G: 2.6574 \n",
            "[14/25][102/391] Loss_D: 0.5054 Loss_G: 4.1068 \n",
            "[14/25][103/391] Loss_D: 0.4991 Loss_G: 2.4674 \n",
            "[14/25][104/391] Loss_D: 0.5460 Loss_G: 2.3199 \n",
            "[14/25][105/391] Loss_D: 0.4763 Loss_G: 2.6659 \n",
            "[14/25][106/391] Loss_D: 0.4030 Loss_G: 3.3568 \n",
            "[14/25][107/391] Loss_D: 0.4133 Loss_G: 2.2478 \n",
            "[14/25][108/391] Loss_D: 0.4563 Loss_G: 2.8450 \n",
            "[14/25][109/391] Loss_D: 0.3525 Loss_G: 3.0317 \n",
            "[14/25][110/391] Loss_D: 0.3781 Loss_G: 2.3606 \n",
            "[14/25][111/391] Loss_D: 0.2801 Loss_G: 3.3917 \n",
            "[14/25][112/391] Loss_D: 0.4871 Loss_G: 1.5657 \n",
            "[14/25][113/391] Loss_D: 0.5785 Loss_G: 3.1030 \n",
            "[14/25][114/391] Loss_D: 0.4325 Loss_G: 2.7572 \n",
            "[14/25][115/391] Loss_D: 0.3459 Loss_G: 2.7299 \n",
            "[14/25][116/391] Loss_D: 0.4592 Loss_G: 2.7222 \n",
            "[14/25][117/391] Loss_D: 0.3958 Loss_G: 2.3411 \n",
            "[14/25][118/391] Loss_D: 0.3203 Loss_G: 2.9189 \n",
            "[14/25][119/391] Loss_D: 0.3636 Loss_G: 2.5365 \n",
            "[14/25][120/391] Loss_D: 0.3285 Loss_G: 3.7416 \n",
            "[14/25][121/391] Loss_D: 0.6579 Loss_G: 1.2760 \n",
            "[14/25][122/391] Loss_D: 0.4353 Loss_G: 2.8523 \n",
            "[14/25][123/391] Loss_D: 0.4307 Loss_G: 3.6914 \n",
            "[14/25][124/391] Loss_D: 0.6284 Loss_G: 1.3532 \n",
            "[14/25][125/391] Loss_D: 0.6008 Loss_G: 4.3649 \n",
            "[14/25][126/391] Loss_D: 0.7118 Loss_G: 1.3892 \n",
            "[14/25][127/391] Loss_D: 0.9467 Loss_G: 5.0721 \n",
            "[14/25][128/391] Loss_D: 1.2041 Loss_G: 0.4255 \n",
            "[14/25][129/391] Loss_D: 1.9122 Loss_G: 6.5317 \n",
            "[14/25][130/391] Loss_D: 2.0111 Loss_G: 0.3817 \n",
            "[14/25][131/391] Loss_D: 2.2885 Loss_G: 8.1504 \n",
            "[14/25][132/391] Loss_D: 2.8091 Loss_G: 0.0389 \n",
            "[14/25][133/391] Loss_D: 4.7804 Loss_G: 6.6660 \n",
            "[14/25][134/391] Loss_D: 2.9714 Loss_G: 0.4821 \n",
            "[14/25][135/391] Loss_D: 2.4353 Loss_G: 5.5913 \n",
            "[14/25][136/391] Loss_D: 1.9150 Loss_G: 0.6239 \n",
            "[14/25][137/391] Loss_D: 1.8614 Loss_G: 4.4553 \n",
            "[14/25][138/391] Loss_D: 1.2341 Loss_G: 1.3596 \n",
            "[14/25][139/391] Loss_D: 1.1930 Loss_G: 2.5138 \n",
            "[14/25][140/391] Loss_D: 0.8992 Loss_G: 2.0630 \n",
            "[14/25][141/391] Loss_D: 1.1877 Loss_G: 2.7120 \n",
            "[14/25][142/391] Loss_D: 0.7593 Loss_G: 2.2232 \n",
            "[14/25][143/391] Loss_D: 0.7926 Loss_G: 2.2427 \n",
            "[14/25][144/391] Loss_D: 0.4684 Loss_G: 3.5040 \n",
            "[14/25][145/391] Loss_D: 0.8006 Loss_G: 1.3917 \n",
            "[14/25][146/391] Loss_D: 1.1330 Loss_G: 5.0481 \n",
            "[14/25][147/391] Loss_D: 2.2980 Loss_G: 0.5419 \n",
            "[14/25][148/391] Loss_D: 2.1450 Loss_G: 4.2819 \n",
            "[14/25][149/391] Loss_D: 0.6995 Loss_G: 2.7974 \n",
            "[14/25][150/391] Loss_D: 0.6462 Loss_G: 1.5803 \n",
            "[14/25][151/391] Loss_D: 0.8792 Loss_G: 4.1436 \n",
            "[14/25][152/391] Loss_D: 1.3799 Loss_G: 0.8557 \n",
            "[14/25][153/391] Loss_D: 1.5760 Loss_G: 5.4781 \n",
            "[14/25][154/391] Loss_D: 1.1564 Loss_G: 2.2139 \n",
            "[14/25][155/391] Loss_D: 0.4676 Loss_G: 2.9059 \n",
            "[14/25][156/391] Loss_D: 0.4947 Loss_G: 3.7628 \n",
            "[14/25][157/391] Loss_D: 0.5261 Loss_G: 2.9527 \n",
            "[14/25][158/391] Loss_D: 0.5101 Loss_G: 2.4156 \n",
            "[14/25][159/391] Loss_D: 0.5993 Loss_G: 3.4455 \n",
            "[14/25][160/391] Loss_D: 0.5847 Loss_G: 2.3614 \n",
            "[14/25][161/391] Loss_D: 0.7377 Loss_G: 1.7789 \n",
            "[14/25][162/391] Loss_D: 0.7046 Loss_G: 2.6583 \n",
            "[14/25][163/391] Loss_D: 0.4467 Loss_G: 2.6941 \n",
            "[14/25][164/391] Loss_D: 0.4395 Loss_G: 3.0639 \n",
            "[14/25][165/391] Loss_D: 0.6147 Loss_G: 1.7205 \n",
            "[14/25][166/391] Loss_D: 0.8327 Loss_G: 3.4638 \n",
            "[14/25][167/391] Loss_D: 0.5599 Loss_G: 2.4923 \n",
            "[14/25][168/391] Loss_D: 0.6354 Loss_G: 2.8258 \n",
            "[14/25][169/391] Loss_D: 0.6641 Loss_G: 1.8026 \n",
            "[14/25][170/391] Loss_D: 0.5654 Loss_G: 3.3273 \n",
            "[14/25][171/391] Loss_D: 0.4786 Loss_G: 2.7385 \n",
            "[14/25][172/391] Loss_D: 0.4717 Loss_G: 2.6473 \n",
            "[14/25][173/391] Loss_D: 0.5199 Loss_G: 2.4320 \n",
            "[14/25][174/391] Loss_D: 0.6007 Loss_G: 2.7699 \n",
            "[14/25][175/391] Loss_D: 0.4648 Loss_G: 2.2310 \n",
            "[14/25][176/391] Loss_D: 0.5158 Loss_G: 3.7319 \n",
            "[14/25][177/391] Loss_D: 0.5952 Loss_G: 2.2302 \n",
            "[14/25][178/391] Loss_D: 0.4886 Loss_G: 2.9825 \n",
            "[14/25][179/391] Loss_D: 0.4218 Loss_G: 2.6225 \n",
            "[14/25][180/391] Loss_D: 0.4443 Loss_G: 2.5162 \n",
            "[14/25][181/391] Loss_D: 0.3715 Loss_G: 3.2590 \n",
            "[14/25][182/391] Loss_D: 0.4706 Loss_G: 2.4605 \n",
            "[14/25][183/391] Loss_D: 0.4197 Loss_G: 2.8789 \n",
            "[14/25][184/391] Loss_D: 0.5431 Loss_G: 1.7184 \n",
            "[14/25][185/391] Loss_D: 0.5404 Loss_G: 3.0824 \n",
            "[14/25][186/391] Loss_D: 0.5618 Loss_G: 2.3457 \n",
            "[14/25][187/391] Loss_D: 0.3683 Loss_G: 2.7361 \n",
            "[14/25][188/391] Loss_D: 0.3777 Loss_G: 3.0624 \n",
            "[14/25][189/391] Loss_D: 0.4946 Loss_G: 1.9479 \n",
            "[14/25][190/391] Loss_D: 0.4629 Loss_G: 2.3135 \n",
            "[14/25][191/391] Loss_D: 0.5216 Loss_G: 3.6342 \n",
            "[14/25][192/391] Loss_D: 0.5290 Loss_G: 2.1021 \n",
            "[14/25][193/391] Loss_D: 0.4752 Loss_G: 2.0446 \n",
            "[14/25][194/391] Loss_D: 0.4723 Loss_G: 3.3381 \n",
            "[14/25][195/391] Loss_D: 0.6026 Loss_G: 1.7694 \n",
            "[14/25][196/391] Loss_D: 0.5179 Loss_G: 3.7564 \n",
            "[14/25][197/391] Loss_D: 0.4841 Loss_G: 2.6454 \n",
            "[14/25][198/391] Loss_D: 0.4941 Loss_G: 2.4428 \n",
            "[14/25][199/391] Loss_D: 0.4641 Loss_G: 3.2616 \n",
            "[14/25][200/391] Loss_D: 0.6918 Loss_G: 1.4745 \n",
            "saving the output\n",
            "[14/25][201/391] Loss_D: 0.4595 Loss_G: 2.9689 \n",
            "[14/25][202/391] Loss_D: 0.3590 Loss_G: 3.0698 \n",
            "[14/25][203/391] Loss_D: 0.3885 Loss_G: 2.6983 \n",
            "[14/25][204/391] Loss_D: 0.4209 Loss_G: 2.7346 \n",
            "[14/25][205/391] Loss_D: 0.4805 Loss_G: 1.9862 \n",
            "[14/25][206/391] Loss_D: 0.5735 Loss_G: 4.0954 \n",
            "[14/25][207/391] Loss_D: 0.6239 Loss_G: 1.9812 \n",
            "[14/25][208/391] Loss_D: 0.4408 Loss_G: 2.9831 \n",
            "[14/25][209/391] Loss_D: 0.3751 Loss_G: 2.5988 \n",
            "[14/25][210/391] Loss_D: 0.4327 Loss_G: 2.4613 \n",
            "[14/25][211/391] Loss_D: 0.3531 Loss_G: 2.9078 \n",
            "[14/25][212/391] Loss_D: 0.4658 Loss_G: 2.5143 \n",
            "[14/25][213/391] Loss_D: 0.4829 Loss_G: 2.0430 \n",
            "[14/25][214/391] Loss_D: 0.4672 Loss_G: 3.0713 \n",
            "[14/25][215/391] Loss_D: 0.4899 Loss_G: 2.5069 \n",
            "[14/25][216/391] Loss_D: 0.3709 Loss_G: 3.0257 \n",
            "[14/25][217/391] Loss_D: 0.4709 Loss_G: 2.3032 \n",
            "[14/25][218/391] Loss_D: 0.4890 Loss_G: 2.4760 \n",
            "[14/25][219/391] Loss_D: 0.5040 Loss_G: 3.4219 \n",
            "[14/25][220/391] Loss_D: 0.4546 Loss_G: 2.2549 \n",
            "[14/25][221/391] Loss_D: 0.4541 Loss_G: 2.5452 \n",
            "[14/25][222/391] Loss_D: 0.3381 Loss_G: 2.8718 \n",
            "[14/25][223/391] Loss_D: 0.5716 Loss_G: 1.6774 \n",
            "[14/25][224/391] Loss_D: 0.5557 Loss_G: 4.0231 \n",
            "[14/25][225/391] Loss_D: 0.5040 Loss_G: 2.5415 \n",
            "[14/25][226/391] Loss_D: 0.3687 Loss_G: 2.4295 \n",
            "[14/25][227/391] Loss_D: 0.3897 Loss_G: 3.1812 \n",
            "[14/25][228/391] Loss_D: 0.3866 Loss_G: 2.6956 \n",
            "[14/25][229/391] Loss_D: 0.4143 Loss_G: 2.1020 \n",
            "[14/25][230/391] Loss_D: 0.4352 Loss_G: 3.0671 \n",
            "[14/25][231/391] Loss_D: 0.3017 Loss_G: 3.4843 \n",
            "[14/25][232/391] Loss_D: 0.4598 Loss_G: 1.9140 \n",
            "[14/25][233/391] Loss_D: 0.5737 Loss_G: 3.6650 \n",
            "[14/25][234/391] Loss_D: 0.4459 Loss_G: 2.2662 \n",
            "[14/25][235/391] Loss_D: 0.5634 Loss_G: 2.8167 \n",
            "[14/25][236/391] Loss_D: 0.4185 Loss_G: 2.8840 \n",
            "[14/25][237/391] Loss_D: 0.4045 Loss_G: 2.6174 \n",
            "[14/25][238/391] Loss_D: 0.3219 Loss_G: 2.5384 \n",
            "[14/25][239/391] Loss_D: 0.3948 Loss_G: 2.8070 \n",
            "[14/25][240/391] Loss_D: 0.6106 Loss_G: 4.7879 \n",
            "[14/25][241/391] Loss_D: 1.0350 Loss_G: 1.0186 \n",
            "[14/25][242/391] Loss_D: 0.8515 Loss_G: 4.2264 \n",
            "[14/25][243/391] Loss_D: 0.5607 Loss_G: 2.5206 \n",
            "[14/25][244/391] Loss_D: 0.4561 Loss_G: 2.4051 \n",
            "[14/25][245/391] Loss_D: 0.5224 Loss_G: 3.9538 \n",
            "[14/25][246/391] Loss_D: 0.4743 Loss_G: 2.3686 \n",
            "[14/25][247/391] Loss_D: 0.4068 Loss_G: 2.4411 \n",
            "[14/25][248/391] Loss_D: 0.4654 Loss_G: 3.5711 \n",
            "[14/25][249/391] Loss_D: 0.5924 Loss_G: 1.7005 \n",
            "[14/25][250/391] Loss_D: 0.6632 Loss_G: 4.3965 \n",
            "[14/25][251/391] Loss_D: 0.6118 Loss_G: 2.2942 \n",
            "[14/25][252/391] Loss_D: 0.4131 Loss_G: 2.3575 \n",
            "[14/25][253/391] Loss_D: 0.5732 Loss_G: 3.7976 \n",
            "[14/25][254/391] Loss_D: 0.5541 Loss_G: 2.1943 \n",
            "[14/25][255/391] Loss_D: 0.4370 Loss_G: 2.6622 \n",
            "[14/25][256/391] Loss_D: 0.3426 Loss_G: 3.7161 \n",
            "[14/25][257/391] Loss_D: 0.3828 Loss_G: 2.7247 \n",
            "[14/25][258/391] Loss_D: 0.4373 Loss_G: 2.2389 \n",
            "[14/25][259/391] Loss_D: 0.4799 Loss_G: 4.0703 \n",
            "[14/25][260/391] Loss_D: 0.5668 Loss_G: 2.0033 \n",
            "[14/25][261/391] Loss_D: 0.5587 Loss_G: 3.1322 \n",
            "[14/25][262/391] Loss_D: 0.4085 Loss_G: 2.7980 \n",
            "[14/25][263/391] Loss_D: 0.3876 Loss_G: 3.1828 \n",
            "[14/25][264/391] Loss_D: 0.3963 Loss_G: 2.2448 \n",
            "[14/25][265/391] Loss_D: 0.4682 Loss_G: 3.0950 \n",
            "[14/25][266/391] Loss_D: 0.4283 Loss_G: 3.1906 \n",
            "[14/25][267/391] Loss_D: 0.3487 Loss_G: 2.4590 \n",
            "[14/25][268/391] Loss_D: 0.4387 Loss_G: 2.9331 \n",
            "[14/25][269/391] Loss_D: 0.5629 Loss_G: 1.7477 \n",
            "[14/25][270/391] Loss_D: 0.4417 Loss_G: 3.6317 \n",
            "[14/25][271/391] Loss_D: 0.3144 Loss_G: 3.2774 \n",
            "[14/25][272/391] Loss_D: 0.4172 Loss_G: 1.7693 \n",
            "[14/25][273/391] Loss_D: 0.8791 Loss_G: 5.0351 \n",
            "[14/25][274/391] Loss_D: 1.3929 Loss_G: 0.8948 \n",
            "[14/25][275/391] Loss_D: 1.1550 Loss_G: 5.5598 \n",
            "[14/25][276/391] Loss_D: 0.9114 Loss_G: 1.9276 \n",
            "[14/25][277/391] Loss_D: 0.7756 Loss_G: 4.0792 \n",
            "[14/25][278/391] Loss_D: 0.6428 Loss_G: 2.1581 \n",
            "[14/25][279/391] Loss_D: 0.5807 Loss_G: 2.8490 \n",
            "[14/25][280/391] Loss_D: 0.3549 Loss_G: 3.5703 \n",
            "[14/25][281/391] Loss_D: 0.3970 Loss_G: 2.8450 \n",
            "[14/25][282/391] Loss_D: 0.3569 Loss_G: 3.5250 \n",
            "[14/25][283/391] Loss_D: 0.5273 Loss_G: 2.1134 \n",
            "[14/25][284/391] Loss_D: 0.4716 Loss_G: 3.6617 \n",
            "[14/25][285/391] Loss_D: 0.5143 Loss_G: 2.3908 \n",
            "[14/25][286/391] Loss_D: 0.4582 Loss_G: 2.9139 \n",
            "[14/25][287/391] Loss_D: 0.5020 Loss_G: 2.5351 \n",
            "[14/25][288/391] Loss_D: 0.6549 Loss_G: 2.6801 \n",
            "[14/25][289/391] Loss_D: 0.3893 Loss_G: 3.0625 \n",
            "[14/25][290/391] Loss_D: 0.4179 Loss_G: 2.1343 \n",
            "[14/25][291/391] Loss_D: 0.5677 Loss_G: 4.1627 \n",
            "[14/25][292/391] Loss_D: 0.6631 Loss_G: 1.7007 \n",
            "[14/25][293/391] Loss_D: 0.5053 Loss_G: 3.2314 \n",
            "[14/25][294/391] Loss_D: 0.3482 Loss_G: 3.6217 \n",
            "[14/25][295/391] Loss_D: 0.4420 Loss_G: 1.9477 \n",
            "[14/25][296/391] Loss_D: 0.5734 Loss_G: 3.9194 \n",
            "[14/25][297/391] Loss_D: 0.5522 Loss_G: 1.8534 \n",
            "[14/25][298/391] Loss_D: 0.5075 Loss_G: 1.9634 \n",
            "[14/25][299/391] Loss_D: 0.6031 Loss_G: 4.6655 \n",
            "[14/25][300/391] Loss_D: 0.7036 Loss_G: 1.2759 \n",
            "saving the output\n",
            "[14/25][301/391] Loss_D: 0.9069 Loss_G: 4.3762 \n",
            "[14/25][302/391] Loss_D: 0.7591 Loss_G: 1.6087 \n",
            "[14/25][303/391] Loss_D: 0.8514 Loss_G: 4.5908 \n",
            "[14/25][304/391] Loss_D: 1.0169 Loss_G: 0.9599 \n",
            "[14/25][305/391] Loss_D: 0.9802 Loss_G: 5.0168 \n",
            "[14/25][306/391] Loss_D: 0.6753 Loss_G: 1.4721 \n",
            "[14/25][307/391] Loss_D: 0.4901 Loss_G: 3.2384 \n",
            "[14/25][308/391] Loss_D: 0.4796 Loss_G: 3.3891 \n",
            "[14/25][309/391] Loss_D: 0.4681 Loss_G: 1.6605 \n",
            "[14/25][310/391] Loss_D: 0.6701 Loss_G: 4.7773 \n",
            "[14/25][311/391] Loss_D: 1.1545 Loss_G: 0.8357 \n",
            "[14/25][312/391] Loss_D: 1.3198 Loss_G: 4.9952 \n",
            "[14/25][313/391] Loss_D: 1.3103 Loss_G: 1.2340 \n",
            "[14/25][314/391] Loss_D: 0.8035 Loss_G: 3.5663 \n",
            "[14/25][315/391] Loss_D: 0.5205 Loss_G: 2.6970 \n",
            "[14/25][316/391] Loss_D: 0.5356 Loss_G: 2.0908 \n",
            "[14/25][317/391] Loss_D: 0.5638 Loss_G: 3.3678 \n",
            "[14/25][318/391] Loss_D: 0.3977 Loss_G: 2.8400 \n",
            "[14/25][319/391] Loss_D: 0.4275 Loss_G: 2.3313 \n",
            "[14/25][320/391] Loss_D: 0.4885 Loss_G: 3.2799 \n",
            "[14/25][321/391] Loss_D: 0.4819 Loss_G: 2.6543 \n",
            "[14/25][322/391] Loss_D: 0.4689 Loss_G: 3.0276 \n",
            "[14/25][323/391] Loss_D: 0.3860 Loss_G: 3.1031 \n",
            "[14/25][324/391] Loss_D: 0.4171 Loss_G: 2.2224 \n",
            "[14/25][325/391] Loss_D: 0.6090 Loss_G: 3.5023 \n",
            "[14/25][326/391] Loss_D: 0.4927 Loss_G: 3.0639 \n",
            "[14/25][327/391] Loss_D: 0.4700 Loss_G: 2.1867 \n",
            "[14/25][328/391] Loss_D: 0.4036 Loss_G: 2.9758 \n",
            "[14/25][329/391] Loss_D: 0.3538 Loss_G: 3.3174 \n",
            "[14/25][330/391] Loss_D: 0.4568 Loss_G: 2.1272 \n",
            "[14/25][331/391] Loss_D: 0.5557 Loss_G: 3.7416 \n",
            "[14/25][332/391] Loss_D: 0.5200 Loss_G: 2.1092 \n",
            "[14/25][333/391] Loss_D: 0.3957 Loss_G: 2.6067 \n",
            "[14/25][334/391] Loss_D: 0.3929 Loss_G: 3.7502 \n",
            "[14/25][335/391] Loss_D: 0.6513 Loss_G: 1.3218 \n",
            "[14/25][336/391] Loss_D: 0.8779 Loss_G: 4.6571 \n",
            "[14/25][337/391] Loss_D: 0.3909 Loss_G: 2.4972 \n",
            "[14/25][338/391] Loss_D: 0.4211 Loss_G: 2.3276 \n",
            "[14/25][339/391] Loss_D: 0.8722 Loss_G: 4.6607 \n",
            "[14/25][340/391] Loss_D: 1.2156 Loss_G: 0.3904 \n",
            "[14/25][341/391] Loss_D: 1.4350 Loss_G: 5.0248 \n",
            "[14/25][342/391] Loss_D: 0.9673 Loss_G: 0.3745 \n",
            "[14/25][343/391] Loss_D: 1.8090 Loss_G: 6.6480 \n",
            "[14/25][344/391] Loss_D: 1.6577 Loss_G: 1.1971 \n",
            "[14/25][345/391] Loss_D: 0.8526 Loss_G: 3.8367 \n",
            "[14/25][346/391] Loss_D: 0.6411 Loss_G: 3.0051 \n",
            "[14/25][347/391] Loss_D: 1.0636 Loss_G: 0.7255 \n",
            "[14/25][348/391] Loss_D: 1.6569 Loss_G: 6.3324 \n",
            "[14/25][349/391] Loss_D: 1.8979 Loss_G: 0.6034 \n",
            "[14/25][350/391] Loss_D: 1.3476 Loss_G: 5.2456 \n",
            "[14/25][351/391] Loss_D: 0.7943 Loss_G: 2.3606 \n",
            "[14/25][352/391] Loss_D: 0.5014 Loss_G: 2.4519 \n",
            "[14/25][353/391] Loss_D: 0.5055 Loss_G: 3.4648 \n",
            "[14/25][354/391] Loss_D: 0.5604 Loss_G: 2.4528 \n",
            "[14/25][355/391] Loss_D: 0.6553 Loss_G: 3.2045 \n",
            "[14/25][356/391] Loss_D: 0.5521 Loss_G: 2.3578 \n",
            "[14/25][357/391] Loss_D: 0.5350 Loss_G: 3.7398 \n",
            "[14/25][358/391] Loss_D: 0.4709 Loss_G: 2.2384 \n",
            "[14/25][359/391] Loss_D: 0.8418 Loss_G: 3.3207 \n",
            "[14/25][360/391] Loss_D: 0.4106 Loss_G: 2.7139 \n",
            "[14/25][361/391] Loss_D: 0.6834 Loss_G: 4.0569 \n",
            "[14/25][362/391] Loss_D: 0.6977 Loss_G: 1.6180 \n",
            "[14/25][363/391] Loss_D: 0.7476 Loss_G: 3.8909 \n",
            "[14/25][364/391] Loss_D: 0.5672 Loss_G: 2.5339 \n",
            "[14/25][365/391] Loss_D: 0.5622 Loss_G: 3.5987 \n",
            "[14/25][366/391] Loss_D: 0.4306 Loss_G: 2.7711 \n",
            "[14/25][367/391] Loss_D: 0.4430 Loss_G: 2.5639 \n",
            "[14/25][368/391] Loss_D: 0.3810 Loss_G: 3.2216 \n",
            "[14/25][369/391] Loss_D: 0.4403 Loss_G: 2.2001 \n",
            "[14/25][370/391] Loss_D: 0.5076 Loss_G: 3.1713 \n",
            "[14/25][371/391] Loss_D: 0.4304 Loss_G: 2.7819 \n",
            "[14/25][372/391] Loss_D: 0.4954 Loss_G: 2.2556 \n",
            "[14/25][373/391] Loss_D: 0.5636 Loss_G: 2.5085 \n",
            "[14/25][374/391] Loss_D: 0.4906 Loss_G: 4.1283 \n",
            "[14/25][375/391] Loss_D: 0.8188 Loss_G: 1.6681 \n",
            "[14/25][376/391] Loss_D: 0.5727 Loss_G: 3.2460 \n",
            "[14/25][377/391] Loss_D: 0.5048 Loss_G: 3.1216 \n",
            "[14/25][378/391] Loss_D: 0.3692 Loss_G: 2.4043 \n",
            "[14/25][379/391] Loss_D: 0.3822 Loss_G: 3.2507 \n",
            "[14/25][380/391] Loss_D: 0.3515 Loss_G: 3.6306 \n",
            "[14/25][381/391] Loss_D: 0.4484 Loss_G: 2.1194 \n",
            "[14/25][382/391] Loss_D: 0.5426 Loss_G: 2.8576 \n",
            "[14/25][383/391] Loss_D: 0.3312 Loss_G: 4.0854 \n",
            "[14/25][384/391] Loss_D: 0.7223 Loss_G: 1.4227 \n",
            "[14/25][385/391] Loss_D: 0.7473 Loss_G: 3.8154 \n",
            "[14/25][386/391] Loss_D: 0.4711 Loss_G: 2.7323 \n",
            "[14/25][387/391] Loss_D: 0.4660 Loss_G: 2.2030 \n",
            "[14/25][388/391] Loss_D: 0.5266 Loss_G: 3.5475 \n",
            "[14/25][389/391] Loss_D: 0.4866 Loss_G: 2.4740 \n",
            "[14/25][390/391] Loss_D: 0.4658 Loss_G: 3.2131 \n",
            "[15/25][0/391] Loss_D: 0.3812 Loss_G: 2.5980 \n",
            "saving the output\n",
            "[15/25][1/391] Loss_D: 0.4959 Loss_G: 2.2331 \n",
            "[15/25][2/391] Loss_D: 0.4338 Loss_G: 2.9759 \n",
            "[15/25][3/391] Loss_D: 0.6204 Loss_G: 2.0101 \n",
            "[15/25][4/391] Loss_D: 0.5988 Loss_G: 3.3220 \n",
            "[15/25][5/391] Loss_D: 0.4040 Loss_G: 2.7427 \n",
            "[15/25][6/391] Loss_D: 0.4941 Loss_G: 1.6405 \n",
            "[15/25][7/391] Loss_D: 0.5490 Loss_G: 3.9971 \n",
            "[15/25][8/391] Loss_D: 0.7576 Loss_G: 1.7746 \n",
            "[15/25][9/391] Loss_D: 0.6298 Loss_G: 3.8067 \n",
            "[15/25][10/391] Loss_D: 0.4134 Loss_G: 2.9815 \n",
            "[15/25][11/391] Loss_D: 0.4280 Loss_G: 2.4283 \n",
            "[15/25][12/391] Loss_D: 0.5123 Loss_G: 2.1552 \n",
            "[15/25][13/391] Loss_D: 0.4984 Loss_G: 2.9583 \n",
            "[15/25][14/391] Loss_D: 0.4798 Loss_G: 2.9666 \n",
            "[15/25][15/391] Loss_D: 0.6446 Loss_G: 1.1196 \n",
            "[15/25][16/391] Loss_D: 0.9396 Loss_G: 4.7518 \n",
            "[15/25][17/391] Loss_D: 0.7544 Loss_G: 1.3021 \n",
            "[15/25][18/391] Loss_D: 0.8138 Loss_G: 4.7371 \n",
            "[15/25][19/391] Loss_D: 0.6649 Loss_G: 1.6726 \n",
            "[15/25][20/391] Loss_D: 0.4876 Loss_G: 3.2659 \n",
            "[15/25][21/391] Loss_D: 0.5826 Loss_G: 2.3366 \n",
            "[15/25][22/391] Loss_D: 0.5129 Loss_G: 3.2083 \n",
            "[15/25][23/391] Loss_D: 0.4600 Loss_G: 2.0341 \n",
            "[15/25][24/391] Loss_D: 0.5877 Loss_G: 3.0335 \n",
            "[15/25][25/391] Loss_D: 0.5759 Loss_G: 2.1045 \n",
            "[15/25][26/391] Loss_D: 0.4553 Loss_G: 2.4088 \n",
            "[15/25][27/391] Loss_D: 0.7140 Loss_G: 2.4792 \n",
            "[15/25][28/391] Loss_D: 0.3476 Loss_G: 3.5318 \n",
            "[15/25][29/391] Loss_D: 0.8351 Loss_G: 1.0213 \n",
            "[15/25][30/391] Loss_D: 0.9083 Loss_G: 4.2392 \n",
            "[15/25][31/391] Loss_D: 0.6806 Loss_G: 1.2910 \n",
            "[15/25][32/391] Loss_D: 0.6695 Loss_G: 3.5924 \n",
            "[15/25][33/391] Loss_D: 0.4827 Loss_G: 2.6391 \n",
            "[15/25][34/391] Loss_D: 0.5108 Loss_G: 1.4384 \n",
            "[15/25][35/391] Loss_D: 0.7726 Loss_G: 4.0001 \n",
            "[15/25][36/391] Loss_D: 0.3634 Loss_G: 3.2706 \n",
            "[15/25][37/391] Loss_D: 0.4110 Loss_G: 1.8355 \n",
            "[15/25][38/391] Loss_D: 0.5302 Loss_G: 3.3506 \n",
            "[15/25][39/391] Loss_D: 0.4750 Loss_G: 2.6033 \n",
            "[15/25][40/391] Loss_D: 0.3793 Loss_G: 2.2052 \n",
            "[15/25][41/391] Loss_D: 0.5217 Loss_G: 3.5500 \n",
            "[15/25][42/391] Loss_D: 0.5405 Loss_G: 1.9718 \n",
            "[15/25][43/391] Loss_D: 0.5144 Loss_G: 2.3708 \n",
            "[15/25][44/391] Loss_D: 0.4104 Loss_G: 3.4477 \n",
            "[15/25][45/391] Loss_D: 0.3065 Loss_G: 2.8849 \n",
            "[15/25][46/391] Loss_D: 0.5891 Loss_G: 1.3261 \n",
            "[15/25][47/391] Loss_D: 0.9671 Loss_G: 4.8919 \n",
            "[15/25][48/391] Loss_D: 0.5078 Loss_G: 3.2207 \n",
            "[15/25][49/391] Loss_D: 0.3698 Loss_G: 1.6110 \n",
            "[15/25][50/391] Loss_D: 0.5980 Loss_G: 5.0977 \n",
            "[15/25][51/391] Loss_D: 0.9213 Loss_G: 1.3597 \n",
            "[15/25][52/391] Loss_D: 0.7945 Loss_G: 4.0177 \n",
            "[15/25][53/391] Loss_D: 0.5676 Loss_G: 2.2757 \n",
            "[15/25][54/391] Loss_D: 0.4434 Loss_G: 2.5368 \n",
            "[15/25][55/391] Loss_D: 0.5128 Loss_G: 3.9154 \n",
            "[15/25][56/391] Loss_D: 0.6308 Loss_G: 1.8659 \n",
            "[15/25][57/391] Loss_D: 0.5002 Loss_G: 2.9888 \n",
            "[15/25][58/391] Loss_D: 0.3912 Loss_G: 3.1642 \n",
            "[15/25][59/391] Loss_D: 0.2646 Loss_G: 3.0672 \n",
            "[15/25][60/391] Loss_D: 0.5038 Loss_G: 1.5357 \n",
            "[15/25][61/391] Loss_D: 0.7388 Loss_G: 4.7628 \n",
            "[15/25][62/391] Loss_D: 0.9816 Loss_G: 1.7608 \n",
            "[15/25][63/391] Loss_D: 0.6618 Loss_G: 3.3287 \n",
            "[15/25][64/391] Loss_D: 0.5573 Loss_G: 2.5392 \n",
            "[15/25][65/391] Loss_D: 0.4061 Loss_G: 2.6824 \n",
            "[15/25][66/391] Loss_D: 0.3530 Loss_G: 3.0265 \n",
            "[15/25][67/391] Loss_D: 0.5191 Loss_G: 1.7109 \n",
            "[15/25][68/391] Loss_D: 0.4685 Loss_G: 3.3876 \n",
            "[15/25][69/391] Loss_D: 0.4398 Loss_G: 2.6226 \n",
            "[15/25][70/391] Loss_D: 0.5259 Loss_G: 2.0560 \n",
            "[15/25][71/391] Loss_D: 0.4569 Loss_G: 3.6112 \n",
            "[15/25][72/391] Loss_D: 0.4542 Loss_G: 2.5522 \n",
            "[15/25][73/391] Loss_D: 0.4229 Loss_G: 2.0081 \n",
            "[15/25][74/391] Loss_D: 0.5816 Loss_G: 4.2834 \n",
            "[15/25][75/391] Loss_D: 0.5495 Loss_G: 2.5621 \n",
            "[15/25][76/391] Loss_D: 0.4886 Loss_G: 1.5483 \n",
            "[15/25][77/391] Loss_D: 0.6255 Loss_G: 4.3325 \n",
            "[15/25][78/391] Loss_D: 0.7543 Loss_G: 1.5001 \n",
            "[15/25][79/391] Loss_D: 0.6744 Loss_G: 4.0767 \n",
            "[15/25][80/391] Loss_D: 0.4374 Loss_G: 2.9040 \n",
            "[15/25][81/391] Loss_D: 0.5219 Loss_G: 1.5784 \n",
            "[15/25][82/391] Loss_D: 0.8130 Loss_G: 4.2453 \n",
            "[15/25][83/391] Loss_D: 0.7784 Loss_G: 1.7633 \n",
            "[15/25][84/391] Loss_D: 0.5435 Loss_G: 2.7724 \n",
            "[15/25][85/391] Loss_D: 0.3659 Loss_G: 3.5803 \n",
            "[15/25][86/391] Loss_D: 0.4187 Loss_G: 2.3690 \n",
            "[15/25][87/391] Loss_D: 0.5072 Loss_G: 2.6157 \n",
            "[15/25][88/391] Loss_D: 0.4079 Loss_G: 2.9153 \n",
            "[15/25][89/391] Loss_D: 0.5012 Loss_G: 2.0776 \n",
            "[15/25][90/391] Loss_D: 0.3953 Loss_G: 2.7983 \n",
            "[15/25][91/391] Loss_D: 0.4002 Loss_G: 3.6275 \n",
            "[15/25][92/391] Loss_D: 0.5711 Loss_G: 1.5990 \n",
            "[15/25][93/391] Loss_D: 0.6362 Loss_G: 4.5406 \n",
            "[15/25][94/391] Loss_D: 0.5960 Loss_G: 1.8366 \n",
            "[15/25][95/391] Loss_D: 0.5183 Loss_G: 2.2789 \n",
            "[15/25][96/391] Loss_D: 0.5537 Loss_G: 3.8318 \n",
            "[15/25][97/391] Loss_D: 0.6060 Loss_G: 1.8935 \n",
            "[15/25][98/391] Loss_D: 0.5406 Loss_G: 2.8625 \n",
            "[15/25][99/391] Loss_D: 0.4699 Loss_G: 3.0376 \n",
            "[15/25][100/391] Loss_D: 0.5732 Loss_G: 1.6450 \n",
            "saving the output\n",
            "[15/25][101/391] Loss_D: 0.5199 Loss_G: 3.0250 \n",
            "[15/25][102/391] Loss_D: 0.3250 Loss_G: 3.4583 \n",
            "[15/25][103/391] Loss_D: 0.2890 Loss_G: 2.8833 \n",
            "[15/25][104/391] Loss_D: 0.4249 Loss_G: 2.9755 \n",
            "[15/25][105/391] Loss_D: 0.4204 Loss_G: 2.4655 \n",
            "[15/25][106/391] Loss_D: 0.5014 Loss_G: 2.0579 \n",
            "[15/25][107/391] Loss_D: 0.5165 Loss_G: 3.6988 \n",
            "[15/25][108/391] Loss_D: 0.3618 Loss_G: 3.0114 \n",
            "[15/25][109/391] Loss_D: 0.3502 Loss_G: 2.0676 \n",
            "[15/25][110/391] Loss_D: 0.4593 Loss_G: 3.5041 \n",
            "[15/25][111/391] Loss_D: 0.3026 Loss_G: 3.0679 \n",
            "[15/25][112/391] Loss_D: 0.2784 Loss_G: 2.4663 \n",
            "[15/25][113/391] Loss_D: 0.5021 Loss_G: 2.6120 \n",
            "[15/25][114/391] Loss_D: 0.4116 Loss_G: 2.1793 \n",
            "[15/25][115/391] Loss_D: 0.4149 Loss_G: 3.3085 \n",
            "[15/25][116/391] Loss_D: 0.3460 Loss_G: 2.8696 \n",
            "[15/25][117/391] Loss_D: 0.3490 Loss_G: 2.2501 \n",
            "[15/25][118/391] Loss_D: 0.4640 Loss_G: 3.6777 \n",
            "[15/25][119/391] Loss_D: 0.5240 Loss_G: 2.0867 \n",
            "[15/25][120/391] Loss_D: 0.4541 Loss_G: 2.5496 \n",
            "[15/25][121/391] Loss_D: 0.5259 Loss_G: 3.3697 \n",
            "[15/25][122/391] Loss_D: 0.4494 Loss_G: 2.4544 \n",
            "[15/25][123/391] Loss_D: 0.4373 Loss_G: 2.4355 \n",
            "[15/25][124/391] Loss_D: 0.4498 Loss_G: 2.7543 \n",
            "[15/25][125/391] Loss_D: 0.5116 Loss_G: 1.9451 \n",
            "[15/25][126/391] Loss_D: 0.5739 Loss_G: 4.2593 \n",
            "[15/25][127/391] Loss_D: 0.7412 Loss_G: 1.1004 \n",
            "[15/25][128/391] Loss_D: 1.0339 Loss_G: 4.5764 \n",
            "[15/25][129/391] Loss_D: 0.7865 Loss_G: 0.7621 \n",
            "[15/25][130/391] Loss_D: 1.1833 Loss_G: 5.8296 \n",
            "[15/25][131/391] Loss_D: 1.5228 Loss_G: 1.0245 \n",
            "[15/25][132/391] Loss_D: 1.1406 Loss_G: 3.7982 \n",
            "[15/25][133/391] Loss_D: 0.7417 Loss_G: 1.8718 \n",
            "[15/25][134/391] Loss_D: 0.4776 Loss_G: 3.5066 \n",
            "[15/25][135/391] Loss_D: 0.5190 Loss_G: 2.4230 \n",
            "[15/25][136/391] Loss_D: 0.5602 Loss_G: 3.4250 \n",
            "[15/25][137/391] Loss_D: 0.6811 Loss_G: 1.3389 \n",
            "[15/25][138/391] Loss_D: 0.7584 Loss_G: 5.0204 \n",
            "[15/25][139/391] Loss_D: 1.2465 Loss_G: 0.6556 \n",
            "[15/25][140/391] Loss_D: 1.2815 Loss_G: 5.8217 \n",
            "[15/25][141/391] Loss_D: 0.9637 Loss_G: 0.3968 \n",
            "[15/25][142/391] Loss_D: 1.5901 Loss_G: 6.8519 \n",
            "[15/25][143/391] Loss_D: 2.2315 Loss_G: 0.5212 \n",
            "[15/25][144/391] Loss_D: 1.1772 Loss_G: 4.8530 \n",
            "[15/25][145/391] Loss_D: 1.4074 Loss_G: 0.6906 \n",
            "[15/25][146/391] Loss_D: 1.5175 Loss_G: 4.8328 \n",
            "[15/25][147/391] Loss_D: 0.9597 Loss_G: 2.0490 \n",
            "[15/25][148/391] Loss_D: 0.4804 Loss_G: 2.4258 \n",
            "[15/25][149/391] Loss_D: 0.8534 Loss_G: 2.3349 \n",
            "[15/25][150/391] Loss_D: 0.6148 Loss_G: 4.1134 \n",
            "[15/25][151/391] Loss_D: 1.1117 Loss_G: 1.0131 \n",
            "[15/25][152/391] Loss_D: 0.8912 Loss_G: 3.9054 \n",
            "[15/25][153/391] Loss_D: 0.5182 Loss_G: 2.9584 \n",
            "[15/25][154/391] Loss_D: 0.5612 Loss_G: 1.9685 \n",
            "[15/25][155/391] Loss_D: 0.5878 Loss_G: 4.1621 \n",
            "[15/25][156/391] Loss_D: 0.7800 Loss_G: 1.3618 \n",
            "[15/25][157/391] Loss_D: 0.7035 Loss_G: 3.4859 \n",
            "[15/25][158/391] Loss_D: 0.7393 Loss_G: 2.3266 \n",
            "[15/25][159/391] Loss_D: 0.5196 Loss_G: 1.8818 \n",
            "[15/25][160/391] Loss_D: 0.8302 Loss_G: 3.8470 \n",
            "[15/25][161/391] Loss_D: 0.4813 Loss_G: 2.5386 \n",
            "[15/25][162/391] Loss_D: 0.3515 Loss_G: 2.3026 \n",
            "[15/25][163/391] Loss_D: 0.6227 Loss_G: 3.4389 \n",
            "[15/25][164/391] Loss_D: 0.4067 Loss_G: 2.5490 \n",
            "[15/25][165/391] Loss_D: 0.4741 Loss_G: 1.8731 \n",
            "[15/25][166/391] Loss_D: 0.6771 Loss_G: 4.0874 \n",
            "[15/25][167/391] Loss_D: 0.5750 Loss_G: 2.7013 \n",
            "[15/25][168/391] Loss_D: 0.4424 Loss_G: 1.8297 \n",
            "[15/25][169/391] Loss_D: 0.6718 Loss_G: 3.6372 \n",
            "[15/25][170/391] Loss_D: 0.3068 Loss_G: 3.5810 \n",
            "[15/25][171/391] Loss_D: 0.5237 Loss_G: 1.9673 \n",
            "[15/25][172/391] Loss_D: 0.5767 Loss_G: 2.7529 \n",
            "[15/25][173/391] Loss_D: 0.4640 Loss_G: 2.3846 \n",
            "[15/25][174/391] Loss_D: 0.6121 Loss_G: 3.4796 \n",
            "[15/25][175/391] Loss_D: 0.7143 Loss_G: 1.5553 \n",
            "[15/25][176/391] Loss_D: 0.6424 Loss_G: 3.3342 \n",
            "[15/25][177/391] Loss_D: 0.3493 Loss_G: 3.2108 \n",
            "[15/25][178/391] Loss_D: 0.4286 Loss_G: 2.6624 \n",
            "[15/25][179/391] Loss_D: 0.3496 Loss_G: 3.1292 \n",
            "[15/25][180/391] Loss_D: 0.5353 Loss_G: 1.6055 \n",
            "[15/25][181/391] Loss_D: 0.6247 Loss_G: 4.4892 \n",
            "[15/25][182/391] Loss_D: 0.6290 Loss_G: 2.1820 \n",
            "[15/25][183/391] Loss_D: 0.5915 Loss_G: 3.0937 \n",
            "[15/25][184/391] Loss_D: 0.4137 Loss_G: 2.4502 \n",
            "[15/25][185/391] Loss_D: 0.5573 Loss_G: 2.8238 \n",
            "[15/25][186/391] Loss_D: 0.4965 Loss_G: 2.8366 \n",
            "[15/25][187/391] Loss_D: 0.5723 Loss_G: 2.1643 \n",
            "[15/25][188/391] Loss_D: 0.5369 Loss_G: 1.9975 \n",
            "[15/25][189/391] Loss_D: 0.5577 Loss_G: 3.9621 \n",
            "[15/25][190/391] Loss_D: 0.5537 Loss_G: 2.0189 \n",
            "[15/25][191/391] Loss_D: 0.4403 Loss_G: 1.9579 \n",
            "[15/25][192/391] Loss_D: 0.4129 Loss_G: 3.6574 \n",
            "[15/25][193/391] Loss_D: 0.5321 Loss_G: 2.1910 \n",
            "[15/25][194/391] Loss_D: 0.4274 Loss_G: 2.7317 \n",
            "[15/25][195/391] Loss_D: 0.3179 Loss_G: 3.0435 \n",
            "[15/25][196/391] Loss_D: 0.4179 Loss_G: 2.2362 \n",
            "[15/25][197/391] Loss_D: 0.4251 Loss_G: 3.3412 \n",
            "[15/25][198/391] Loss_D: 0.5658 Loss_G: 1.8754 \n",
            "[15/25][199/391] Loss_D: 0.5039 Loss_G: 2.8042 \n",
            "[15/25][200/391] Loss_D: 0.4821 Loss_G: 3.0002 \n",
            "saving the output\n",
            "[15/25][201/391] Loss_D: 0.5964 Loss_G: 1.3936 \n",
            "[15/25][202/391] Loss_D: 0.7382 Loss_G: 4.0132 \n",
            "[15/25][203/391] Loss_D: 0.6617 Loss_G: 2.1867 \n",
            "[15/25][204/391] Loss_D: 0.4072 Loss_G: 2.4591 \n",
            "[15/25][205/391] Loss_D: 0.6245 Loss_G: 2.4086 \n",
            "[15/25][206/391] Loss_D: 0.4658 Loss_G: 2.7497 \n",
            "[15/25][207/391] Loss_D: 0.4646 Loss_G: 2.5019 \n",
            "[15/25][208/391] Loss_D: 0.4262 Loss_G: 2.4510 \n",
            "[15/25][209/391] Loss_D: 0.3741 Loss_G: 2.7735 \n",
            "[15/25][210/391] Loss_D: 0.4991 Loss_G: 2.3326 \n",
            "[15/25][211/391] Loss_D: 0.5271 Loss_G: 1.9857 \n",
            "[15/25][212/391] Loss_D: 0.4804 Loss_G: 2.8256 \n",
            "[15/25][213/391] Loss_D: 0.2501 Loss_G: 3.4882 \n",
            "[15/25][214/391] Loss_D: 0.3284 Loss_G: 2.6935 \n",
            "[15/25][215/391] Loss_D: 0.2893 Loss_G: 2.7419 \n",
            "[15/25][216/391] Loss_D: 0.3566 Loss_G: 2.3225 \n",
            "[15/25][217/391] Loss_D: 0.4249 Loss_G: 2.7462 \n",
            "[15/25][218/391] Loss_D: 0.3751 Loss_G: 2.4107 \n",
            "[15/25][219/391] Loss_D: 0.5033 Loss_G: 2.6982 \n",
            "[15/25][220/391] Loss_D: 0.3531 Loss_G: 2.6231 \n",
            "[15/25][221/391] Loss_D: 0.4228 Loss_G: 2.8386 \n",
            "[15/25][222/391] Loss_D: 0.3365 Loss_G: 2.6449 \n",
            "[15/25][223/391] Loss_D: 0.3334 Loss_G: 2.5375 \n",
            "[15/25][224/391] Loss_D: 0.2904 Loss_G: 3.1937 \n",
            "[15/25][225/391] Loss_D: 0.4563 Loss_G: 1.9985 \n",
            "[15/25][226/391] Loss_D: 0.3314 Loss_G: 2.6239 \n",
            "[15/25][227/391] Loss_D: 0.3895 Loss_G: 3.0014 \n",
            "[15/25][228/391] Loss_D: 0.5387 Loss_G: 1.5333 \n",
            "[15/25][229/391] Loss_D: 0.4680 Loss_G: 3.1747 \n",
            "[15/25][230/391] Loss_D: 0.4400 Loss_G: 2.2194 \n",
            "[15/25][231/391] Loss_D: 0.3818 Loss_G: 2.8359 \n",
            "[15/25][232/391] Loss_D: 0.5372 Loss_G: 2.4977 \n",
            "[15/25][233/391] Loss_D: 0.3204 Loss_G: 3.2643 \n",
            "[15/25][234/391] Loss_D: 0.6165 Loss_G: 1.2036 \n",
            "[15/25][235/391] Loss_D: 0.6689 Loss_G: 3.9420 \n",
            "[15/25][236/391] Loss_D: 0.4622 Loss_G: 1.8699 \n",
            "[15/25][237/391] Loss_D: 0.5981 Loss_G: 3.1449 \n",
            "[15/25][238/391] Loss_D: 0.5336 Loss_G: 1.9511 \n",
            "[15/25][239/391] Loss_D: 0.5178 Loss_G: 3.2342 \n",
            "[15/25][240/391] Loss_D: 0.4494 Loss_G: 2.4563 \n",
            "[15/25][241/391] Loss_D: 0.4559 Loss_G: 2.5661 \n",
            "[15/25][242/391] Loss_D: 0.4814 Loss_G: 2.7045 \n",
            "[15/25][243/391] Loss_D: 0.4785 Loss_G: 1.8545 \n",
            "[15/25][244/391] Loss_D: 0.4859 Loss_G: 3.1984 \n",
            "[15/25][245/391] Loss_D: 0.4721 Loss_G: 1.9573 \n",
            "[15/25][246/391] Loss_D: 0.4864 Loss_G: 3.6529 \n",
            "[15/25][247/391] Loss_D: 0.4820 Loss_G: 2.1785 \n",
            "[15/25][248/391] Loss_D: 0.4859 Loss_G: 3.1464 \n",
            "[15/25][249/391] Loss_D: 0.3633 Loss_G: 3.1459 \n",
            "[15/25][250/391] Loss_D: 0.5226 Loss_G: 1.9382 \n",
            "[15/25][251/391] Loss_D: 0.4908 Loss_G: 2.3752 \n",
            "[15/25][252/391] Loss_D: 0.4246 Loss_G: 2.9798 \n",
            "[15/25][253/391] Loss_D: 0.4741 Loss_G: 2.4883 \n",
            "[15/25][254/391] Loss_D: 0.4110 Loss_G: 2.5345 \n",
            "[15/25][255/391] Loss_D: 0.3570 Loss_G: 3.1519 \n",
            "[15/25][256/391] Loss_D: 0.2712 Loss_G: 2.8496 \n",
            "[15/25][257/391] Loss_D: 0.4066 Loss_G: 2.6348 \n",
            "[15/25][258/391] Loss_D: 0.3204 Loss_G: 2.7656 \n",
            "[15/25][259/391] Loss_D: 0.3323 Loss_G: 2.7137 \n",
            "[15/25][260/391] Loss_D: 0.4395 Loss_G: 3.3379 \n",
            "[15/25][261/391] Loss_D: 0.4054 Loss_G: 1.9255 \n",
            "[15/25][262/391] Loss_D: 0.4243 Loss_G: 3.3981 \n",
            "[15/25][263/391] Loss_D: 0.4575 Loss_G: 2.0753 \n",
            "[15/25][264/391] Loss_D: 0.3596 Loss_G: 3.7095 \n",
            "[15/25][265/391] Loss_D: 0.4777 Loss_G: 1.7872 \n",
            "[15/25][266/391] Loss_D: 0.4639 Loss_G: 3.2752 \n",
            "[15/25][267/391] Loss_D: 0.4588 Loss_G: 2.1761 \n",
            "[15/25][268/391] Loss_D: 0.5368 Loss_G: 4.2200 \n",
            "[15/25][269/391] Loss_D: 0.6781 Loss_G: 1.5856 \n",
            "[15/25][270/391] Loss_D: 0.6939 Loss_G: 4.2493 \n",
            "[15/25][271/391] Loss_D: 0.2129 Loss_G: 3.7338 \n",
            "[15/25][272/391] Loss_D: 0.6200 Loss_G: 0.4709 \n",
            "[15/25][273/391] Loss_D: 1.4977 Loss_G: 5.9289 \n",
            "[15/25][274/391] Loss_D: 1.6954 Loss_G: 0.0804 \n",
            "[15/25][275/391] Loss_D: 3.5063 Loss_G: 6.5010 \n",
            "[15/25][276/391] Loss_D: 2.2912 Loss_G: 0.5011 \n",
            "[15/25][277/391] Loss_D: 1.5033 Loss_G: 6.8394 \n",
            "[15/25][278/391] Loss_D: 2.4179 Loss_G: 0.1636 \n",
            "[15/25][279/391] Loss_D: 3.0993 Loss_G: 4.7274 \n",
            "[15/25][280/391] Loss_D: 1.9979 Loss_G: 0.8323 \n",
            "[15/25][281/391] Loss_D: 1.6107 Loss_G: 3.9140 \n",
            "[15/25][282/391] Loss_D: 1.2972 Loss_G: 1.6368 \n",
            "[15/25][283/391] Loss_D: 0.7552 Loss_G: 3.3318 \n",
            "[15/25][284/391] Loss_D: 0.7181 Loss_G: 2.5223 \n",
            "[15/25][285/391] Loss_D: 0.5457 Loss_G: 3.8968 \n",
            "[15/25][286/391] Loss_D: 0.8926 Loss_G: 1.0931 \n",
            "[15/25][287/391] Loss_D: 1.3730 Loss_G: 6.3539 \n",
            "[15/25][288/391] Loss_D: 2.9747 Loss_G: 0.0749 \n",
            "[15/25][289/391] Loss_D: 3.2129 Loss_G: 7.4363 \n",
            "[15/25][290/391] Loss_D: 2.4761 Loss_G: 0.2539 \n",
            "[15/25][291/391] Loss_D: 3.0954 Loss_G: 5.7261 \n",
            "[15/25][292/391] Loss_D: 1.2281 Loss_G: 2.4193 \n",
            "[15/25][293/391] Loss_D: 0.9289 Loss_G: 1.9700 \n",
            "[15/25][294/391] Loss_D: 0.8010 Loss_G: 3.9760 \n",
            "[15/25][295/391] Loss_D: 1.0816 Loss_G: 1.0514 \n",
            "[15/25][296/391] Loss_D: 1.0952 Loss_G: 4.2854 \n",
            "[15/25][297/391] Loss_D: 0.7590 Loss_G: 2.3389 \n",
            "[15/25][298/391] Loss_D: 0.8595 Loss_G: 3.2111 \n",
            "[15/25][299/391] Loss_D: 0.8113 Loss_G: 1.3215 \n",
            "[15/25][300/391] Loss_D: 1.2569 Loss_G: 3.8485 \n",
            "saving the output\n",
            "[15/25][301/391] Loss_D: 0.9230 Loss_G: 1.8828 \n",
            "[15/25][302/391] Loss_D: 0.6231 Loss_G: 2.2443 \n",
            "[15/25][303/391] Loss_D: 0.5743 Loss_G: 3.8636 \n",
            "[15/25][304/391] Loss_D: 0.5871 Loss_G: 2.5053 \n",
            "[15/25][305/391] Loss_D: 0.4933 Loss_G: 2.8627 \n",
            "[15/25][306/391] Loss_D: 0.3522 Loss_G: 2.8333 \n",
            "[15/25][307/391] Loss_D: 0.4585 Loss_G: 2.8923 \n",
            "[15/25][308/391] Loss_D: 0.6694 Loss_G: 2.0849 \n",
            "[15/25][309/391] Loss_D: 0.5637 Loss_G: 2.2358 \n",
            "[15/25][310/391] Loss_D: 0.6567 Loss_G: 3.3911 \n",
            "[15/25][311/391] Loss_D: 0.6427 Loss_G: 1.9118 \n",
            "[15/25][312/391] Loss_D: 0.6680 Loss_G: 3.1301 \n",
            "[15/25][313/391] Loss_D: 0.5991 Loss_G: 2.2307 \n",
            "[15/25][314/391] Loss_D: 0.5973 Loss_G: 2.3500 \n",
            "[15/25][315/391] Loss_D: 0.4774 Loss_G: 2.7869 \n",
            "[15/25][316/391] Loss_D: 0.5393 Loss_G: 2.1002 \n",
            "[15/25][317/391] Loss_D: 0.5684 Loss_G: 2.8703 \n",
            "[15/25][318/391] Loss_D: 0.4716 Loss_G: 2.5432 \n",
            "[15/25][319/391] Loss_D: 0.4788 Loss_G: 2.1841 \n",
            "[15/25][320/391] Loss_D: 0.6018 Loss_G: 3.1771 \n",
            "[15/25][321/391] Loss_D: 0.4239 Loss_G: 2.6539 \n",
            "[15/25][322/391] Loss_D: 0.5905 Loss_G: 2.2178 \n",
            "[15/25][323/391] Loss_D: 0.4102 Loss_G: 3.4874 \n",
            "[15/25][324/391] Loss_D: 0.6051 Loss_G: 2.1048 \n",
            "[15/25][325/391] Loss_D: 0.5528 Loss_G: 3.2600 \n",
            "[15/25][326/391] Loss_D: 0.5537 Loss_G: 2.5480 \n",
            "[15/25][327/391] Loss_D: 0.5284 Loss_G: 2.7547 \n",
            "[15/25][328/391] Loss_D: 0.7146 Loss_G: 1.5500 \n",
            "[15/25][329/391] Loss_D: 0.5491 Loss_G: 3.2019 \n",
            "[15/25][330/391] Loss_D: 0.4233 Loss_G: 3.0271 \n",
            "[15/25][331/391] Loss_D: 0.4556 Loss_G: 2.1640 \n",
            "[15/25][332/391] Loss_D: 0.5338 Loss_G: 2.9031 \n",
            "[15/25][333/391] Loss_D: 0.6509 Loss_G: 2.8655 \n",
            "[15/25][334/391] Loss_D: 0.6155 Loss_G: 1.3491 \n",
            "[15/25][335/391] Loss_D: 0.7429 Loss_G: 4.1145 \n",
            "[15/25][336/391] Loss_D: 0.6283 Loss_G: 2.3435 \n",
            "[15/25][337/391] Loss_D: 0.5462 Loss_G: 2.0081 \n",
            "[15/25][338/391] Loss_D: 0.5781 Loss_G: 3.6141 \n",
            "[15/25][339/391] Loss_D: 0.5870 Loss_G: 2.0424 \n",
            "[15/25][340/391] Loss_D: 0.4265 Loss_G: 2.1922 \n",
            "[15/25][341/391] Loss_D: 0.6480 Loss_G: 3.4325 \n",
            "[15/25][342/391] Loss_D: 0.3526 Loss_G: 3.2028 \n",
            "[15/25][343/391] Loss_D: 0.2374 Loss_G: 3.1514 \n",
            "[15/25][344/391] Loss_D: 0.3988 Loss_G: 2.4433 \n",
            "[15/25][345/391] Loss_D: 0.6474 Loss_G: 1.6984 \n",
            "[15/25][346/391] Loss_D: 0.6687 Loss_G: 3.7561 \n",
            "[15/25][347/391] Loss_D: 0.6147 Loss_G: 2.0727 \n",
            "[15/25][348/391] Loss_D: 0.6391 Loss_G: 2.3399 \n",
            "[15/25][349/391] Loss_D: 0.4047 Loss_G: 3.3446 \n",
            "[15/25][350/391] Loss_D: 0.4801 Loss_G: 2.2571 \n",
            "[15/25][351/391] Loss_D: 0.4009 Loss_G: 2.7759 \n",
            "[15/25][352/391] Loss_D: 0.4534 Loss_G: 2.5038 \n",
            "[15/25][353/391] Loss_D: 0.5127 Loss_G: 2.1808 \n",
            "[15/25][354/391] Loss_D: 0.4345 Loss_G: 2.7200 \n",
            "[15/25][355/391] Loss_D: 0.3812 Loss_G: 2.8453 \n",
            "[15/25][356/391] Loss_D: 0.5018 Loss_G: 2.6418 \n",
            "[15/25][357/391] Loss_D: 0.5595 Loss_G: 1.5964 \n",
            "[15/25][358/391] Loss_D: 0.7153 Loss_G: 4.2440 \n",
            "[15/25][359/391] Loss_D: 0.5840 Loss_G: 2.1829 \n",
            "[15/25][360/391] Loss_D: 0.5148 Loss_G: 3.0355 \n",
            "[15/25][361/391] Loss_D: 0.4391 Loss_G: 2.3343 \n",
            "[15/25][362/391] Loss_D: 0.4810 Loss_G: 2.6134 \n",
            "[15/25][363/391] Loss_D: 0.6120 Loss_G: 2.0566 \n",
            "[15/25][364/391] Loss_D: 0.6058 Loss_G: 3.4409 \n",
            "[15/25][365/391] Loss_D: 0.9094 Loss_G: 1.0964 \n",
            "[15/25][366/391] Loss_D: 0.9544 Loss_G: 4.6224 \n",
            "[15/25][367/391] Loss_D: 0.9823 Loss_G: 1.1391 \n",
            "[15/25][368/391] Loss_D: 0.7437 Loss_G: 4.0682 \n",
            "[15/25][369/391] Loss_D: 0.6914 Loss_G: 1.8904 \n",
            "[15/25][370/391] Loss_D: 0.4910 Loss_G: 3.2386 \n",
            "[15/25][371/391] Loss_D: 0.5757 Loss_G: 2.6908 \n",
            "[15/25][372/391] Loss_D: 0.4127 Loss_G: 2.2612 \n",
            "[15/25][373/391] Loss_D: 0.4101 Loss_G: 3.3210 \n",
            "[15/25][374/391] Loss_D: 0.4201 Loss_G: 2.6050 \n",
            "[15/25][375/391] Loss_D: 0.4902 Loss_G: 2.0496 \n",
            "[15/25][376/391] Loss_D: 0.4937 Loss_G: 2.8260 \n",
            "[15/25][377/391] Loss_D: 0.5924 Loss_G: 2.2652 \n",
            "[15/25][378/391] Loss_D: 0.4708 Loss_G: 3.1382 \n",
            "[15/25][379/391] Loss_D: 0.3416 Loss_G: 3.0426 \n",
            "[15/25][380/391] Loss_D: 0.4864 Loss_G: 1.8930 \n",
            "[15/25][381/391] Loss_D: 0.6738 Loss_G: 3.5050 \n",
            "[15/25][382/391] Loss_D: 0.5629 Loss_G: 2.2758 \n",
            "[15/25][383/391] Loss_D: 0.5157 Loss_G: 2.7507 \n",
            "[15/25][384/391] Loss_D: 0.5766 Loss_G: 2.8598 \n",
            "[15/25][385/391] Loss_D: 0.4826 Loss_G: 1.9815 \n",
            "[15/25][386/391] Loss_D: 0.6172 Loss_G: 3.8801 \n",
            "[15/25][387/391] Loss_D: 0.4620 Loss_G: 2.6206 \n",
            "[15/25][388/391] Loss_D: 0.4310 Loss_G: 2.4569 \n",
            "[15/25][389/391] Loss_D: 0.3975 Loss_G: 3.3316 \n",
            "[15/25][390/391] Loss_D: 0.4194 Loss_G: 2.4324 \n",
            "[16/25][0/391] Loss_D: 0.3569 Loss_G: 2.7121 \n",
            "saving the output\n",
            "[16/25][1/391] Loss_D: 0.2673 Loss_G: 3.5928 \n",
            "[16/25][2/391] Loss_D: 0.5881 Loss_G: 1.6127 \n",
            "[16/25][3/391] Loss_D: 0.6721 Loss_G: 4.2395 \n",
            "[16/25][4/391] Loss_D: 1.1589 Loss_G: 0.8463 \n",
            "[16/25][5/391] Loss_D: 0.6928 Loss_G: 3.7429 \n",
            "[16/25][6/391] Loss_D: 0.3798 Loss_G: 3.2311 \n",
            "[16/25][7/391] Loss_D: 0.5774 Loss_G: 1.0518 \n",
            "[16/25][8/391] Loss_D: 1.2388 Loss_G: 4.9401 \n",
            "[16/25][9/391] Loss_D: 0.9829 Loss_G: 1.4379 \n",
            "[16/25][10/391] Loss_D: 0.8779 Loss_G: 5.1344 \n",
            "[16/25][11/391] Loss_D: 0.9016 Loss_G: 1.5444 \n",
            "[16/25][12/391] Loss_D: 0.8029 Loss_G: 3.9542 \n",
            "[16/25][13/391] Loss_D: 0.8375 Loss_G: 1.5025 \n",
            "[16/25][14/391] Loss_D: 0.7590 Loss_G: 3.3312 \n",
            "[16/25][15/391] Loss_D: 0.5156 Loss_G: 2.6492 \n",
            "[16/25][16/391] Loss_D: 0.5144 Loss_G: 1.8457 \n",
            "[16/25][17/391] Loss_D: 0.5147 Loss_G: 3.2379 \n",
            "[16/25][18/391] Loss_D: 0.4984 Loss_G: 2.1982 \n",
            "[16/25][19/391] Loss_D: 0.5298 Loss_G: 1.7931 \n",
            "[16/25][20/391] Loss_D: 0.6256 Loss_G: 3.9121 \n",
            "[16/25][21/391] Loss_D: 0.5423 Loss_G: 2.2961 \n",
            "[16/25][22/391] Loss_D: 0.4630 Loss_G: 2.0343 \n",
            "[16/25][23/391] Loss_D: 0.6097 Loss_G: 3.1769 \n",
            "[16/25][24/391] Loss_D: 0.3983 Loss_G: 2.6858 \n",
            "[16/25][25/391] Loss_D: 0.5532 Loss_G: 1.4404 \n",
            "[16/25][26/391] Loss_D: 0.5840 Loss_G: 4.3415 \n",
            "[16/25][27/391] Loss_D: 0.4695 Loss_G: 2.4997 \n",
            "[16/25][28/391] Loss_D: 0.3776 Loss_G: 1.8036 \n",
            "[16/25][29/391] Loss_D: 0.5227 Loss_G: 3.5041 \n",
            "[16/25][30/391] Loss_D: 0.3704 Loss_G: 2.8927 \n",
            "[16/25][31/391] Loss_D: 0.4505 Loss_G: 2.0506 \n",
            "[16/25][32/391] Loss_D: 0.5002 Loss_G: 2.1276 \n",
            "[16/25][33/391] Loss_D: 0.4572 Loss_G: 2.9847 \n",
            "[16/25][34/391] Loss_D: 0.3565 Loss_G: 3.2564 \n",
            "[16/25][35/391] Loss_D: 0.5402 Loss_G: 1.5911 \n",
            "[16/25][36/391] Loss_D: 0.3388 Loss_G: 2.5865 \n",
            "[16/25][37/391] Loss_D: 0.3572 Loss_G: 3.4297 \n",
            "[16/25][38/391] Loss_D: 0.4115 Loss_G: 2.5077 \n",
            "[16/25][39/391] Loss_D: 0.3379 Loss_G: 2.2754 \n",
            "[16/25][40/391] Loss_D: 0.4436 Loss_G: 3.2991 \n",
            "[16/25][41/391] Loss_D: 0.4649 Loss_G: 1.9258 \n",
            "[16/25][42/391] Loss_D: 0.4599 Loss_G: 3.3822 \n",
            "[16/25][43/391] Loss_D: 0.4141 Loss_G: 2.4118 \n",
            "[16/25][44/391] Loss_D: 0.3874 Loss_G: 2.4185 \n",
            "[16/25][45/391] Loss_D: 0.3827 Loss_G: 3.0874 \n",
            "[16/25][46/391] Loss_D: 0.5518 Loss_G: 1.8619 \n",
            "[16/25][47/391] Loss_D: 0.3665 Loss_G: 2.5015 \n",
            "[16/25][48/391] Loss_D: 0.4677 Loss_G: 2.6490 \n",
            "[16/25][49/391] Loss_D: 0.4190 Loss_G: 2.2245 \n",
            "[16/25][50/391] Loss_D: 0.5381 Loss_G: 3.1320 \n",
            "[16/25][51/391] Loss_D: 0.4376 Loss_G: 2.2626 \n",
            "[16/25][52/391] Loss_D: 0.4106 Loss_G: 2.3558 \n",
            "[16/25][53/391] Loss_D: 0.4543 Loss_G: 2.9970 \n",
            "[16/25][54/391] Loss_D: 0.4029 Loss_G: 2.2570 \n",
            "[16/25][55/391] Loss_D: 0.3504 Loss_G: 2.8805 \n",
            "[16/25][56/391] Loss_D: 0.5093 Loss_G: 1.9065 \n",
            "[16/25][57/391] Loss_D: 0.5063 Loss_G: 3.3402 \n",
            "[16/25][58/391] Loss_D: 0.4110 Loss_G: 2.6476 \n",
            "[16/25][59/391] Loss_D: 0.5343 Loss_G: 1.5777 \n",
            "[16/25][60/391] Loss_D: 0.5571 Loss_G: 3.6675 \n",
            "[16/25][61/391] Loss_D: 0.5326 Loss_G: 2.2867 \n",
            "[16/25][62/391] Loss_D: 0.4361 Loss_G: 2.0515 \n",
            "[16/25][63/391] Loss_D: 0.5980 Loss_G: 4.4087 \n",
            "[16/25][64/391] Loss_D: 0.7078 Loss_G: 1.5517 \n",
            "[16/25][65/391] Loss_D: 0.4892 Loss_G: 3.3270 \n",
            "[16/25][66/391] Loss_D: 0.5343 Loss_G: 1.8785 \n",
            "[16/25][67/391] Loss_D: 0.5031 Loss_G: 2.9580 \n",
            "[16/25][68/391] Loss_D: 0.3705 Loss_G: 3.0647 \n",
            "[16/25][69/391] Loss_D: 0.4196 Loss_G: 2.4201 \n",
            "[16/25][70/391] Loss_D: 0.4852 Loss_G: 2.1959 \n",
            "[16/25][71/391] Loss_D: 0.2830 Loss_G: 3.1988 \n",
            "[16/25][72/391] Loss_D: 0.3339 Loss_G: 2.7783 \n",
            "[16/25][73/391] Loss_D: 0.4976 Loss_G: 1.5457 \n",
            "[16/25][74/391] Loss_D: 0.7675 Loss_G: 3.9680 \n",
            "[16/25][75/391] Loss_D: 0.7604 Loss_G: 1.2384 \n",
            "[16/25][76/391] Loss_D: 0.6321 Loss_G: 4.3867 \n",
            "[16/25][77/391] Loss_D: 0.8214 Loss_G: 1.2151 \n",
            "[16/25][78/391] Loss_D: 0.7767 Loss_G: 3.7313 \n",
            "[16/25][79/391] Loss_D: 0.5985 Loss_G: 1.6627 \n",
            "[16/25][80/391] Loss_D: 0.5012 Loss_G: 3.2533 \n",
            "[16/25][81/391] Loss_D: 0.3616 Loss_G: 2.8187 \n",
            "[16/25][82/391] Loss_D: 0.4546 Loss_G: 2.2491 \n",
            "[16/25][83/391] Loss_D: 0.5355 Loss_G: 3.1820 \n",
            "[16/25][84/391] Loss_D: 0.5311 Loss_G: 1.9638 \n",
            "[16/25][85/391] Loss_D: 0.5473 Loss_G: 2.2051 \n",
            "[16/25][86/391] Loss_D: 0.7313 Loss_G: 4.2022 \n",
            "[16/25][87/391] Loss_D: 0.3273 Loss_G: 3.3921 \n",
            "[16/25][88/391] Loss_D: 0.5156 Loss_G: 1.3457 \n",
            "[16/25][89/391] Loss_D: 0.6729 Loss_G: 4.2671 \n",
            "[16/25][90/391] Loss_D: 0.5211 Loss_G: 2.2120 \n",
            "[16/25][91/391] Loss_D: 0.4964 Loss_G: 2.1424 \n",
            "[16/25][92/391] Loss_D: 0.3701 Loss_G: 4.0037 \n",
            "[16/25][93/391] Loss_D: 0.4983 Loss_G: 2.1341 \n",
            "[16/25][94/391] Loss_D: 0.3879 Loss_G: 1.9204 \n",
            "[16/25][95/391] Loss_D: 0.7071 Loss_G: 5.0980 \n",
            "[16/25][96/391] Loss_D: 0.6884 Loss_G: 2.0558 \n",
            "[16/25][97/391] Loss_D: 0.6062 Loss_G: 3.0340 \n",
            "[16/25][98/391] Loss_D: 0.3657 Loss_G: 3.0055 \n",
            "[16/25][99/391] Loss_D: 0.3957 Loss_G: 2.2071 \n",
            "[16/25][100/391] Loss_D: 0.4804 Loss_G: 3.7860 \n",
            "saving the output\n",
            "[16/25][101/391] Loss_D: 0.5976 Loss_G: 1.8449 \n",
            "[16/25][102/391] Loss_D: 0.6079 Loss_G: 3.7416 \n",
            "[16/25][103/391] Loss_D: 0.3922 Loss_G: 2.7756 \n",
            "[16/25][104/391] Loss_D: 0.4032 Loss_G: 2.4169 \n",
            "[16/25][105/391] Loss_D: 0.3670 Loss_G: 2.9597 \n",
            "[16/25][106/391] Loss_D: 0.4873 Loss_G: 2.2123 \n",
            "[16/25][107/391] Loss_D: 0.4186 Loss_G: 3.7211 \n",
            "[16/25][108/391] Loss_D: 0.6660 Loss_G: 1.2681 \n",
            "[16/25][109/391] Loss_D: 0.7067 Loss_G: 4.7434 \n",
            "[16/25][110/391] Loss_D: 0.6072 Loss_G: 1.9716 \n",
            "[16/25][111/391] Loss_D: 0.4299 Loss_G: 2.8372 \n",
            "[16/25][112/391] Loss_D: 0.3689 Loss_G: 3.9535 \n",
            "[16/25][113/391] Loss_D: 0.7091 Loss_G: 1.2755 \n",
            "[16/25][114/391] Loss_D: 0.8367 Loss_G: 4.1811 \n",
            "[16/25][115/391] Loss_D: 0.5708 Loss_G: 1.8806 \n",
            "[16/25][116/391] Loss_D: 0.4300 Loss_G: 2.8214 \n",
            "[16/25][117/391] Loss_D: 0.3752 Loss_G: 2.7417 \n",
            "[16/25][118/391] Loss_D: 0.4879 Loss_G: 2.8154 \n",
            "[16/25][119/391] Loss_D: 0.5732 Loss_G: 1.6552 \n",
            "[16/25][120/391] Loss_D: 0.5634 Loss_G: 3.5943 \n",
            "[16/25][121/391] Loss_D: 0.6199 Loss_G: 1.7845 \n",
            "[16/25][122/391] Loss_D: 0.5287 Loss_G: 3.0241 \n",
            "[16/25][123/391] Loss_D: 0.5506 Loss_G: 2.1842 \n",
            "[16/25][124/391] Loss_D: 0.5446 Loss_G: 2.3111 \n",
            "[16/25][125/391] Loss_D: 0.3480 Loss_G: 3.5504 \n",
            "[16/25][126/391] Loss_D: 0.4876 Loss_G: 2.0583 \n",
            "[16/25][127/391] Loss_D: 0.4917 Loss_G: 3.1741 \n",
            "[16/25][128/391] Loss_D: 0.3802 Loss_G: 2.5866 \n",
            "[16/25][129/391] Loss_D: 0.4503 Loss_G: 2.5369 \n",
            "[16/25][130/391] Loss_D: 0.4668 Loss_G: 2.5305 \n",
            "[16/25][131/391] Loss_D: 0.5133 Loss_G: 2.0764 \n",
            "[16/25][132/391] Loss_D: 0.3784 Loss_G: 3.0980 \n",
            "[16/25][133/391] Loss_D: 0.3591 Loss_G: 2.7026 \n",
            "[16/25][134/391] Loss_D: 0.2609 Loss_G: 3.2170 \n",
            "[16/25][135/391] Loss_D: 0.5225 Loss_G: 1.7772 \n",
            "[16/25][136/391] Loss_D: 0.5415 Loss_G: 2.9287 \n",
            "[16/25][137/391] Loss_D: 0.3149 Loss_G: 3.0888 \n",
            "[16/25][138/391] Loss_D: 0.3917 Loss_G: 2.2434 \n",
            "[16/25][139/391] Loss_D: 0.4954 Loss_G: 3.0708 \n",
            "[16/25][140/391] Loss_D: 0.3953 Loss_G: 2.3033 \n",
            "[16/25][141/391] Loss_D: 0.4813 Loss_G: 2.4590 \n",
            "[16/25][142/391] Loss_D: 0.3585 Loss_G: 3.0745 \n",
            "[16/25][143/391] Loss_D: 0.3567 Loss_G: 3.1227 \n",
            "[16/25][144/391] Loss_D: 0.5242 Loss_G: 1.4607 \n",
            "[16/25][145/391] Loss_D: 0.6678 Loss_G: 3.6663 \n",
            "[16/25][146/391] Loss_D: 0.4563 Loss_G: 2.4908 \n",
            "[16/25][147/391] Loss_D: 0.3654 Loss_G: 3.1571 \n",
            "[16/25][148/391] Loss_D: 0.3223 Loss_G: 2.6688 \n",
            "[16/25][149/391] Loss_D: 0.3665 Loss_G: 2.1379 \n",
            "[16/25][150/391] Loss_D: 0.6306 Loss_G: 3.1047 \n",
            "[16/25][151/391] Loss_D: 0.4723 Loss_G: 2.4991 \n",
            "[16/25][152/391] Loss_D: 0.5908 Loss_G: 1.4131 \n",
            "[16/25][153/391] Loss_D: 0.6476 Loss_G: 4.3194 \n",
            "[16/25][154/391] Loss_D: 0.8503 Loss_G: 1.2650 \n",
            "[16/25][155/391] Loss_D: 0.7316 Loss_G: 3.7438 \n",
            "[16/25][156/391] Loss_D: 0.3015 Loss_G: 2.8806 \n",
            "[16/25][157/391] Loss_D: 0.4319 Loss_G: 1.7318 \n",
            "[16/25][158/391] Loss_D: 0.5944 Loss_G: 4.3337 \n",
            "[16/25][159/391] Loss_D: 0.8371 Loss_G: 1.1447 \n",
            "[16/25][160/391] Loss_D: 0.7265 Loss_G: 4.0657 \n",
            "[16/25][161/391] Loss_D: 0.5506 Loss_G: 1.8395 \n",
            "[16/25][162/391] Loss_D: 0.3942 Loss_G: 3.1536 \n",
            "[16/25][163/391] Loss_D: 0.3075 Loss_G: 3.6037 \n",
            "[16/25][164/391] Loss_D: 0.4955 Loss_G: 1.7873 \n",
            "[16/25][165/391] Loss_D: 0.3212 Loss_G: 2.6019 \n",
            "[16/25][166/391] Loss_D: 0.4446 Loss_G: 3.2696 \n",
            "[16/25][167/391] Loss_D: 0.3670 Loss_G: 2.9085 \n",
            "[16/25][168/391] Loss_D: 0.5941 Loss_G: 1.1527 \n",
            "[16/25][169/391] Loss_D: 1.2282 Loss_G: 5.5025 \n",
            "[16/25][170/391] Loss_D: 1.2483 Loss_G: 0.2415 \n",
            "[16/25][171/391] Loss_D: 2.2081 Loss_G: 7.3252 \n",
            "[16/25][172/391] Loss_D: 2.1026 Loss_G: 0.5479 \n",
            "[16/25][173/391] Loss_D: 1.5702 Loss_G: 5.1766 \n",
            "[16/25][174/391] Loss_D: 1.2807 Loss_G: 0.9071 \n",
            "[16/25][175/391] Loss_D: 1.0480 Loss_G: 4.2010 \n",
            "[16/25][176/391] Loss_D: 0.7687 Loss_G: 1.9261 \n",
            "[16/25][177/391] Loss_D: 0.8179 Loss_G: 2.8825 \n",
            "[16/25][178/391] Loss_D: 0.8313 Loss_G: 2.0928 \n",
            "[16/25][179/391] Loss_D: 0.8140 Loss_G: 2.5408 \n",
            "[16/25][180/391] Loss_D: 0.8127 Loss_G: 1.9237 \n",
            "[16/25][181/391] Loss_D: 0.6820 Loss_G: 2.3908 \n",
            "[16/25][182/391] Loss_D: 0.6915 Loss_G: 3.2447 \n",
            "[16/25][183/391] Loss_D: 0.6941 Loss_G: 1.5389 \n",
            "[16/25][184/391] Loss_D: 0.7596 Loss_G: 4.3163 \n",
            "[16/25][185/391] Loss_D: 0.4321 Loss_G: 3.1663 \n",
            "[16/25][186/391] Loss_D: 0.7839 Loss_G: 0.5482 \n",
            "[16/25][187/391] Loss_D: 1.2410 Loss_G: 5.9318 \n",
            "[16/25][188/391] Loss_D: 1.0504 Loss_G: 1.9281 \n",
            "[16/25][189/391] Loss_D: 0.4858 Loss_G: 3.3461 \n",
            "[16/25][190/391] Loss_D: 0.5203 Loss_G: 2.3625 \n",
            "[16/25][191/391] Loss_D: 0.5396 Loss_G: 2.2409 \n",
            "[16/25][192/391] Loss_D: 0.5846 Loss_G: 4.3657 \n",
            "[16/25][193/391] Loss_D: 0.7520 Loss_G: 1.5104 \n",
            "[16/25][194/391] Loss_D: 0.4473 Loss_G: 3.4986 \n",
            "[16/25][195/391] Loss_D: 0.3041 Loss_G: 3.5278 \n",
            "[16/25][196/391] Loss_D: 0.4752 Loss_G: 1.9132 \n",
            "[16/25][197/391] Loss_D: 0.6138 Loss_G: 3.1438 \n",
            "[16/25][198/391] Loss_D: 0.5769 Loss_G: 2.6801 \n",
            "[16/25][199/391] Loss_D: 0.4704 Loss_G: 2.3446 \n",
            "[16/25][200/391] Loss_D: 0.5331 Loss_G: 2.6987 \n",
            "saving the output\n",
            "[16/25][201/391] Loss_D: 0.3953 Loss_G: 3.8247 \n",
            "[16/25][202/391] Loss_D: 0.5838 Loss_G: 1.8350 \n",
            "[16/25][203/391] Loss_D: 0.5596 Loss_G: 3.1863 \n",
            "[16/25][204/391] Loss_D: 0.3929 Loss_G: 2.8556 \n",
            "[16/25][205/391] Loss_D: 0.4964 Loss_G: 1.7291 \n",
            "[16/25][206/391] Loss_D: 0.6486 Loss_G: 3.8405 \n",
            "[16/25][207/391] Loss_D: 0.7424 Loss_G: 1.6847 \n",
            "[16/25][208/391] Loss_D: 0.5123 Loss_G: 3.2578 \n",
            "[16/25][209/391] Loss_D: 0.3973 Loss_G: 2.8711 \n",
            "[16/25][210/391] Loss_D: 0.4281 Loss_G: 2.1553 \n",
            "[16/25][211/391] Loss_D: 0.5587 Loss_G: 3.2415 \n",
            "[16/25][212/391] Loss_D: 0.5951 Loss_G: 1.4817 \n",
            "[16/25][213/391] Loss_D: 0.6126 Loss_G: 3.3219 \n",
            "[16/25][214/391] Loss_D: 0.5074 Loss_G: 2.1978 \n",
            "[16/25][215/391] Loss_D: 0.5264 Loss_G: 2.5208 \n",
            "[16/25][216/391] Loss_D: 0.4377 Loss_G: 2.6825 \n",
            "[16/25][217/391] Loss_D: 0.4340 Loss_G: 2.4837 \n",
            "[16/25][218/391] Loss_D: 0.3685 Loss_G: 3.3889 \n",
            "[16/25][219/391] Loss_D: 0.5224 Loss_G: 1.7300 \n",
            "[16/25][220/391] Loss_D: 0.4903 Loss_G: 3.3274 \n",
            "[16/25][221/391] Loss_D: 0.5603 Loss_G: 2.2015 \n",
            "[16/25][222/391] Loss_D: 0.4810 Loss_G: 2.7782 \n",
            "[16/25][223/391] Loss_D: 0.3616 Loss_G: 2.8941 \n",
            "[16/25][224/391] Loss_D: 0.4297 Loss_G: 1.9557 \n",
            "[16/25][225/391] Loss_D: 0.5647 Loss_G: 4.0087 \n",
            "[16/25][226/391] Loss_D: 0.6366 Loss_G: 2.0016 \n",
            "[16/25][227/391] Loss_D: 0.4779 Loss_G: 3.0723 \n",
            "[16/25][228/391] Loss_D: 0.4723 Loss_G: 3.0272 \n",
            "[16/25][229/391] Loss_D: 0.4291 Loss_G: 1.8698 \n",
            "[16/25][230/391] Loss_D: 0.6197 Loss_G: 3.4108 \n",
            "[16/25][231/391] Loss_D: 0.4462 Loss_G: 2.7814 \n",
            "[16/25][232/391] Loss_D: 0.5399 Loss_G: 1.6149 \n",
            "[16/25][233/391] Loss_D: 0.6781 Loss_G: 4.1911 \n",
            "[16/25][234/391] Loss_D: 0.6141 Loss_G: 2.1238 \n",
            "[16/25][235/391] Loss_D: 0.5741 Loss_G: 3.9864 \n",
            "[16/25][236/391] Loss_D: 0.6196 Loss_G: 1.3602 \n",
            "[16/25][237/391] Loss_D: 0.6394 Loss_G: 3.7687 \n",
            "[16/25][238/391] Loss_D: 0.5370 Loss_G: 2.1139 \n",
            "[16/25][239/391] Loss_D: 0.4053 Loss_G: 2.9805 \n",
            "[16/25][240/391] Loss_D: 0.3307 Loss_G: 3.7029 \n",
            "[16/25][241/391] Loss_D: 0.7721 Loss_G: 1.0019 \n",
            "[16/25][242/391] Loss_D: 0.7668 Loss_G: 4.7457 \n",
            "[16/25][243/391] Loss_D: 1.0801 Loss_G: 0.8465 \n",
            "[16/25][244/391] Loss_D: 1.0977 Loss_G: 4.9302 \n",
            "[16/25][245/391] Loss_D: 0.9782 Loss_G: 0.5048 \n",
            "[16/25][246/391] Loss_D: 1.4790 Loss_G: 7.2335 \n",
            "[16/25][247/391] Loss_D: 1.9158 Loss_G: 0.9697 \n",
            "[16/25][248/391] Loss_D: 1.5669 Loss_G: 5.7728 \n",
            "[16/25][249/391] Loss_D: 1.6763 Loss_G: 0.4199 \n",
            "[16/25][250/391] Loss_D: 1.7238 Loss_G: 5.9205 \n",
            "[16/25][251/391] Loss_D: 1.7967 Loss_G: 0.6544 \n",
            "[16/25][252/391] Loss_D: 1.5582 Loss_G: 4.5580 \n",
            "[16/25][253/391] Loss_D: 1.1326 Loss_G: 1.6206 \n",
            "[16/25][254/391] Loss_D: 1.0892 Loss_G: 3.7452 \n",
            "[16/25][255/391] Loss_D: 1.0541 Loss_G: 1.4437 \n",
            "[16/25][256/391] Loss_D: 0.7843 Loss_G: 3.4014 \n",
            "[16/25][257/391] Loss_D: 0.6918 Loss_G: 2.3486 \n",
            "[16/25][258/391] Loss_D: 0.5694 Loss_G: 2.4695 \n",
            "[16/25][259/391] Loss_D: 0.5334 Loss_G: 2.2590 \n",
            "[16/25][260/391] Loss_D: 0.5900 Loss_G: 2.7895 \n",
            "[16/25][261/391] Loss_D: 0.5432 Loss_G: 3.3863 \n",
            "[16/25][262/391] Loss_D: 0.9339 Loss_G: 0.9984 \n",
            "[16/25][263/391] Loss_D: 1.3403 Loss_G: 4.4921 \n",
            "[16/25][264/391] Loss_D: 1.1148 Loss_G: 1.0975 \n",
            "[16/25][265/391] Loss_D: 0.9880 Loss_G: 3.8791 \n",
            "[16/25][266/391] Loss_D: 0.6887 Loss_G: 2.1948 \n",
            "[16/25][267/391] Loss_D: 0.5363 Loss_G: 2.3649 \n",
            "[16/25][268/391] Loss_D: 0.4964 Loss_G: 3.6246 \n",
            "[16/25][269/391] Loss_D: 0.5480 Loss_G: 2.6133 \n",
            "[16/25][270/391] Loss_D: 0.6828 Loss_G: 2.2171 \n",
            "[16/25][271/391] Loss_D: 0.6114 Loss_G: 3.1659 \n",
            "[16/25][272/391] Loss_D: 0.4692 Loss_G: 2.4157 \n",
            "[16/25][273/391] Loss_D: 0.4581 Loss_G: 3.1473 \n",
            "[16/25][274/391] Loss_D: 0.7370 Loss_G: 1.2535 \n",
            "[16/25][275/391] Loss_D: 0.8253 Loss_G: 4.0112 \n",
            "[16/25][276/391] Loss_D: 0.6494 Loss_G: 2.2767 \n",
            "[16/25][277/391] Loss_D: 0.5732 Loss_G: 3.7151 \n",
            "[16/25][278/391] Loss_D: 0.4115 Loss_G: 2.8684 \n",
            "[16/25][279/391] Loss_D: 0.4312 Loss_G: 2.2275 \n",
            "[16/25][280/391] Loss_D: 0.5389 Loss_G: 3.3491 \n",
            "[16/25][281/391] Loss_D: 0.4491 Loss_G: 2.4988 \n",
            "[16/25][282/391] Loss_D: 0.5527 Loss_G: 1.9552 \n",
            "[16/25][283/391] Loss_D: 0.6293 Loss_G: 3.2292 \n",
            "[16/25][284/391] Loss_D: 0.5511 Loss_G: 2.4181 \n",
            "[16/25][285/391] Loss_D: 0.6219 Loss_G: 2.8652 \n",
            "[16/25][286/391] Loss_D: 0.4003 Loss_G: 2.8412 \n",
            "[16/25][287/391] Loss_D: 0.3718 Loss_G: 2.6089 \n",
            "[16/25][288/391] Loss_D: 0.5725 Loss_G: 1.9670 \n",
            "[16/25][289/391] Loss_D: 0.4081 Loss_G: 3.3756 \n",
            "[16/25][290/391] Loss_D: 0.5072 Loss_G: 2.1159 \n",
            "[16/25][291/391] Loss_D: 0.4294 Loss_G: 2.7233 \n",
            "[16/25][292/391] Loss_D: 0.4477 Loss_G: 3.1500 \n",
            "[16/25][293/391] Loss_D: 0.4937 Loss_G: 2.3079 \n",
            "[16/25][294/391] Loss_D: 0.4713 Loss_G: 2.3235 \n",
            "[16/25][295/391] Loss_D: 0.3763 Loss_G: 3.3411 \n",
            "[16/25][296/391] Loss_D: 0.5638 Loss_G: 2.0684 \n",
            "[16/25][297/391] Loss_D: 0.4512 Loss_G: 2.1666 \n",
            "[16/25][298/391] Loss_D: 0.4533 Loss_G: 3.1586 \n",
            "[16/25][299/391] Loss_D: 0.4565 Loss_G: 2.6247 \n",
            "[16/25][300/391] Loss_D: 0.5878 Loss_G: 1.5960 \n",
            "saving the output\n",
            "[16/25][301/391] Loss_D: 0.5382 Loss_G: 4.1760 \n",
            "[16/25][302/391] Loss_D: 0.6110 Loss_G: 2.0378 \n",
            "[16/25][303/391] Loss_D: 0.4841 Loss_G: 2.4453 \n",
            "[16/25][304/391] Loss_D: 0.5002 Loss_G: 2.9732 \n",
            "[16/25][305/391] Loss_D: 0.3196 Loss_G: 3.0353 \n",
            "[16/25][306/391] Loss_D: 0.4882 Loss_G: 1.6322 \n",
            "[16/25][307/391] Loss_D: 0.5546 Loss_G: 3.5952 \n",
            "[16/25][308/391] Loss_D: 0.4038 Loss_G: 2.7209 \n",
            "[16/25][309/391] Loss_D: 0.4151 Loss_G: 2.1221 \n",
            "[16/25][310/391] Loss_D: 0.4194 Loss_G: 3.0584 \n",
            "[16/25][311/391] Loss_D: 0.5891 Loss_G: 2.1798 \n",
            "[16/25][312/391] Loss_D: 0.4228 Loss_G: 3.0640 \n",
            "[16/25][313/391] Loss_D: 0.5511 Loss_G: 1.5964 \n",
            "[16/25][314/391] Loss_D: 0.4968 Loss_G: 3.2844 \n",
            "[16/25][315/391] Loss_D: 0.4397 Loss_G: 2.7368 \n",
            "[16/25][316/391] Loss_D: 0.5685 Loss_G: 1.2480 \n",
            "[16/25][317/391] Loss_D: 0.6036 Loss_G: 3.7823 \n",
            "[16/25][318/391] Loss_D: 0.3845 Loss_G: 3.0777 \n",
            "[16/25][319/391] Loss_D: 0.4171 Loss_G: 1.7178 \n",
            "[16/25][320/391] Loss_D: 0.5447 Loss_G: 3.7895 \n",
            "[16/25][321/391] Loss_D: 0.4386 Loss_G: 2.4751 \n",
            "[16/25][322/391] Loss_D: 0.3436 Loss_G: 2.5979 \n",
            "[16/25][323/391] Loss_D: 0.4950 Loss_G: 2.4079 \n",
            "[16/25][324/391] Loss_D: 0.4365 Loss_G: 2.7554 \n",
            "[16/25][325/391] Loss_D: 0.3217 Loss_G: 2.7357 \n",
            "[16/25][326/391] Loss_D: 0.3509 Loss_G: 2.7190 \n",
            "[16/25][327/391] Loss_D: 0.6236 Loss_G: 1.2364 \n",
            "[16/25][328/391] Loss_D: 0.5652 Loss_G: 3.6084 \n",
            "[16/25][329/391] Loss_D: 0.5679 Loss_G: 2.3267 \n",
            "[16/25][330/391] Loss_D: 0.3331 Loss_G: 2.6754 \n",
            "[16/25][331/391] Loss_D: 0.5506 Loss_G: 2.7090 \n",
            "[16/25][332/391] Loss_D: 0.3655 Loss_G: 2.6952 \n",
            "[16/25][333/391] Loss_D: 0.4639 Loss_G: 1.7104 \n",
            "[16/25][334/391] Loss_D: 0.5382 Loss_G: 3.4773 \n",
            "[16/25][335/391] Loss_D: 0.3142 Loss_G: 3.1459 \n",
            "[16/25][336/391] Loss_D: 0.4465 Loss_G: 1.7035 \n",
            "[16/25][337/391] Loss_D: 0.4998 Loss_G: 3.0029 \n",
            "[16/25][338/391] Loss_D: 0.3703 Loss_G: 2.9811 \n",
            "[16/25][339/391] Loss_D: 0.3927 Loss_G: 2.4094 \n",
            "[16/25][340/391] Loss_D: 0.3930 Loss_G: 3.2190 \n",
            "[16/25][341/391] Loss_D: 0.5454 Loss_G: 1.6662 \n",
            "[16/25][342/391] Loss_D: 0.5209 Loss_G: 3.1803 \n",
            "[16/25][343/391] Loss_D: 0.4850 Loss_G: 2.5017 \n",
            "[16/25][344/391] Loss_D: 0.4277 Loss_G: 2.7851 \n",
            "[16/25][345/391] Loss_D: 0.3481 Loss_G: 2.5439 \n",
            "[16/25][346/391] Loss_D: 0.4631 Loss_G: 2.7645 \n",
            "[16/25][347/391] Loss_D: 0.3552 Loss_G: 2.5586 \n",
            "[16/25][348/391] Loss_D: 0.2827 Loss_G: 2.9373 \n",
            "[16/25][349/391] Loss_D: 0.2804 Loss_G: 2.9639 \n",
            "[16/25][350/391] Loss_D: 0.3751 Loss_G: 2.6572 \n",
            "[16/25][351/391] Loss_D: 0.4540 Loss_G: 2.4802 \n",
            "[16/25][352/391] Loss_D: 0.3743 Loss_G: 3.2427 \n",
            "[16/25][353/391] Loss_D: 0.3697 Loss_G: 2.5579 \n",
            "[16/25][354/391] Loss_D: 0.4223 Loss_G: 2.2456 \n",
            "[16/25][355/391] Loss_D: 0.3763 Loss_G: 3.5888 \n",
            "[16/25][356/391] Loss_D: 0.6769 Loss_G: 1.2563 \n",
            "[16/25][357/391] Loss_D: 0.7167 Loss_G: 3.8187 \n",
            "[16/25][358/391] Loss_D: 0.6631 Loss_G: 1.6665 \n",
            "[16/25][359/391] Loss_D: 0.4816 Loss_G: 3.4680 \n",
            "[16/25][360/391] Loss_D: 0.5059 Loss_G: 2.4353 \n",
            "[16/25][361/391] Loss_D: 0.3124 Loss_G: 2.8409 \n",
            "[16/25][362/391] Loss_D: 0.4202 Loss_G: 2.4660 \n",
            "[16/25][363/391] Loss_D: 0.3788 Loss_G: 2.3924 \n",
            "[16/25][364/391] Loss_D: 0.3688 Loss_G: 2.6439 \n",
            "[16/25][365/391] Loss_D: 0.3471 Loss_G: 2.8007 \n",
            "[16/25][366/391] Loss_D: 0.4662 Loss_G: 2.7851 \n",
            "[16/25][367/391] Loss_D: 0.4663 Loss_G: 1.7703 \n",
            "[16/25][368/391] Loss_D: 0.5744 Loss_G: 3.8827 \n",
            "[16/25][369/391] Loss_D: 0.4073 Loss_G: 2.5100 \n",
            "[16/25][370/391] Loss_D: 0.2297 Loss_G: 2.7073 \n",
            "[16/25][371/391] Loss_D: 0.4710 Loss_G: 2.1028 \n",
            "[16/25][372/391] Loss_D: 0.5513 Loss_G: 3.0018 \n",
            "[16/25][373/391] Loss_D: 0.3834 Loss_G: 2.8248 \n",
            "[16/25][374/391] Loss_D: 0.4059 Loss_G: 2.6482 \n",
            "[16/25][375/391] Loss_D: 0.4786 Loss_G: 1.6511 \n",
            "[16/25][376/391] Loss_D: 0.5775 Loss_G: 3.3200 \n",
            "[16/25][377/391] Loss_D: 0.5539 Loss_G: 1.8808 \n",
            "[16/25][378/391] Loss_D: 0.4534 Loss_G: 2.5487 \n",
            "[16/25][379/391] Loss_D: 0.3249 Loss_G: 3.0716 \n",
            "[16/25][380/391] Loss_D: 0.3382 Loss_G: 3.3314 \n",
            "[16/25][381/391] Loss_D: 0.3519 Loss_G: 2.1256 \n",
            "[16/25][382/391] Loss_D: 0.5198 Loss_G: 2.5477 \n",
            "[16/25][383/391] Loss_D: 0.3809 Loss_G: 3.1507 \n",
            "[16/25][384/391] Loss_D: 0.3793 Loss_G: 2.4153 \n",
            "[16/25][385/391] Loss_D: 0.3883 Loss_G: 2.4550 \n",
            "[16/25][386/391] Loss_D: 0.6577 Loss_G: 3.0995 \n",
            "[16/25][387/391] Loss_D: 0.5324 Loss_G: 1.9386 \n",
            "[16/25][388/391] Loss_D: 0.4287 Loss_G: 3.0094 \n",
            "[16/25][389/391] Loss_D: 0.5252 Loss_G: 2.1417 \n",
            "[16/25][390/391] Loss_D: 0.3901 Loss_G: 2.8974 \n",
            "[17/25][0/391] Loss_D: 0.3518 Loss_G: 2.8379 \n",
            "saving the output\n",
            "[17/25][1/391] Loss_D: 0.4227 Loss_G: 2.8525 \n",
            "[17/25][2/391] Loss_D: 0.3953 Loss_G: 2.4646 \n",
            "[17/25][3/391] Loss_D: 0.4882 Loss_G: 3.2126 \n",
            "[17/25][4/391] Loss_D: 0.4684 Loss_G: 2.1005 \n",
            "[17/25][5/391] Loss_D: 0.4574 Loss_G: 3.8276 \n",
            "[17/25][6/391] Loss_D: 0.5362 Loss_G: 1.6121 \n",
            "[17/25][7/391] Loss_D: 0.5988 Loss_G: 3.9620 \n",
            "[17/25][8/391] Loss_D: 0.5615 Loss_G: 1.8242 \n",
            "[17/25][9/391] Loss_D: 0.4763 Loss_G: 3.4091 \n",
            "[17/25][10/391] Loss_D: 0.3862 Loss_G: 2.3999 \n",
            "[17/25][11/391] Loss_D: 0.3079 Loss_G: 3.4428 \n",
            "[17/25][12/391] Loss_D: 0.4317 Loss_G: 2.2168 \n",
            "[17/25][13/391] Loss_D: 0.3479 Loss_G: 3.1242 \n",
            "[17/25][14/391] Loss_D: 0.5489 Loss_G: 2.0169 \n",
            "[17/25][15/391] Loss_D: 0.4869 Loss_G: 3.9210 \n",
            "[17/25][16/391] Loss_D: 0.4995 Loss_G: 1.9928 \n",
            "[17/25][17/391] Loss_D: 0.4219 Loss_G: 3.2868 \n",
            "[17/25][18/391] Loss_D: 0.5962 Loss_G: 1.7191 \n",
            "[17/25][19/391] Loss_D: 0.5578 Loss_G: 3.9619 \n",
            "[17/25][20/391] Loss_D: 0.6392 Loss_G: 1.5129 \n",
            "[17/25][21/391] Loss_D: 0.5443 Loss_G: 3.6203 \n",
            "[17/25][22/391] Loss_D: 0.4029 Loss_G: 2.4016 \n",
            "[17/25][23/391] Loss_D: 0.4218 Loss_G: 3.3550 \n",
            "[17/25][24/391] Loss_D: 0.3864 Loss_G: 2.8281 \n",
            "[17/25][25/391] Loss_D: 0.3461 Loss_G: 2.4391 \n",
            "[17/25][26/391] Loss_D: 0.4330 Loss_G: 2.4863 \n",
            "[17/25][27/391] Loss_D: 0.3445 Loss_G: 2.8974 \n",
            "[17/25][28/391] Loss_D: 0.4854 Loss_G: 1.9714 \n",
            "[17/25][29/391] Loss_D: 0.4096 Loss_G: 3.2424 \n",
            "[17/25][30/391] Loss_D: 0.3712 Loss_G: 2.4428 \n",
            "[17/25][31/391] Loss_D: 0.4777 Loss_G: 3.4246 \n",
            "[17/25][32/391] Loss_D: 0.3818 Loss_G: 2.5597 \n",
            "[17/25][33/391] Loss_D: 0.5071 Loss_G: 2.5182 \n",
            "[17/25][34/391] Loss_D: 0.4024 Loss_G: 2.2359 \n",
            "[17/25][35/391] Loss_D: 0.3969 Loss_G: 2.5690 \n",
            "[17/25][36/391] Loss_D: 0.3135 Loss_G: 3.5036 \n",
            "[17/25][37/391] Loss_D: 0.3588 Loss_G: 2.4011 \n",
            "[17/25][38/391] Loss_D: 0.3337 Loss_G: 3.2484 \n",
            "[17/25][39/391] Loss_D: 0.2333 Loss_G: 3.2134 \n",
            "[17/25][40/391] Loss_D: 0.4622 Loss_G: 1.5464 \n",
            "[17/25][41/391] Loss_D: 0.5754 Loss_G: 4.0677 \n",
            "[17/25][42/391] Loss_D: 0.5030 Loss_G: 2.2223 \n",
            "[17/25][43/391] Loss_D: 0.4726 Loss_G: 3.1154 \n",
            "[17/25][44/391] Loss_D: 0.3264 Loss_G: 2.8625 \n",
            "[17/25][45/391] Loss_D: 0.4769 Loss_G: 2.6629 \n",
            "[17/25][46/391] Loss_D: 0.3715 Loss_G: 2.6334 \n",
            "[17/25][47/391] Loss_D: 0.4269 Loss_G: 1.8468 \n",
            "[17/25][48/391] Loss_D: 0.5108 Loss_G: 3.4365 \n",
            "[17/25][49/391] Loss_D: 0.4701 Loss_G: 2.3672 \n",
            "[17/25][50/391] Loss_D: 0.3900 Loss_G: 3.3769 \n",
            "[17/25][51/391] Loss_D: 0.2719 Loss_G: 2.9231 \n",
            "[17/25][52/391] Loss_D: 0.3469 Loss_G: 2.2528 \n",
            "[17/25][53/391] Loss_D: 0.2975 Loss_G: 3.3000 \n",
            "[17/25][54/391] Loss_D: 0.5440 Loss_G: 1.6957 \n",
            "[17/25][55/391] Loss_D: 0.7017 Loss_G: 4.3951 \n",
            "[17/25][56/391] Loss_D: 1.0531 Loss_G: 0.4900 \n",
            "[17/25][57/391] Loss_D: 1.6219 Loss_G: 4.3651 \n",
            "[17/25][58/391] Loss_D: 0.6684 Loss_G: 0.1585 \n",
            "[17/25][59/391] Loss_D: 2.7366 Loss_G: 10.6745 \n",
            "[17/25][60/391] Loss_D: 6.4424 Loss_G: 1.2572 \n",
            "[17/25][61/391] Loss_D: 1.1393 Loss_G: 3.7530 \n",
            "[17/25][62/391] Loss_D: 0.9016 Loss_G: 0.5112 \n",
            "[17/25][63/391] Loss_D: 1.9968 Loss_G: 9.4655 \n",
            "[17/25][64/391] Loss_D: 4.4304 Loss_G: 0.1954 \n",
            "[17/25][65/391] Loss_D: 2.4903 Loss_G: 6.9023 \n",
            "[17/25][66/391] Loss_D: 3.2073 Loss_G: 0.2126 \n",
            "[17/25][67/391] Loss_D: 2.4334 Loss_G: 5.2356 \n",
            "[17/25][68/391] Loss_D: 1.9151 Loss_G: 0.3990 \n",
            "[17/25][69/391] Loss_D: 2.4602 Loss_G: 5.1062 \n",
            "[17/25][70/391] Loss_D: 1.6242 Loss_G: 0.9312 \n",
            "[17/25][71/391] Loss_D: 1.2140 Loss_G: 3.5705 \n",
            "[17/25][72/391] Loss_D: 1.3889 Loss_G: 1.3651 \n",
            "[17/25][73/391] Loss_D: 1.0104 Loss_G: 3.4959 \n",
            "[17/25][74/391] Loss_D: 0.7852 Loss_G: 2.5923 \n",
            "[17/25][75/391] Loss_D: 1.0089 Loss_G: 2.5719 \n",
            "[17/25][76/391] Loss_D: 1.2013 Loss_G: 1.1214 \n",
            "[17/25][77/391] Loss_D: 1.1243 Loss_G: 5.0411 \n",
            "[17/25][78/391] Loss_D: 1.0385 Loss_G: 1.6107 \n",
            "[17/25][79/391] Loss_D: 0.6971 Loss_G: 3.2608 \n",
            "[17/25][80/391] Loss_D: 0.8477 Loss_G: 1.7220 \n",
            "[17/25][81/391] Loss_D: 0.8479 Loss_G: 4.6329 \n",
            "[17/25][82/391] Loss_D: 1.0226 Loss_G: 1.6972 \n",
            "[17/25][83/391] Loss_D: 0.6959 Loss_G: 3.0157 \n",
            "[17/25][84/391] Loss_D: 0.7678 Loss_G: 2.7404 \n",
            "[17/25][85/391] Loss_D: 0.5072 Loss_G: 2.6670 \n",
            "[17/25][86/391] Loss_D: 0.6464 Loss_G: 2.1687 \n",
            "[17/25][87/391] Loss_D: 0.6295 Loss_G: 3.2601 \n",
            "[17/25][88/391] Loss_D: 0.6468 Loss_G: 1.7578 \n",
            "[17/25][89/391] Loss_D: 0.9594 Loss_G: 4.7824 \n",
            "[17/25][90/391] Loss_D: 1.2296 Loss_G: 1.3233 \n",
            "[17/25][91/391] Loss_D: 1.1179 Loss_G: 4.2531 \n",
            "[17/25][92/391] Loss_D: 0.9007 Loss_G: 1.5964 \n",
            "[17/25][93/391] Loss_D: 0.6770 Loss_G: 3.3387 \n",
            "[17/25][94/391] Loss_D: 0.6121 Loss_G: 2.3030 \n",
            "[17/25][95/391] Loss_D: 0.4736 Loss_G: 2.9818 \n",
            "[17/25][96/391] Loss_D: 0.7545 Loss_G: 1.7285 \n",
            "[17/25][97/391] Loss_D: 0.8432 Loss_G: 3.8205 \n",
            "[17/25][98/391] Loss_D: 0.9323 Loss_G: 1.3263 \n",
            "[17/25][99/391] Loss_D: 0.7231 Loss_G: 3.1294 \n",
            "[17/25][100/391] Loss_D: 0.3293 Loss_G: 3.2204 \n",
            "saving the output\n",
            "[17/25][101/391] Loss_D: 0.3743 Loss_G: 2.5013 \n",
            "[17/25][102/391] Loss_D: 0.4177 Loss_G: 3.1037 \n",
            "[17/25][103/391] Loss_D: 0.4739 Loss_G: 2.2215 \n",
            "[17/25][104/391] Loss_D: 0.4639 Loss_G: 3.2692 \n",
            "[17/25][105/391] Loss_D: 0.7568 Loss_G: 1.2139 \n",
            "[17/25][106/391] Loss_D: 0.7161 Loss_G: 3.5200 \n",
            "[17/25][107/391] Loss_D: 0.4799 Loss_G: 2.4252 \n",
            "[17/25][108/391] Loss_D: 0.4909 Loss_G: 2.8633 \n",
            "[17/25][109/391] Loss_D: 0.6380 Loss_G: 1.7603 \n",
            "[17/25][110/391] Loss_D: 0.5612 Loss_G: 2.8985 \n",
            "[17/25][111/391] Loss_D: 0.4693 Loss_G: 2.3765 \n",
            "[17/25][112/391] Loss_D: 0.5164 Loss_G: 2.4793 \n",
            "[17/25][113/391] Loss_D: 0.5721 Loss_G: 2.2849 \n",
            "[17/25][114/391] Loss_D: 0.3708 Loss_G: 2.9100 \n",
            "[17/25][115/391] Loss_D: 0.4412 Loss_G: 2.9206 \n",
            "[17/25][116/391] Loss_D: 0.5377 Loss_G: 1.6741 \n",
            "[17/25][117/391] Loss_D: 0.8025 Loss_G: 2.6657 \n",
            "[17/25][118/391] Loss_D: 0.3837 Loss_G: 3.2371 \n",
            "[17/25][119/391] Loss_D: 0.6062 Loss_G: 1.5496 \n",
            "[17/25][120/391] Loss_D: 0.5341 Loss_G: 3.5418 \n",
            "[17/25][121/391] Loss_D: 0.4743 Loss_G: 2.3042 \n",
            "[17/25][122/391] Loss_D: 0.6270 Loss_G: 1.8468 \n",
            "[17/25][123/391] Loss_D: 0.7430 Loss_G: 3.5281 \n",
            "[17/25][124/391] Loss_D: 0.5628 Loss_G: 2.2067 \n",
            "[17/25][125/391] Loss_D: 0.4976 Loss_G: 1.9720 \n",
            "[17/25][126/391] Loss_D: 0.5466 Loss_G: 3.1217 \n",
            "[17/25][127/391] Loss_D: 0.6613 Loss_G: 1.7245 \n",
            "[17/25][128/391] Loss_D: 0.4706 Loss_G: 2.6006 \n",
            "[17/25][129/391] Loss_D: 0.4977 Loss_G: 2.1772 \n",
            "[17/25][130/391] Loss_D: 0.3583 Loss_G: 2.9754 \n",
            "[17/25][131/391] Loss_D: 0.4009 Loss_G: 2.6741 \n",
            "[17/25][132/391] Loss_D: 0.4767 Loss_G: 2.3926 \n",
            "[17/25][133/391] Loss_D: 0.3894 Loss_G: 2.4407 \n",
            "[17/25][134/391] Loss_D: 0.4516 Loss_G: 2.4482 \n",
            "[17/25][135/391] Loss_D: 0.4919 Loss_G: 2.1069 \n",
            "[17/25][136/391] Loss_D: 0.5183 Loss_G: 2.3882 \n",
            "[17/25][137/391] Loss_D: 0.4191 Loss_G: 3.0543 \n",
            "[17/25][138/391] Loss_D: 0.4526 Loss_G: 2.2172 \n",
            "[17/25][139/391] Loss_D: 0.4094 Loss_G: 2.8877 \n",
            "[17/25][140/391] Loss_D: 0.4087 Loss_G: 2.5959 \n",
            "[17/25][141/391] Loss_D: 0.3728 Loss_G: 2.4803 \n",
            "[17/25][142/391] Loss_D: 0.3736 Loss_G: 2.5307 \n",
            "[17/25][143/391] Loss_D: 0.4182 Loss_G: 3.0355 \n",
            "[17/25][144/391] Loss_D: 0.4350 Loss_G: 2.2839 \n",
            "[17/25][145/391] Loss_D: 0.4275 Loss_G: 2.6310 \n",
            "[17/25][146/391] Loss_D: 0.4396 Loss_G: 2.6855 \n",
            "[17/25][147/391] Loss_D: 0.5124 Loss_G: 2.0665 \n",
            "[17/25][148/391] Loss_D: 0.4076 Loss_G: 2.6551 \n",
            "[17/25][149/391] Loss_D: 0.3935 Loss_G: 2.2548 \n",
            "[17/25][150/391] Loss_D: 0.4599 Loss_G: 3.2293 \n",
            "[17/25][151/391] Loss_D: 0.3770 Loss_G: 2.6531 \n",
            "[17/25][152/391] Loss_D: 0.4873 Loss_G: 1.9274 \n",
            "[17/25][153/391] Loss_D: 0.5254 Loss_G: 2.8922 \n",
            "[17/25][154/391] Loss_D: 0.4385 Loss_G: 2.3399 \n",
            "[17/25][155/391] Loss_D: 0.4159 Loss_G: 2.8816 \n",
            "[17/25][156/391] Loss_D: 0.3311 Loss_G: 2.5286 \n",
            "[17/25][157/391] Loss_D: 0.5296 Loss_G: 2.6414 \n",
            "[17/25][158/391] Loss_D: 0.5526 Loss_G: 2.3911 \n",
            "[17/25][159/391] Loss_D: 0.3731 Loss_G: 2.5843 \n",
            "[17/25][160/391] Loss_D: 0.3300 Loss_G: 2.8288 \n",
            "[17/25][161/391] Loss_D: 0.3866 Loss_G: 2.4716 \n",
            "[17/25][162/391] Loss_D: 0.3513 Loss_G: 2.8018 \n",
            "[17/25][163/391] Loss_D: 0.4944 Loss_G: 1.8432 \n",
            "[17/25][164/391] Loss_D: 0.5676 Loss_G: 2.9037 \n",
            "[17/25][165/391] Loss_D: 0.3285 Loss_G: 2.9255 \n",
            "[17/25][166/391] Loss_D: 0.3091 Loss_G: 2.2641 \n",
            "[17/25][167/391] Loss_D: 0.6112 Loss_G: 2.9274 \n",
            "[17/25][168/391] Loss_D: 0.3470 Loss_G: 2.9821 \n",
            "[17/25][169/391] Loss_D: 0.3558 Loss_G: 2.4004 \n",
            "[17/25][170/391] Loss_D: 0.3395 Loss_G: 2.7370 \n",
            "[17/25][171/391] Loss_D: 0.4378 Loss_G: 2.5339 \n",
            "[17/25][172/391] Loss_D: 0.4084 Loss_G: 2.0222 \n",
            "[17/25][173/391] Loss_D: 0.5449 Loss_G: 2.6429 \n",
            "[17/25][174/391] Loss_D: 0.4521 Loss_G: 2.6072 \n",
            "[17/25][175/391] Loss_D: 0.3495 Loss_G: 2.2833 \n",
            "[17/25][176/391] Loss_D: 0.4084 Loss_G: 3.1300 \n",
            "[17/25][177/391] Loss_D: 0.5986 Loss_G: 1.2768 \n",
            "[17/25][178/391] Loss_D: 0.6017 Loss_G: 4.1941 \n",
            "[17/25][179/391] Loss_D: 0.3052 Loss_G: 3.2306 \n",
            "[17/25][180/391] Loss_D: 0.6008 Loss_G: 0.6047 \n",
            "[17/25][181/391] Loss_D: 1.3270 Loss_G: 5.6803 \n",
            "[17/25][182/391] Loss_D: 0.9712 Loss_G: 0.9072 \n",
            "[17/25][183/391] Loss_D: 1.1321 Loss_G: 4.7032 \n",
            "[17/25][184/391] Loss_D: 1.0101 Loss_G: 1.5783 \n",
            "[17/25][185/391] Loss_D: 0.9109 Loss_G: 3.1508 \n",
            "[17/25][186/391] Loss_D: 0.6559 Loss_G: 1.9812 \n",
            "[17/25][187/391] Loss_D: 0.6659 Loss_G: 3.8718 \n",
            "[17/25][188/391] Loss_D: 0.8268 Loss_G: 1.3301 \n",
            "[17/25][189/391] Loss_D: 1.0485 Loss_G: 4.1097 \n",
            "[17/25][190/391] Loss_D: 0.5124 Loss_G: 2.6504 \n",
            "[17/25][191/391] Loss_D: 0.4101 Loss_G: 2.1786 \n",
            "[17/25][192/391] Loss_D: 0.4212 Loss_G: 3.0353 \n",
            "[17/25][193/391] Loss_D: 0.5745 Loss_G: 2.9790 \n",
            "[17/25][194/391] Loss_D: 0.6514 Loss_G: 1.7545 \n",
            "[17/25][195/391] Loss_D: 0.4665 Loss_G: 2.9723 \n",
            "[17/25][196/391] Loss_D: 0.5578 Loss_G: 1.8267 \n",
            "[17/25][197/391] Loss_D: 0.5447 Loss_G: 3.5870 \n",
            "[17/25][198/391] Loss_D: 0.6163 Loss_G: 2.1760 \n",
            "[17/25][199/391] Loss_D: 0.4817 Loss_G: 1.7152 \n",
            "[17/25][200/391] Loss_D: 0.5782 Loss_G: 3.4080 \n",
            "saving the output\n",
            "[17/25][201/391] Loss_D: 0.5067 Loss_G: 2.1594 \n",
            "[17/25][202/391] Loss_D: 0.3512 Loss_G: 3.3000 \n",
            "[17/25][203/391] Loss_D: 0.4418 Loss_G: 2.4969 \n",
            "[17/25][204/391] Loss_D: 0.3837 Loss_G: 2.6221 \n",
            "[17/25][205/391] Loss_D: 0.4444 Loss_G: 2.5576 \n",
            "[17/25][206/391] Loss_D: 0.4843 Loss_G: 2.9557 \n",
            "[17/25][207/391] Loss_D: 0.4192 Loss_G: 2.4986 \n",
            "[17/25][208/391] Loss_D: 0.2948 Loss_G: 3.2458 \n",
            "[17/25][209/391] Loss_D: 0.4032 Loss_G: 2.2664 \n",
            "[17/25][210/391] Loss_D: 0.4355 Loss_G: 2.9512 \n",
            "[17/25][211/391] Loss_D: 0.4223 Loss_G: 2.4971 \n",
            "[17/25][212/391] Loss_D: 0.5591 Loss_G: 2.1989 \n",
            "[17/25][213/391] Loss_D: 0.4971 Loss_G: 2.5318 \n",
            "[17/25][214/391] Loss_D: 0.3185 Loss_G: 3.0009 \n",
            "[17/25][215/391] Loss_D: 0.2601 Loss_G: 2.9828 \n",
            "[17/25][216/391] Loss_D: 0.3601 Loss_G: 2.6782 \n",
            "[17/25][217/391] Loss_D: 0.4830 Loss_G: 1.7482 \n",
            "[17/25][218/391] Loss_D: 0.5263 Loss_G: 3.5436 \n",
            "[17/25][219/391] Loss_D: 0.4618 Loss_G: 2.4435 \n",
            "[17/25][220/391] Loss_D: 0.2665 Loss_G: 3.2708 \n",
            "[17/25][221/391] Loss_D: 0.4774 Loss_G: 2.3146 \n",
            "[17/25][222/391] Loss_D: 0.4379 Loss_G: 2.8043 \n",
            "[17/25][223/391] Loss_D: 0.5516 Loss_G: 1.4585 \n",
            "[17/25][224/391] Loss_D: 0.5746 Loss_G: 3.5874 \n",
            "[17/25][225/391] Loss_D: 0.5395 Loss_G: 1.8901 \n",
            "[17/25][226/391] Loss_D: 0.4630 Loss_G: 2.4899 \n",
            "[17/25][227/391] Loss_D: 0.5263 Loss_G: 3.0904 \n",
            "[17/25][228/391] Loss_D: 0.4711 Loss_G: 1.9253 \n",
            "[17/25][229/391] Loss_D: 0.4829 Loss_G: 3.1427 \n",
            "[17/25][230/391] Loss_D: 0.5390 Loss_G: 1.9882 \n",
            "[17/25][231/391] Loss_D: 0.3593 Loss_G: 3.4789 \n",
            "[17/25][232/391] Loss_D: 0.4961 Loss_G: 1.8878 \n",
            "[17/25][233/391] Loss_D: 0.6118 Loss_G: 3.1339 \n",
            "[17/25][234/391] Loss_D: 0.6207 Loss_G: 1.4722 \n",
            "[17/25][235/391] Loss_D: 0.7907 Loss_G: 3.9896 \n",
            "[17/25][236/391] Loss_D: 0.6875 Loss_G: 1.6270 \n",
            "[17/25][237/391] Loss_D: 0.5039 Loss_G: 3.0734 \n",
            "[17/25][238/391] Loss_D: 0.4675 Loss_G: 2.4001 \n",
            "[17/25][239/391] Loss_D: 0.4362 Loss_G: 1.9523 \n",
            "[17/25][240/391] Loss_D: 0.5179 Loss_G: 3.5459 \n",
            "[17/25][241/391] Loss_D: 0.2334 Loss_G: 3.6258 \n",
            "[17/25][242/391] Loss_D: 0.6811 Loss_G: 1.0548 \n",
            "[17/25][243/391] Loss_D: 0.7866 Loss_G: 4.1736 \n",
            "[17/25][244/391] Loss_D: 0.6034 Loss_G: 1.4448 \n",
            "[17/25][245/391] Loss_D: 0.6952 Loss_G: 3.1632 \n",
            "[17/25][246/391] Loss_D: 0.5680 Loss_G: 2.2703 \n",
            "[17/25][247/391] Loss_D: 0.5510 Loss_G: 2.6545 \n",
            "[17/25][248/391] Loss_D: 0.3868 Loss_G: 3.1431 \n",
            "[17/25][249/391] Loss_D: 0.4844 Loss_G: 1.6101 \n",
            "[17/25][250/391] Loss_D: 0.5680 Loss_G: 3.5161 \n",
            "[17/25][251/391] Loss_D: 0.4576 Loss_G: 2.2850 \n",
            "[17/25][252/391] Loss_D: 0.4885 Loss_G: 2.1230 \n",
            "[17/25][253/391] Loss_D: 0.3893 Loss_G: 4.0497 \n",
            "[17/25][254/391] Loss_D: 0.5203 Loss_G: 1.9727 \n",
            "[17/25][255/391] Loss_D: 0.4932 Loss_G: 2.0412 \n",
            "[17/25][256/391] Loss_D: 0.5472 Loss_G: 3.4671 \n",
            "[17/25][257/391] Loss_D: 0.3402 Loss_G: 3.2517 \n",
            "[17/25][258/391] Loss_D: 0.5722 Loss_G: 0.8266 \n",
            "[17/25][259/391] Loss_D: 1.4695 Loss_G: 5.3213 \n",
            "[17/25][260/391] Loss_D: 0.9924 Loss_G: 0.5286 \n",
            "[17/25][261/391] Loss_D: 1.4453 Loss_G: 7.0923 \n",
            "[17/25][262/391] Loss_D: 2.2335 Loss_G: 0.9560 \n",
            "[17/25][263/391] Loss_D: 1.1486 Loss_G: 3.6108 \n",
            "[17/25][264/391] Loss_D: 0.9593 Loss_G: 1.1135 \n",
            "[17/25][265/391] Loss_D: 1.2239 Loss_G: 4.3030 \n",
            "[17/25][266/391] Loss_D: 1.1584 Loss_G: 1.0470 \n",
            "[17/25][267/391] Loss_D: 1.1680 Loss_G: 3.9413 \n",
            "[17/25][268/391] Loss_D: 1.1231 Loss_G: 1.5644 \n",
            "[17/25][269/391] Loss_D: 0.7024 Loss_G: 3.3884 \n",
            "[17/25][270/391] Loss_D: 0.9109 Loss_G: 1.2274 \n",
            "[17/25][271/391] Loss_D: 1.2395 Loss_G: 4.4915 \n",
            "[17/25][272/391] Loss_D: 1.4165 Loss_G: 0.9226 \n",
            "[17/25][273/391] Loss_D: 1.2257 Loss_G: 4.0230 \n",
            "[17/25][274/391] Loss_D: 0.7470 Loss_G: 2.1239 \n",
            "[17/25][275/391] Loss_D: 0.4544 Loss_G: 3.1149 \n",
            "[17/25][276/391] Loss_D: 0.5901 Loss_G: 2.5521 \n",
            "[17/25][277/391] Loss_D: 0.5829 Loss_G: 2.6222 \n",
            "[17/25][278/391] Loss_D: 0.6195 Loss_G: 2.6792 \n",
            "[17/25][279/391] Loss_D: 0.7673 Loss_G: 3.3352 \n",
            "[17/25][280/391] Loss_D: 0.5516 Loss_G: 2.3314 \n",
            "[17/25][281/391] Loss_D: 0.7568 Loss_G: 2.5823 \n",
            "[17/25][282/391] Loss_D: 0.4457 Loss_G: 3.2374 \n",
            "[17/25][283/391] Loss_D: 0.7641 Loss_G: 1.4550 \n",
            "[17/25][284/391] Loss_D: 0.9146 Loss_G: 5.3427 \n",
            "[17/25][285/391] Loss_D: 0.9642 Loss_G: 1.8683 \n",
            "[17/25][286/391] Loss_D: 0.6715 Loss_G: 3.0709 \n",
            "[17/25][287/391] Loss_D: 0.4402 Loss_G: 4.2333 \n",
            "[17/25][288/391] Loss_D: 0.6285 Loss_G: 1.8098 \n",
            "[17/25][289/391] Loss_D: 0.6464 Loss_G: 3.9567 \n",
            "[17/25][290/391] Loss_D: 0.3642 Loss_G: 3.2939 \n",
            "[17/25][291/391] Loss_D: 0.3245 Loss_G: 2.5921 \n",
            "[17/25][292/391] Loss_D: 0.3488 Loss_G: 3.3504 \n",
            "[17/25][293/391] Loss_D: 0.4038 Loss_G: 2.2004 \n",
            "[17/25][294/391] Loss_D: 0.6144 Loss_G: 4.4004 \n",
            "[17/25][295/391] Loss_D: 0.9459 Loss_G: 0.8367 \n",
            "[17/25][296/391] Loss_D: 0.9113 Loss_G: 4.1623 \n",
            "[17/25][297/391] Loss_D: 0.7899 Loss_G: 1.2911 \n",
            "[17/25][298/391] Loss_D: 0.8328 Loss_G: 4.4066 \n",
            "[17/25][299/391] Loss_D: 0.5242 Loss_G: 2.5489 \n",
            "[17/25][300/391] Loss_D: 0.5664 Loss_G: 1.2505 \n",
            "saving the output\n",
            "[17/25][301/391] Loss_D: 0.7793 Loss_G: 4.1686 \n",
            "[17/25][302/391] Loss_D: 0.4135 Loss_G: 2.8969 \n",
            "[17/25][303/391] Loss_D: 0.2738 Loss_G: 2.9153 \n",
            "[17/25][304/391] Loss_D: 0.5140 Loss_G: 2.6075 \n",
            "[17/25][305/391] Loss_D: 0.4283 Loss_G: 2.9737 \n",
            "[17/25][306/391] Loss_D: 0.7949 Loss_G: 0.9295 \n",
            "[17/25][307/391] Loss_D: 0.9956 Loss_G: 4.1798 \n",
            "[17/25][308/391] Loss_D: 0.8914 Loss_G: 0.6245 \n",
            "[17/25][309/391] Loss_D: 1.2052 Loss_G: 5.0130 \n",
            "[17/25][310/391] Loss_D: 1.1559 Loss_G: 0.2236 \n",
            "[17/25][311/391] Loss_D: 2.1904 Loss_G: 6.0508 \n",
            "[17/25][312/391] Loss_D: 2.0745 Loss_G: 1.3787 \n",
            "[17/25][313/391] Loss_D: 0.9433 Loss_G: 2.8459 \n",
            "[17/25][314/391] Loss_D: 0.8733 Loss_G: 1.5601 \n",
            "[17/25][315/391] Loss_D: 0.9495 Loss_G: 3.4378 \n",
            "[17/25][316/391] Loss_D: 1.0230 Loss_G: 1.1971 \n",
            "[17/25][317/391] Loss_D: 0.9249 Loss_G: 3.7315 \n",
            "[17/25][318/391] Loss_D: 0.8898 Loss_G: 1.2761 \n",
            "[17/25][319/391] Loss_D: 0.8176 Loss_G: 3.1373 \n",
            "[17/25][320/391] Loss_D: 0.5681 Loss_G: 3.2170 \n",
            "[17/25][321/391] Loss_D: 0.3755 Loss_G: 3.2163 \n",
            "[17/25][322/391] Loss_D: 0.8392 Loss_G: 0.6845 \n",
            "[17/25][323/391] Loss_D: 1.1285 Loss_G: 5.3016 \n",
            "[17/25][324/391] Loss_D: 1.0339 Loss_G: 1.6099 \n",
            "[17/25][325/391] Loss_D: 0.6554 Loss_G: 3.5674 \n",
            "[17/25][326/391] Loss_D: 0.4531 Loss_G: 3.1090 \n",
            "[17/25][327/391] Loss_D: 0.5125 Loss_G: 1.7895 \n",
            "[17/25][328/391] Loss_D: 0.7364 Loss_G: 3.2021 \n",
            "[17/25][329/391] Loss_D: 0.5940 Loss_G: 2.7326 \n",
            "[17/25][330/391] Loss_D: 0.5038 Loss_G: 2.6562 \n",
            "[17/25][331/391] Loss_D: 0.6484 Loss_G: 1.5985 \n",
            "[17/25][332/391] Loss_D: 0.8914 Loss_G: 3.6377 \n",
            "[17/25][333/391] Loss_D: 0.5517 Loss_G: 2.1420 \n",
            "[17/25][334/391] Loss_D: 0.5138 Loss_G: 2.1265 \n",
            "[17/25][335/391] Loss_D: 0.4744 Loss_G: 2.9766 \n",
            "[17/25][336/391] Loss_D: 0.4510 Loss_G: 3.2080 \n",
            "[17/25][337/391] Loss_D: 0.7058 Loss_G: 1.1365 \n",
            "[17/25][338/391] Loss_D: 0.7700 Loss_G: 3.8559 \n",
            "[17/25][339/391] Loss_D: 0.5573 Loss_G: 2.5768 \n",
            "[17/25][340/391] Loss_D: 0.4767 Loss_G: 1.6350 \n",
            "[17/25][341/391] Loss_D: 0.6163 Loss_G: 4.0713 \n",
            "[17/25][342/391] Loss_D: 0.6473 Loss_G: 1.6332 \n",
            "[17/25][343/391] Loss_D: 0.6284 Loss_G: 3.7388 \n",
            "[17/25][344/391] Loss_D: 0.5268 Loss_G: 2.1043 \n",
            "[17/25][345/391] Loss_D: 0.5372 Loss_G: 2.0282 \n",
            "[17/25][346/391] Loss_D: 0.5128 Loss_G: 3.1582 \n",
            "[17/25][347/391] Loss_D: 0.5841 Loss_G: 2.0751 \n",
            "[17/25][348/391] Loss_D: 0.4259 Loss_G: 2.6681 \n",
            "[17/25][349/391] Loss_D: 0.5103 Loss_G: 2.5137 \n",
            "[17/25][350/391] Loss_D: 0.3368 Loss_G: 2.7310 \n",
            "[17/25][351/391] Loss_D: 0.5794 Loss_G: 2.4052 \n",
            "[17/25][352/391] Loss_D: 0.4146 Loss_G: 2.2199 \n",
            "[17/25][353/391] Loss_D: 0.5197 Loss_G: 2.6386 \n",
            "[17/25][354/391] Loss_D: 0.4603 Loss_G: 2.1418 \n",
            "[17/25][355/391] Loss_D: 0.3789 Loss_G: 2.9077 \n",
            "[17/25][356/391] Loss_D: 0.3644 Loss_G: 2.6434 \n",
            "[17/25][357/391] Loss_D: 0.4608 Loss_G: 2.2658 \n",
            "[17/25][358/391] Loss_D: 0.3545 Loss_G: 2.9431 \n",
            "[17/25][359/391] Loss_D: 0.3998 Loss_G: 2.6792 \n",
            "[17/25][360/391] Loss_D: 0.3100 Loss_G: 2.9476 \n",
            "[17/25][361/391] Loss_D: 0.5078 Loss_G: 1.6972 \n",
            "[17/25][362/391] Loss_D: 0.7304 Loss_G: 3.3318 \n",
            "[17/25][363/391] Loss_D: 0.2850 Loss_G: 3.4858 \n",
            "[17/25][364/391] Loss_D: 0.6710 Loss_G: 1.3394 \n",
            "[17/25][365/391] Loss_D: 0.7251 Loss_G: 3.3684 \n",
            "[17/25][366/391] Loss_D: 0.4131 Loss_G: 2.5987 \n",
            "[17/25][367/391] Loss_D: 0.4019 Loss_G: 2.9398 \n",
            "[17/25][368/391] Loss_D: 0.3729 Loss_G: 2.9693 \n",
            "[17/25][369/391] Loss_D: 0.3339 Loss_G: 2.3649 \n",
            "[17/25][370/391] Loss_D: 0.4398 Loss_G: 2.7509 \n",
            "[17/25][371/391] Loss_D: 0.4462 Loss_G: 3.3763 \n",
            "[17/25][372/391] Loss_D: 0.5165 Loss_G: 1.9249 \n",
            "[17/25][373/391] Loss_D: 0.3678 Loss_G: 2.4708 \n",
            "[17/25][374/391] Loss_D: 0.4876 Loss_G: 2.3151 \n",
            "[17/25][375/391] Loss_D: 0.4904 Loss_G: 2.6776 \n",
            "[17/25][376/391] Loss_D: 0.4375 Loss_G: 2.3909 \n",
            "[17/25][377/391] Loss_D: 0.3164 Loss_G: 3.4137 \n",
            "[17/25][378/391] Loss_D: 0.7669 Loss_G: 1.0615 \n",
            "[17/25][379/391] Loss_D: 0.8178 Loss_G: 4.1592 \n",
            "[17/25][380/391] Loss_D: 0.3908 Loss_G: 2.9715 \n",
            "[17/25][381/391] Loss_D: 0.4248 Loss_G: 2.1303 \n",
            "[17/25][382/391] Loss_D: 0.4767 Loss_G: 2.6692 \n",
            "[17/25][383/391] Loss_D: 0.5396 Loss_G: 2.6733 \n",
            "[17/25][384/391] Loss_D: 0.5126 Loss_G: 1.9204 \n",
            "[17/25][385/391] Loss_D: 0.5615 Loss_G: 3.8862 \n",
            "[17/25][386/391] Loss_D: 0.9185 Loss_G: 1.0704 \n",
            "[17/25][387/391] Loss_D: 0.5297 Loss_G: 3.4588 \n",
            "[17/25][388/391] Loss_D: 0.5312 Loss_G: 2.4413 \n",
            "[17/25][389/391] Loss_D: 0.3366 Loss_G: 2.7367 \n",
            "[17/25][390/391] Loss_D: 0.5207 Loss_G: 1.9625 \n",
            "[18/25][0/391] Loss_D: 0.4903 Loss_G: 4.2626 \n",
            "saving the output\n",
            "[18/25][1/391] Loss_D: 0.7808 Loss_G: 0.8826 \n",
            "[18/25][2/391] Loss_D: 0.7485 Loss_G: 4.0545 \n",
            "[18/25][3/391] Loss_D: 0.3687 Loss_G: 3.0222 \n",
            "[18/25][4/391] Loss_D: 0.5488 Loss_G: 1.0717 \n",
            "[18/25][5/391] Loss_D: 0.8616 Loss_G: 4.5467 \n",
            "[18/25][6/391] Loss_D: 0.9064 Loss_G: 0.8820 \n",
            "[18/25][7/391] Loss_D: 1.0082 Loss_G: 4.9161 \n",
            "[18/25][8/391] Loss_D: 1.3295 Loss_G: 0.9489 \n",
            "[18/25][9/391] Loss_D: 1.1230 Loss_G: 4.5414 \n",
            "[18/25][10/391] Loss_D: 0.4789 Loss_G: 2.2466 \n",
            "[18/25][11/391] Loss_D: 0.4794 Loss_G: 3.5331 \n",
            "[18/25][12/391] Loss_D: 0.5462 Loss_G: 1.6128 \n",
            "[18/25][13/391] Loss_D: 0.8730 Loss_G: 4.5905 \n",
            "[18/25][14/391] Loss_D: 0.8978 Loss_G: 1.0293 \n",
            "[18/25][15/391] Loss_D: 0.7410 Loss_G: 3.6045 \n",
            "[18/25][16/391] Loss_D: 0.4944 Loss_G: 2.7647 \n",
            "[18/25][17/391] Loss_D: 0.6631 Loss_G: 1.6750 \n",
            "[18/25][18/391] Loss_D: 0.5852 Loss_G: 3.3953 \n",
            "[18/25][19/391] Loss_D: 0.3949 Loss_G: 2.7478 \n",
            "[18/25][20/391] Loss_D: 0.6004 Loss_G: 1.3888 \n",
            "[18/25][21/391] Loss_D: 0.7711 Loss_G: 3.9624 \n",
            "[18/25][22/391] Loss_D: 0.5750 Loss_G: 2.4316 \n",
            "[18/25][23/391] Loss_D: 0.3666 Loss_G: 2.0019 \n",
            "[18/25][24/391] Loss_D: 0.6633 Loss_G: 3.5429 \n",
            "[18/25][25/391] Loss_D: 0.4109 Loss_G: 2.7206 \n",
            "[18/25][26/391] Loss_D: 0.5638 Loss_G: 1.9374 \n",
            "[18/25][27/391] Loss_D: 0.4972 Loss_G: 2.8748 \n",
            "[18/25][28/391] Loss_D: 0.5878 Loss_G: 1.8387 \n",
            "[18/25][29/391] Loss_D: 0.4768 Loss_G: 3.2274 \n",
            "[18/25][30/391] Loss_D: 0.2976 Loss_G: 2.9823 \n",
            "[18/25][31/391] Loss_D: 0.4714 Loss_G: 1.7084 \n",
            "[18/25][32/391] Loss_D: 0.6438 Loss_G: 3.8446 \n",
            "[18/25][33/391] Loss_D: 0.6519 Loss_G: 1.7074 \n",
            "[18/25][34/391] Loss_D: 0.4441 Loss_G: 2.7593 \n",
            "[18/25][35/391] Loss_D: 0.4356 Loss_G: 2.9735 \n",
            "[18/25][36/391] Loss_D: 0.4628 Loss_G: 2.8149 \n",
            "[18/25][37/391] Loss_D: 0.2862 Loss_G: 2.6087 \n",
            "[18/25][38/391] Loss_D: 0.3373 Loss_G: 2.8184 \n",
            "[18/25][39/391] Loss_D: 0.3725 Loss_G: 2.7675 \n",
            "[18/25][40/391] Loss_D: 0.3416 Loss_G: 2.6551 \n",
            "[18/25][41/391] Loss_D: 0.4008 Loss_G: 2.4360 \n",
            "[18/25][42/391] Loss_D: 0.2603 Loss_G: 3.1179 \n",
            "[18/25][43/391] Loss_D: 0.3607 Loss_G: 3.0904 \n",
            "[18/25][44/391] Loss_D: 0.4670 Loss_G: 1.8569 \n",
            "[18/25][45/391] Loss_D: 0.5953 Loss_G: 2.9870 \n",
            "[18/25][46/391] Loss_D: 0.3795 Loss_G: 2.5569 \n",
            "[18/25][47/391] Loss_D: 0.5044 Loss_G: 2.8981 \n",
            "[18/25][48/391] Loss_D: 0.5295 Loss_G: 1.7738 \n",
            "[18/25][49/391] Loss_D: 0.5023 Loss_G: 3.1733 \n",
            "[18/25][50/391] Loss_D: 0.5339 Loss_G: 2.1772 \n",
            "[18/25][51/391] Loss_D: 0.5201 Loss_G: 3.2668 \n",
            "[18/25][52/391] Loss_D: 0.5076 Loss_G: 1.9776 \n",
            "[18/25][53/391] Loss_D: 0.4272 Loss_G: 2.3085 \n",
            "[18/25][54/391] Loss_D: 0.4183 Loss_G: 3.8308 \n",
            "[18/25][55/391] Loss_D: 0.5807 Loss_G: 1.6792 \n",
            "[18/25][56/391] Loss_D: 0.4475 Loss_G: 2.4509 \n",
            "[18/25][57/391] Loss_D: 0.5485 Loss_G: 3.3653 \n",
            "[18/25][58/391] Loss_D: 0.5807 Loss_G: 1.7247 \n",
            "[18/25][59/391] Loss_D: 0.4871 Loss_G: 3.0177 \n",
            "[18/25][60/391] Loss_D: 0.4365 Loss_G: 2.7741 \n",
            "[18/25][61/391] Loss_D: 0.3558 Loss_G: 2.5158 \n",
            "[18/25][62/391] Loss_D: 0.3368 Loss_G: 2.9399 \n",
            "[18/25][63/391] Loss_D: 0.3597 Loss_G: 2.9569 \n",
            "[18/25][64/391] Loss_D: 0.4656 Loss_G: 1.9644 \n",
            "[18/25][65/391] Loss_D: 0.5703 Loss_G: 3.0546 \n",
            "[18/25][66/391] Loss_D: 0.3344 Loss_G: 2.9217 \n",
            "[18/25][67/391] Loss_D: 0.4156 Loss_G: 2.0049 \n",
            "[18/25][68/391] Loss_D: 0.5204 Loss_G: 3.7530 \n",
            "[18/25][69/391] Loss_D: 0.9463 Loss_G: 0.8598 \n",
            "[18/25][70/391] Loss_D: 1.0856 Loss_G: 4.5930 \n",
            "[18/25][71/391] Loss_D: 0.8129 Loss_G: 0.7961 \n",
            "[18/25][72/391] Loss_D: 0.8819 Loss_G: 4.4945 \n",
            "[18/25][73/391] Loss_D: 0.7054 Loss_G: 1.8886 \n",
            "[18/25][74/391] Loss_D: 0.5542 Loss_G: 2.3765 \n",
            "[18/25][75/391] Loss_D: 0.6980 Loss_G: 2.4059 \n",
            "[18/25][76/391] Loss_D: 0.3765 Loss_G: 3.2196 \n",
            "[18/25][77/391] Loss_D: 0.4268 Loss_G: 2.6824 \n",
            "[18/25][78/391] Loss_D: 0.4213 Loss_G: 3.2675 \n",
            "[18/25][79/391] Loss_D: 0.4869 Loss_G: 1.9581 \n",
            "[18/25][80/391] Loss_D: 0.4842 Loss_G: 2.6868 \n",
            "[18/25][81/391] Loss_D: 0.4066 Loss_G: 2.5831 \n",
            "[18/25][82/391] Loss_D: 0.4109 Loss_G: 2.7543 \n",
            "[18/25][83/391] Loss_D: 0.6328 Loss_G: 1.0684 \n",
            "[18/25][84/391] Loss_D: 1.2812 Loss_G: 4.7546 \n",
            "[18/25][85/391] Loss_D: 1.2163 Loss_G: 0.4729 \n",
            "[18/25][86/391] Loss_D: 1.2722 Loss_G: 5.5402 \n",
            "[18/25][87/391] Loss_D: 1.8534 Loss_G: 0.5036 \n",
            "[18/25][88/391] Loss_D: 1.3462 Loss_G: 4.4212 \n",
            "[18/25][89/391] Loss_D: 1.2094 Loss_G: 0.8719 \n",
            "[18/25][90/391] Loss_D: 1.2307 Loss_G: 4.3557 \n",
            "[18/25][91/391] Loss_D: 1.1375 Loss_G: 1.5008 \n",
            "[18/25][92/391] Loss_D: 0.6834 Loss_G: 2.6507 \n",
            "[18/25][93/391] Loss_D: 0.6482 Loss_G: 2.9038 \n",
            "[18/25][94/391] Loss_D: 0.4766 Loss_G: 2.7557 \n",
            "[18/25][95/391] Loss_D: 0.5209 Loss_G: 1.7575 \n",
            "[18/25][96/391] Loss_D: 0.6873 Loss_G: 4.0679 \n",
            "[18/25][97/391] Loss_D: 0.9582 Loss_G: 0.6324 \n",
            "[18/25][98/391] Loss_D: 1.1710 Loss_G: 4.5153 \n",
            "[18/25][99/391] Loss_D: 1.0040 Loss_G: 1.3574 \n",
            "[18/25][100/391] Loss_D: 0.5774 Loss_G: 2.6123 \n",
            "saving the output\n",
            "[18/25][101/391] Loss_D: 0.5531 Loss_G: 2.5023 \n",
            "[18/25][102/391] Loss_D: 0.3974 Loss_G: 3.5775 \n",
            "[18/25][103/391] Loss_D: 0.6275 Loss_G: 1.5889 \n",
            "[18/25][104/391] Loss_D: 0.4719 Loss_G: 3.3361 \n",
            "[18/25][105/391] Loss_D: 0.6213 Loss_G: 1.5031 \n",
            "[18/25][106/391] Loss_D: 0.5835 Loss_G: 3.6780 \n",
            "[18/25][107/391] Loss_D: 0.3375 Loss_G: 3.4990 \n",
            "[18/25][108/391] Loss_D: 0.5457 Loss_G: 1.4485 \n",
            "[18/25][109/391] Loss_D: 0.4762 Loss_G: 3.4800 \n",
            "[18/25][110/391] Loss_D: 0.3925 Loss_G: 2.5217 \n",
            "[18/25][111/391] Loss_D: 0.3800 Loss_G: 2.8646 \n",
            "[18/25][112/391] Loss_D: 0.3605 Loss_G: 2.9193 \n",
            "[18/25][113/391] Loss_D: 0.5520 Loss_G: 1.7210 \n",
            "[18/25][114/391] Loss_D: 0.5773 Loss_G: 4.1432 \n",
            "[18/25][115/391] Loss_D: 0.6670 Loss_G: 1.8023 \n",
            "[18/25][116/391] Loss_D: 0.7199 Loss_G: 3.5959 \n",
            "[18/25][117/391] Loss_D: 0.4811 Loss_G: 2.4418 \n",
            "[18/25][118/391] Loss_D: 0.3673 Loss_G: 1.9771 \n",
            "[18/25][119/391] Loss_D: 0.6092 Loss_G: 3.6977 \n",
            "[18/25][120/391] Loss_D: 0.4802 Loss_G: 2.5532 \n",
            "[18/25][121/391] Loss_D: 0.3750 Loss_G: 2.3764 \n",
            "[18/25][122/391] Loss_D: 0.4048 Loss_G: 2.5812 \n",
            "[18/25][123/391] Loss_D: 0.3742 Loss_G: 2.8482 \n",
            "[18/25][124/391] Loss_D: 0.3955 Loss_G: 2.8129 \n",
            "[18/25][125/391] Loss_D: 0.3684 Loss_G: 2.8129 \n",
            "[18/25][126/391] Loss_D: 0.5922 Loss_G: 1.7310 \n",
            "[18/25][127/391] Loss_D: 0.6998 Loss_G: 4.3768 \n",
            "[18/25][128/391] Loss_D: 0.5912 Loss_G: 2.0811 \n",
            "[18/25][129/391] Loss_D: 0.4280 Loss_G: 3.1235 \n",
            "[18/25][130/391] Loss_D: 0.4332 Loss_G: 2.3167 \n",
            "[18/25][131/391] Loss_D: 0.3319 Loss_G: 3.4133 \n",
            "[18/25][132/391] Loss_D: 0.3047 Loss_G: 3.0115 \n",
            "[18/25][133/391] Loss_D: 0.7184 Loss_G: 1.3687 \n",
            "[18/25][134/391] Loss_D: 0.4178 Loss_G: 3.4856 \n",
            "[18/25][135/391] Loss_D: 0.4458 Loss_G: 3.3141 \n",
            "[18/25][136/391] Loss_D: 0.6318 Loss_G: 1.6050 \n",
            "[18/25][137/391] Loss_D: 0.5566 Loss_G: 2.5464 \n",
            "[18/25][138/391] Loss_D: 0.4967 Loss_G: 4.0104 \n",
            "[18/25][139/391] Loss_D: 0.7349 Loss_G: 1.1777 \n",
            "[18/25][140/391] Loss_D: 0.6135 Loss_G: 3.7412 \n",
            "[18/25][141/391] Loss_D: 0.4640 Loss_G: 2.4815 \n",
            "[18/25][142/391] Loss_D: 0.4335 Loss_G: 2.7374 \n",
            "[18/25][143/391] Loss_D: 0.4865 Loss_G: 2.0868 \n",
            "[18/25][144/391] Loss_D: 0.5703 Loss_G: 3.0256 \n",
            "[18/25][145/391] Loss_D: 0.3793 Loss_G: 2.8970 \n",
            "[18/25][146/391] Loss_D: 0.5162 Loss_G: 2.8228 \n",
            "[18/25][147/391] Loss_D: 0.4593 Loss_G: 1.9522 \n",
            "[18/25][148/391] Loss_D: 0.5228 Loss_G: 1.9856 \n",
            "[18/25][149/391] Loss_D: 0.5698 Loss_G: 3.5961 \n",
            "[18/25][150/391] Loss_D: 0.4731 Loss_G: 2.3821 \n",
            "[18/25][151/391] Loss_D: 0.6380 Loss_G: 2.0373 \n",
            "[18/25][152/391] Loss_D: 0.6444 Loss_G: 4.7416 \n",
            "[18/25][153/391] Loss_D: 0.9587 Loss_G: 1.5294 \n",
            "[18/25][154/391] Loss_D: 0.6647 Loss_G: 3.7523 \n",
            "[18/25][155/391] Loss_D: 0.4666 Loss_G: 2.2337 \n",
            "[18/25][156/391] Loss_D: 0.4345 Loss_G: 1.6660 \n",
            "[18/25][157/391] Loss_D: 0.6468 Loss_G: 4.4561 \n",
            "[18/25][158/391] Loss_D: 0.9499 Loss_G: 0.5983 \n",
            "[18/25][159/391] Loss_D: 1.5011 Loss_G: 5.1741 \n",
            "[18/25][160/391] Loss_D: 1.1663 Loss_G: 0.5778 \n",
            "[18/25][161/391] Loss_D: 1.4751 Loss_G: 5.4840 \n",
            "[18/25][162/391] Loss_D: 1.2097 Loss_G: 1.0668 \n",
            "[18/25][163/391] Loss_D: 0.8159 Loss_G: 3.9166 \n",
            "[18/25][164/391] Loss_D: 0.8890 Loss_G: 1.3959 \n",
            "[18/25][165/391] Loss_D: 0.6939 Loss_G: 3.0003 \n",
            "[18/25][166/391] Loss_D: 0.4680 Loss_G: 3.0288 \n",
            "[18/25][167/391] Loss_D: 0.4289 Loss_G: 2.5127 \n",
            "[18/25][168/391] Loss_D: 0.5694 Loss_G: 2.8088 \n",
            "[18/25][169/391] Loss_D: 0.7012 Loss_G: 1.3667 \n",
            "[18/25][170/391] Loss_D: 0.9091 Loss_G: 5.2607 \n",
            "[18/25][171/391] Loss_D: 1.5227 Loss_G: 0.8752 \n",
            "[18/25][172/391] Loss_D: 1.4170 Loss_G: 4.7334 \n",
            "[18/25][173/391] Loss_D: 1.1650 Loss_G: 1.4433 \n",
            "[18/25][174/391] Loss_D: 0.4703 Loss_G: 2.8834 \n",
            "[18/25][175/391] Loss_D: 0.4646 Loss_G: 3.7381 \n",
            "[18/25][176/391] Loss_D: 0.4085 Loss_G: 2.5201 \n",
            "[18/25][177/391] Loss_D: 0.4365 Loss_G: 3.0244 \n",
            "[18/25][178/391] Loss_D: 0.5995 Loss_G: 2.0664 \n",
            "[18/25][179/391] Loss_D: 0.6310 Loss_G: 3.6324 \n",
            "[18/25][180/391] Loss_D: 0.6239 Loss_G: 1.6664 \n",
            "[18/25][181/391] Loss_D: 0.7138 Loss_G: 3.1077 \n",
            "[18/25][182/391] Loss_D: 0.4066 Loss_G: 3.0498 \n",
            "[18/25][183/391] Loss_D: 0.3800 Loss_G: 2.0818 \n",
            "[18/25][184/391] Loss_D: 0.7430 Loss_G: 4.2888 \n",
            "[18/25][185/391] Loss_D: 0.8529 Loss_G: 0.7754 \n",
            "[18/25][186/391] Loss_D: 1.2753 Loss_G: 4.5372 \n",
            "[18/25][187/391] Loss_D: 0.5998 Loss_G: 0.8469 \n",
            "[18/25][188/391] Loss_D: 1.1241 Loss_G: 6.7814 \n",
            "[18/25][189/391] Loss_D: 2.5896 Loss_G: 0.5819 \n",
            "[18/25][190/391] Loss_D: 1.8873 Loss_G: 5.5602 \n",
            "[18/25][191/391] Loss_D: 2.1236 Loss_G: 0.3107 \n",
            "[18/25][192/391] Loss_D: 2.3132 Loss_G: 5.4160 \n",
            "[18/25][193/391] Loss_D: 1.2471 Loss_G: 0.5818 \n",
            "[18/25][194/391] Loss_D: 1.8775 Loss_G: 6.2970 \n",
            "[18/25][195/391] Loss_D: 2.2007 Loss_G: 1.5320 \n",
            "[18/25][196/391] Loss_D: 1.2386 Loss_G: 3.0635 \n",
            "[18/25][197/391] Loss_D: 0.8139 Loss_G: 2.6140 \n",
            "[18/25][198/391] Loss_D: 0.8688 Loss_G: 0.9984 \n",
            "[18/25][199/391] Loss_D: 1.0443 Loss_G: 4.2549 \n",
            "[18/25][200/391] Loss_D: 0.9728 Loss_G: 1.6602 \n",
            "saving the output\n",
            "[18/25][201/391] Loss_D: 0.7811 Loss_G: 2.6162 \n",
            "[18/25][202/391] Loss_D: 0.6983 Loss_G: 2.1858 \n",
            "[18/25][203/391] Loss_D: 0.5871 Loss_G: 2.9372 \n",
            "[18/25][204/391] Loss_D: 0.6224 Loss_G: 2.3448 \n",
            "[18/25][205/391] Loss_D: 0.4605 Loss_G: 2.9764 \n",
            "[18/25][206/391] Loss_D: 0.7632 Loss_G: 1.3488 \n",
            "[18/25][207/391] Loss_D: 0.8626 Loss_G: 4.4390 \n",
            "[18/25][208/391] Loss_D: 1.1795 Loss_G: 1.1261 \n",
            "[18/25][209/391] Loss_D: 0.9778 Loss_G: 3.7255 \n",
            "[18/25][210/391] Loss_D: 0.4590 Loss_G: 2.9987 \n",
            "[18/25][211/391] Loss_D: 0.5046 Loss_G: 2.0001 \n",
            "[18/25][212/391] Loss_D: 0.5213 Loss_G: 2.7368 \n",
            "[18/25][213/391] Loss_D: 0.4613 Loss_G: 3.0265 \n",
            "[18/25][214/391] Loss_D: 0.5514 Loss_G: 1.9851 \n",
            "[18/25][215/391] Loss_D: 0.6219 Loss_G: 3.3807 \n",
            "[18/25][216/391] Loss_D: 0.2778 Loss_G: 3.6723 \n",
            "[18/25][217/391] Loss_D: 0.6172 Loss_G: 1.4398 \n",
            "[18/25][218/391] Loss_D: 0.6693 Loss_G: 3.7114 \n",
            "[18/25][219/391] Loss_D: 0.5259 Loss_G: 2.6462 \n",
            "[18/25][220/391] Loss_D: 0.5676 Loss_G: 2.2504 \n",
            "[18/25][221/391] Loss_D: 0.5487 Loss_G: 2.4777 \n",
            "[18/25][222/391] Loss_D: 0.5006 Loss_G: 2.9610 \n",
            "[18/25][223/391] Loss_D: 0.3679 Loss_G: 3.1178 \n",
            "[18/25][224/391] Loss_D: 0.5244 Loss_G: 1.8307 \n",
            "[18/25][225/391] Loss_D: 0.6471 Loss_G: 3.0669 \n",
            "[18/25][226/391] Loss_D: 0.4715 Loss_G: 2.8293 \n",
            "[18/25][227/391] Loss_D: 0.5394 Loss_G: 1.8328 \n",
            "[18/25][228/391] Loss_D: 0.6470 Loss_G: 2.9633 \n",
            "[18/25][229/391] Loss_D: 0.4516 Loss_G: 2.8970 \n",
            "[18/25][230/391] Loss_D: 0.3044 Loss_G: 2.9551 \n",
            "[18/25][231/391] Loss_D: 0.4822 Loss_G: 1.9497 \n",
            "[18/25][232/391] Loss_D: 0.4327 Loss_G: 2.4766 \n",
            "[18/25][233/391] Loss_D: 0.4019 Loss_G: 3.3749 \n",
            "[18/25][234/391] Loss_D: 0.3710 Loss_G: 2.7179 \n",
            "[18/25][235/391] Loss_D: 0.4615 Loss_G: 1.8548 \n",
            "[18/25][236/391] Loss_D: 0.3892 Loss_G: 3.0714 \n",
            "[18/25][237/391] Loss_D: 0.4295 Loss_G: 2.8339 \n",
            "[18/25][238/391] Loss_D: 0.6348 Loss_G: 1.4554 \n",
            "[18/25][239/391] Loss_D: 0.6434 Loss_G: 3.8750 \n",
            "[18/25][240/391] Loss_D: 0.4239 Loss_G: 2.7447 \n",
            "[18/25][241/391] Loss_D: 0.4811 Loss_G: 1.9643 \n",
            "[18/25][242/391] Loss_D: 0.5645 Loss_G: 3.2726 \n",
            "[18/25][243/391] Loss_D: 0.6809 Loss_G: 1.4143 \n",
            "[18/25][244/391] Loss_D: 0.7816 Loss_G: 3.7726 \n",
            "[18/25][245/391] Loss_D: 0.5122 Loss_G: 2.4819 \n",
            "[18/25][246/391] Loss_D: 0.4575 Loss_G: 2.2845 \n",
            "[18/25][247/391] Loss_D: 0.5937 Loss_G: 2.4730 \n",
            "[18/25][248/391] Loss_D: 0.4836 Loss_G: 3.0693 \n",
            "[18/25][249/391] Loss_D: 0.3975 Loss_G: 2.7960 \n",
            "[18/25][250/391] Loss_D: 0.4425 Loss_G: 2.4586 \n",
            "[18/25][251/391] Loss_D: 0.3285 Loss_G: 2.8266 \n",
            "[18/25][252/391] Loss_D: 0.3791 Loss_G: 2.4037 \n",
            "[18/25][253/391] Loss_D: 0.4211 Loss_G: 2.7786 \n",
            "[18/25][254/391] Loss_D: 0.5844 Loss_G: 2.1519 \n",
            "[18/25][255/391] Loss_D: 0.6248 Loss_G: 2.4088 \n",
            "[18/25][256/391] Loss_D: 0.3530 Loss_G: 2.9079 \n",
            "[18/25][257/391] Loss_D: 0.3747 Loss_G: 3.1151 \n",
            "[18/25][258/391] Loss_D: 0.3249 Loss_G: 2.6846 \n",
            "[18/25][259/391] Loss_D: 0.5040 Loss_G: 2.4403 \n",
            "[18/25][260/391] Loss_D: 0.5397 Loss_G: 1.9501 \n",
            "[18/25][261/391] Loss_D: 0.5100 Loss_G: 2.9294 \n",
            "[18/25][262/391] Loss_D: 0.4063 Loss_G: 2.6090 \n",
            "[18/25][263/391] Loss_D: 0.4119 Loss_G: 2.2895 \n",
            "[18/25][264/391] Loss_D: 0.4311 Loss_G: 2.7099 \n",
            "[18/25][265/391] Loss_D: 0.5421 Loss_G: 2.6404 \n",
            "[18/25][266/391] Loss_D: 0.4831 Loss_G: 2.0237 \n",
            "[18/25][267/391] Loss_D: 0.4592 Loss_G: 3.1334 \n",
            "[18/25][268/391] Loss_D: 0.5292 Loss_G: 2.0423 \n",
            "[18/25][269/391] Loss_D: 0.4526 Loss_G: 2.9614 \n",
            "[18/25][270/391] Loss_D: 0.4887 Loss_G: 2.3926 \n",
            "[18/25][271/391] Loss_D: 0.3865 Loss_G: 2.5986 \n",
            "[18/25][272/391] Loss_D: 0.3566 Loss_G: 2.8350 \n",
            "[18/25][273/391] Loss_D: 0.3208 Loss_G: 3.1068 \n",
            "[18/25][274/391] Loss_D: 0.3620 Loss_G: 2.6773 \n",
            "[18/25][275/391] Loss_D: 0.3932 Loss_G: 2.2113 \n",
            "[18/25][276/391] Loss_D: 0.4323 Loss_G: 2.4268 \n",
            "[18/25][277/391] Loss_D: 0.4166 Loss_G: 3.3184 \n",
            "[18/25][278/391] Loss_D: 0.5379 Loss_G: 1.7836 \n",
            "[18/25][279/391] Loss_D: 0.4697 Loss_G: 3.0126 \n",
            "[18/25][280/391] Loss_D: 0.6296 Loss_G: 1.5475 \n",
            "[18/25][281/391] Loss_D: 0.5710 Loss_G: 3.5233 \n",
            "[18/25][282/391] Loss_D: 0.3553 Loss_G: 2.9061 \n",
            "[18/25][283/391] Loss_D: 0.5329 Loss_G: 2.4530 \n",
            "[18/25][284/391] Loss_D: 0.3638 Loss_G: 2.6898 \n",
            "[18/25][285/391] Loss_D: 0.4793 Loss_G: 1.9281 \n",
            "[18/25][286/391] Loss_D: 0.4672 Loss_G: 3.2459 \n",
            "[18/25][287/391] Loss_D: 0.3357 Loss_G: 2.9086 \n",
            "[18/25][288/391] Loss_D: 0.5356 Loss_G: 2.2335 \n",
            "[18/25][289/391] Loss_D: 0.3851 Loss_G: 2.6734 \n",
            "[18/25][290/391] Loss_D: 0.3533 Loss_G: 2.6171 \n",
            "[18/25][291/391] Loss_D: 0.3373 Loss_G: 3.0715 \n",
            "[18/25][292/391] Loss_D: 0.1865 Loss_G: 3.5974 \n",
            "[18/25][293/391] Loss_D: 0.7568 Loss_G: 0.7505 \n",
            "[18/25][294/391] Loss_D: 0.6980 Loss_G: 3.6269 \n",
            "[18/25][295/391] Loss_D: 0.3274 Loss_G: 3.3701 \n",
            "[18/25][296/391] Loss_D: 0.3860 Loss_G: 1.9914 \n",
            "[18/25][297/391] Loss_D: 0.4659 Loss_G: 3.5535 \n",
            "[18/25][298/391] Loss_D: 0.7801 Loss_G: 1.1471 \n",
            "[18/25][299/391] Loss_D: 0.8113 Loss_G: 4.2487 \n",
            "[18/25][300/391] Loss_D: 0.5988 Loss_G: 2.1032 \n",
            "saving the output\n",
            "[18/25][301/391] Loss_D: 0.5292 Loss_G: 2.8989 \n",
            "[18/25][302/391] Loss_D: 0.4304 Loss_G: 2.4058 \n",
            "[18/25][303/391] Loss_D: 0.5337 Loss_G: 2.3644 \n",
            "[18/25][304/391] Loss_D: 0.4660 Loss_G: 3.0395 \n",
            "[18/25][305/391] Loss_D: 0.5041 Loss_G: 1.9406 \n",
            "[18/25][306/391] Loss_D: 0.5461 Loss_G: 3.8407 \n",
            "[18/25][307/391] Loss_D: 0.2930 Loss_G: 3.1858 \n",
            "[18/25][308/391] Loss_D: 0.5623 Loss_G: 0.7689 \n",
            "[18/25][309/391] Loss_D: 1.3716 Loss_G: 4.8839 \n",
            "[18/25][310/391] Loss_D: 0.8375 Loss_G: 0.4394 \n",
            "[18/25][311/391] Loss_D: 1.3793 Loss_G: 6.0277 \n",
            "[18/25][312/391] Loss_D: 2.4664 Loss_G: 0.6584 \n",
            "[18/25][313/391] Loss_D: 1.4864 Loss_G: 4.7810 \n",
            "[18/25][314/391] Loss_D: 0.9717 Loss_G: 1.6030 \n",
            "[18/25][315/391] Loss_D: 0.6413 Loss_G: 2.4037 \n",
            "[18/25][316/391] Loss_D: 0.5943 Loss_G: 4.3880 \n",
            "[18/25][317/391] Loss_D: 1.2522 Loss_G: 0.5793 \n",
            "[18/25][318/391] Loss_D: 1.3469 Loss_G: 4.8684 \n",
            "[18/25][319/391] Loss_D: 1.1330 Loss_G: 1.3489 \n",
            "[18/25][320/391] Loss_D: 0.5534 Loss_G: 2.9185 \n",
            "[18/25][321/391] Loss_D: 0.5490 Loss_G: 3.6959 \n",
            "[18/25][322/391] Loss_D: 0.5476 Loss_G: 1.9953 \n",
            "[18/25][323/391] Loss_D: 0.5945 Loss_G: 2.6601 \n",
            "[18/25][324/391] Loss_D: 0.6157 Loss_G: 3.4195 \n",
            "[18/25][325/391] Loss_D: 0.5506 Loss_G: 1.9758 \n",
            "[18/25][326/391] Loss_D: 0.6568 Loss_G: 2.3815 \n",
            "[18/25][327/391] Loss_D: 0.3950 Loss_G: 2.6023 \n",
            "[18/25][328/391] Loss_D: 0.7268 Loss_G: 2.3942 \n",
            "[18/25][329/391] Loss_D: 0.4447 Loss_G: 2.6989 \n",
            "[18/25][330/391] Loss_D: 0.4777 Loss_G: 2.8819 \n",
            "[18/25][331/391] Loss_D: 0.5089 Loss_G: 1.7023 \n",
            "[18/25][332/391] Loss_D: 0.5956 Loss_G: 4.1348 \n",
            "[18/25][333/391] Loss_D: 0.4876 Loss_G: 2.7062 \n",
            "[18/25][334/391] Loss_D: 0.3279 Loss_G: 1.9008 \n",
            "[18/25][335/391] Loss_D: 0.5254 Loss_G: 3.3685 \n",
            "[18/25][336/391] Loss_D: 0.3814 Loss_G: 2.7449 \n",
            "[18/25][337/391] Loss_D: 0.4927 Loss_G: 1.8869 \n",
            "[18/25][338/391] Loss_D: 0.5585 Loss_G: 3.3194 \n",
            "[18/25][339/391] Loss_D: 0.4573 Loss_G: 2.4153 \n",
            "[18/25][340/391] Loss_D: 0.3903 Loss_G: 2.8127 \n",
            "[18/25][341/391] Loss_D: 0.3981 Loss_G: 2.3348 \n",
            "[18/25][342/391] Loss_D: 0.6208 Loss_G: 2.3274 \n",
            "[18/25][343/391] Loss_D: 0.5352 Loss_G: 2.1057 \n",
            "[18/25][344/391] Loss_D: 0.5654 Loss_G: 3.0361 \n",
            "[18/25][345/391] Loss_D: 0.4145 Loss_G: 2.5520 \n",
            "[18/25][346/391] Loss_D: 0.6209 Loss_G: 1.0657 \n",
            "[18/25][347/391] Loss_D: 0.7073 Loss_G: 4.8234 \n",
            "[18/25][348/391] Loss_D: 0.8327 Loss_G: 1.5751 \n",
            "[18/25][349/391] Loss_D: 0.6373 Loss_G: 3.7906 \n",
            "[18/25][350/391] Loss_D: 0.4986 Loss_G: 2.2121 \n",
            "[18/25][351/391] Loss_D: 0.5269 Loss_G: 2.5314 \n",
            "[18/25][352/391] Loss_D: 0.4510 Loss_G: 2.8745 \n",
            "[18/25][353/391] Loss_D: 0.4847 Loss_G: 2.0581 \n",
            "[18/25][354/391] Loss_D: 0.5505 Loss_G: 2.5549 \n",
            "[18/25][355/391] Loss_D: 0.3580 Loss_G: 3.0014 \n",
            "[18/25][356/391] Loss_D: 0.3447 Loss_G: 2.4793 \n",
            "[18/25][357/391] Loss_D: 0.4549 Loss_G: 3.0126 \n",
            "[18/25][358/391] Loss_D: 0.3568 Loss_G: 3.5521 \n",
            "[18/25][359/391] Loss_D: 0.7176 Loss_G: 1.3477 \n",
            "[18/25][360/391] Loss_D: 0.8786 Loss_G: 4.1059 \n",
            "[18/25][361/391] Loss_D: 0.7299 Loss_G: 1.4024 \n",
            "[18/25][362/391] Loss_D: 0.5334 Loss_G: 2.9802 \n",
            "[18/25][363/391] Loss_D: 0.4226 Loss_G: 3.1315 \n",
            "[18/25][364/391] Loss_D: 0.6265 Loss_G: 1.2806 \n",
            "[18/25][365/391] Loss_D: 0.7332 Loss_G: 3.6182 \n",
            "[18/25][366/391] Loss_D: 0.4940 Loss_G: 2.3533 \n",
            "[18/25][367/391] Loss_D: 0.3566 Loss_G: 2.7308 \n",
            "[18/25][368/391] Loss_D: 0.4436 Loss_G: 2.7929 \n",
            "[18/25][369/391] Loss_D: 0.3058 Loss_G: 2.9983 \n",
            "[18/25][370/391] Loss_D: 0.5373 Loss_G: 1.9797 \n",
            "[18/25][371/391] Loss_D: 0.4885 Loss_G: 2.3196 \n",
            "[18/25][372/391] Loss_D: 0.4723 Loss_G: 2.9751 \n",
            "[18/25][373/391] Loss_D: 0.4716 Loss_G: 2.0841 \n",
            "[18/25][374/391] Loss_D: 0.5406 Loss_G: 2.8459 \n",
            "[18/25][375/391] Loss_D: 0.4688 Loss_G: 2.3753 \n",
            "[18/25][376/391] Loss_D: 0.6497 Loss_G: 1.8588 \n",
            "[18/25][377/391] Loss_D: 0.5956 Loss_G: 4.0457 \n",
            "[18/25][378/391] Loss_D: 0.3608 Loss_G: 2.7631 \n",
            "[18/25][379/391] Loss_D: 0.4452 Loss_G: 2.1738 \n",
            "[18/25][380/391] Loss_D: 0.4505 Loss_G: 2.3119 \n",
            "[18/25][381/391] Loss_D: 0.4332 Loss_G: 3.3256 \n",
            "[18/25][382/391] Loss_D: 0.3752 Loss_G: 2.5969 \n",
            "[18/25][383/391] Loss_D: 0.3629 Loss_G: 2.6342 \n",
            "[18/25][384/391] Loss_D: 0.5108 Loss_G: 2.2195 \n",
            "[18/25][385/391] Loss_D: 0.6992 Loss_G: 2.9350 \n",
            "[18/25][386/391] Loss_D: 0.7328 Loss_G: 1.2877 \n",
            "[18/25][387/391] Loss_D: 0.5777 Loss_G: 3.7251 \n",
            "[18/25][388/391] Loss_D: 0.3984 Loss_G: 2.8686 \n",
            "[18/25][389/391] Loss_D: 0.4465 Loss_G: 1.8567 \n",
            "[18/25][390/391] Loss_D: 0.5145 Loss_G: 3.3975 \n",
            "[19/25][0/391] Loss_D: 0.3104 Loss_G: 3.0298 \n",
            "saving the output\n",
            "[19/25][1/391] Loss_D: 0.4032 Loss_G: 2.0970 \n",
            "[19/25][2/391] Loss_D: 0.3439 Loss_G: 3.1289 \n",
            "[19/25][3/391] Loss_D: 0.4577 Loss_G: 3.0571 \n",
            "[19/25][4/391] Loss_D: 0.4172 Loss_G: 1.9356 \n",
            "[19/25][5/391] Loss_D: 0.5139 Loss_G: 3.4597 \n",
            "[19/25][6/391] Loss_D: 0.5910 Loss_G: 2.1380 \n",
            "[19/25][7/391] Loss_D: 0.4289 Loss_G: 2.8903 \n",
            "[19/25][8/391] Loss_D: 0.3953 Loss_G: 2.4514 \n",
            "[19/25][9/391] Loss_D: 0.4427 Loss_G: 2.9255 \n",
            "[19/25][10/391] Loss_D: 0.5949 Loss_G: 1.6055 \n",
            "[19/25][11/391] Loss_D: 0.3929 Loss_G: 3.1889 \n",
            "[19/25][12/391] Loss_D: 0.4545 Loss_G: 2.4748 \n",
            "[19/25][13/391] Loss_D: 0.3215 Loss_G: 2.6787 \n",
            "[19/25][14/391] Loss_D: 0.3295 Loss_G: 2.8553 \n",
            "[19/25][15/391] Loss_D: 0.6061 Loss_G: 1.7271 \n",
            "[19/25][16/391] Loss_D: 0.5476 Loss_G: 2.9114 \n",
            "[19/25][17/391] Loss_D: 0.2211 Loss_G: 3.4684 \n",
            "[19/25][18/391] Loss_D: 0.4147 Loss_G: 1.9050 \n",
            "[19/25][19/391] Loss_D: 0.4483 Loss_G: 3.2314 \n",
            "[19/25][20/391] Loss_D: 0.3777 Loss_G: 2.3743 \n",
            "[19/25][21/391] Loss_D: 0.2985 Loss_G: 2.6206 \n",
            "[19/25][22/391] Loss_D: 0.2968 Loss_G: 2.9866 \n",
            "[19/25][23/391] Loss_D: 0.2484 Loss_G: 3.1277 \n",
            "[19/25][24/391] Loss_D: 0.3181 Loss_G: 2.6409 \n",
            "[19/25][25/391] Loss_D: 0.3334 Loss_G: 2.7327 \n",
            "[19/25][26/391] Loss_D: 0.3757 Loss_G: 2.5529 \n",
            "[19/25][27/391] Loss_D: 0.3890 Loss_G: 2.8545 \n",
            "[19/25][28/391] Loss_D: 0.3708 Loss_G: 2.9123 \n",
            "[19/25][29/391] Loss_D: 0.5432 Loss_G: 1.5847 \n",
            "[19/25][30/391] Loss_D: 0.4759 Loss_G: 3.5632 \n",
            "[19/25][31/391] Loss_D: 0.3824 Loss_G: 2.8799 \n",
            "[19/25][32/391] Loss_D: 0.3857 Loss_G: 2.0983 \n",
            "[19/25][33/391] Loss_D: 0.4151 Loss_G: 2.9280 \n",
            "[19/25][34/391] Loss_D: 0.5017 Loss_G: 2.2832 \n",
            "[19/25][35/391] Loss_D: 0.4826 Loss_G: 2.7055 \n",
            "[19/25][36/391] Loss_D: 0.3446 Loss_G: 3.4350 \n",
            "[19/25][37/391] Loss_D: 0.3620 Loss_G: 2.4272 \n",
            "[19/25][38/391] Loss_D: 0.4366 Loss_G: 1.8893 \n",
            "[19/25][39/391] Loss_D: 0.5329 Loss_G: 3.9938 \n",
            "[19/25][40/391] Loss_D: 0.6041 Loss_G: 1.6436 \n",
            "[19/25][41/391] Loss_D: 0.5115 Loss_G: 3.2057 \n",
            "[19/25][42/391] Loss_D: 0.3974 Loss_G: 2.6433 \n",
            "[19/25][43/391] Loss_D: 0.3738 Loss_G: 2.6003 \n",
            "[19/25][44/391] Loss_D: 0.3709 Loss_G: 3.0461 \n",
            "[19/25][45/391] Loss_D: 0.4058 Loss_G: 2.1875 \n",
            "[19/25][46/391] Loss_D: 0.3676 Loss_G: 2.5842 \n",
            "[19/25][47/391] Loss_D: 0.3199 Loss_G: 3.3757 \n",
            "[19/25][48/391] Loss_D: 0.3811 Loss_G: 2.2122 \n",
            "[19/25][49/391] Loss_D: 0.4452 Loss_G: 2.9950 \n",
            "[19/25][50/391] Loss_D: 0.4253 Loss_G: 2.1198 \n",
            "[19/25][51/391] Loss_D: 0.4928 Loss_G: 3.3838 \n",
            "[19/25][52/391] Loss_D: 0.6624 Loss_G: 1.2376 \n",
            "[19/25][53/391] Loss_D: 0.6396 Loss_G: 4.2078 \n",
            "[19/25][54/391] Loss_D: 0.3676 Loss_G: 2.4836 \n",
            "[19/25][55/391] Loss_D: 0.5032 Loss_G: 3.1035 \n",
            "[19/25][56/391] Loss_D: 0.5626 Loss_G: 1.4662 \n",
            "[19/25][57/391] Loss_D: 0.6528 Loss_G: 4.7080 \n",
            "[19/25][58/391] Loss_D: 0.8353 Loss_G: 1.3021 \n",
            "[19/25][59/391] Loss_D: 0.6722 Loss_G: 3.2816 \n",
            "[19/25][60/391] Loss_D: 0.4080 Loss_G: 2.6136 \n",
            "[19/25][61/391] Loss_D: 0.3485 Loss_G: 2.0683 \n",
            "[19/25][62/391] Loss_D: 0.4956 Loss_G: 3.9835 \n",
            "[19/25][63/391] Loss_D: 0.5163 Loss_G: 1.8660 \n",
            "[19/25][64/391] Loss_D: 0.4939 Loss_G: 2.8490 \n",
            "[19/25][65/391] Loss_D: 0.3688 Loss_G: 2.6169 \n",
            "[19/25][66/391] Loss_D: 0.5262 Loss_G: 1.9324 \n",
            "[19/25][67/391] Loss_D: 0.5377 Loss_G: 3.5815 \n",
            "[19/25][68/391] Loss_D: 0.4430 Loss_G: 1.9228 \n",
            "[19/25][69/391] Loss_D: 0.5104 Loss_G: 2.8823 \n",
            "[19/25][70/391] Loss_D: 0.4386 Loss_G: 2.9377 \n",
            "[19/25][71/391] Loss_D: 0.4812 Loss_G: 1.8716 \n",
            "[19/25][72/391] Loss_D: 0.3989 Loss_G: 3.0089 \n",
            "[19/25][73/391] Loss_D: 0.4833 Loss_G: 2.7981 \n",
            "[19/25][74/391] Loss_D: 0.3156 Loss_G: 3.0024 \n",
            "[19/25][75/391] Loss_D: 0.2871 Loss_G: 3.1642 \n",
            "[19/25][76/391] Loss_D: 0.4988 Loss_G: 1.7888 \n",
            "[19/25][77/391] Loss_D: 0.4199 Loss_G: 3.1453 \n",
            "[19/25][78/391] Loss_D: 0.3204 Loss_G: 3.1813 \n",
            "[19/25][79/391] Loss_D: 0.4293 Loss_G: 2.1941 \n",
            "[19/25][80/391] Loss_D: 0.3946 Loss_G: 3.1889 \n",
            "[19/25][81/391] Loss_D: 0.4512 Loss_G: 2.3770 \n",
            "[19/25][82/391] Loss_D: 0.4243 Loss_G: 2.1273 \n",
            "[19/25][83/391] Loss_D: 0.4868 Loss_G: 3.7688 \n",
            "[19/25][84/391] Loss_D: 0.5250 Loss_G: 2.1937 \n",
            "[19/25][85/391] Loss_D: 0.4422 Loss_G: 2.0824 \n",
            "[19/25][86/391] Loss_D: 0.5037 Loss_G: 4.1503 \n",
            "[19/25][87/391] Loss_D: 0.4236 Loss_G: 2.3513 \n",
            "[19/25][88/391] Loss_D: 0.5271 Loss_G: 2.7524 \n",
            "[19/25][89/391] Loss_D: 0.4170 Loss_G: 2.3046 \n",
            "[19/25][90/391] Loss_D: 0.4052 Loss_G: 3.5093 \n",
            "[19/25][91/391] Loss_D: 0.4716 Loss_G: 2.0069 \n",
            "[19/25][92/391] Loss_D: 0.3822 Loss_G: 2.9180 \n",
            "[19/25][93/391] Loss_D: 0.4706 Loss_G: 3.5426 \n",
            "[19/25][94/391] Loss_D: 0.4350 Loss_G: 2.1086 \n",
            "[19/25][95/391] Loss_D: 0.4528 Loss_G: 3.6644 \n",
            "[19/25][96/391] Loss_D: 0.4016 Loss_G: 2.5093 \n",
            "[19/25][97/391] Loss_D: 0.4235 Loss_G: 2.4143 \n",
            "[19/25][98/391] Loss_D: 0.2760 Loss_G: 3.2736 \n",
            "[19/25][99/391] Loss_D: 0.5750 Loss_G: 2.1442 \n",
            "[19/25][100/391] Loss_D: 0.4364 Loss_G: 3.5709 \n",
            "saving the output\n",
            "[19/25][101/391] Loss_D: 0.5374 Loss_G: 1.3541 \n",
            "[19/25][102/391] Loss_D: 0.4566 Loss_G: 3.7411 \n",
            "[19/25][103/391] Loss_D: 0.4593 Loss_G: 2.6139 \n",
            "[19/25][104/391] Loss_D: 0.3592 Loss_G: 2.5388 \n",
            "[19/25][105/391] Loss_D: 0.2813 Loss_G: 3.3725 \n",
            "[19/25][106/391] Loss_D: 0.6468 Loss_G: 1.0106 \n",
            "[19/25][107/391] Loss_D: 0.9965 Loss_G: 4.9455 \n",
            "[19/25][108/391] Loss_D: 0.7475 Loss_G: 1.4438 \n",
            "[19/25][109/391] Loss_D: 0.5933 Loss_G: 4.4408 \n",
            "[19/25][110/391] Loss_D: 0.6197 Loss_G: 1.7626 \n",
            "[19/25][111/391] Loss_D: 0.4462 Loss_G: 2.3829 \n",
            "[19/25][112/391] Loss_D: 0.4736 Loss_G: 3.7324 \n",
            "[19/25][113/391] Loss_D: 0.6564 Loss_G: 1.1692 \n",
            "[19/25][114/391] Loss_D: 0.8418 Loss_G: 4.4840 \n",
            "[19/25][115/391] Loss_D: 0.7988 Loss_G: 0.8944 \n",
            "[19/25][116/391] Loss_D: 1.0175 Loss_G: 5.3937 \n",
            "[19/25][117/391] Loss_D: 1.0082 Loss_G: 0.7890 \n",
            "[19/25][118/391] Loss_D: 0.9304 Loss_G: 5.5846 \n",
            "[19/25][119/391] Loss_D: 1.4277 Loss_G: 0.5341 \n",
            "[19/25][120/391] Loss_D: 1.7346 Loss_G: 5.9348 \n",
            "[19/25][121/391] Loss_D: 1.4021 Loss_G: 0.0674 \n",
            "[19/25][122/391] Loss_D: 3.8603 Loss_G: 7.3121 \n",
            "[19/25][123/391] Loss_D: 2.0478 Loss_G: 0.1902 \n",
            "[19/25][124/391] Loss_D: 3.2382 Loss_G: 9.7510 \n",
            "[19/25][125/391] Loss_D: 6.3936 Loss_G: 0.3443 \n",
            "[19/25][126/391] Loss_D: 2.4317 Loss_G: 6.3853 \n",
            "[19/25][127/391] Loss_D: 2.1955 Loss_G: 0.8301 \n",
            "[19/25][128/391] Loss_D: 1.5157 Loss_G: 4.9181 \n",
            "[19/25][129/391] Loss_D: 1.3551 Loss_G: 0.7733 \n",
            "[19/25][130/391] Loss_D: 1.7239 Loss_G: 5.3313 \n",
            "[19/25][131/391] Loss_D: 1.5339 Loss_G: 0.8289 \n",
            "[19/25][132/391] Loss_D: 1.1323 Loss_G: 3.2010 \n",
            "[19/25][133/391] Loss_D: 0.7782 Loss_G: 2.9254 \n",
            "[19/25][134/391] Loss_D: 1.1614 Loss_G: 0.8544 \n",
            "[19/25][135/391] Loss_D: 1.4510 Loss_G: 4.9572 \n",
            "[19/25][136/391] Loss_D: 1.3335 Loss_G: 0.8318 \n",
            "[19/25][137/391] Loss_D: 1.3395 Loss_G: 4.3821 \n",
            "[19/25][138/391] Loss_D: 0.9259 Loss_G: 1.8288 \n",
            "[19/25][139/391] Loss_D: 0.6747 Loss_G: 2.1739 \n",
            "[19/25][140/391] Loss_D: 0.6860 Loss_G: 2.3981 \n",
            "[19/25][141/391] Loss_D: 0.6256 Loss_G: 2.2620 \n",
            "[19/25][142/391] Loss_D: 0.6771 Loss_G: 2.9689 \n",
            "[19/25][143/391] Loss_D: 0.7212 Loss_G: 2.2845 \n",
            "[19/25][144/391] Loss_D: 0.6681 Loss_G: 1.8590 \n",
            "[19/25][145/391] Loss_D: 0.9203 Loss_G: 4.5030 \n",
            "[19/25][146/391] Loss_D: 1.3024 Loss_G: 0.5338 \n",
            "[19/25][147/391] Loss_D: 1.1901 Loss_G: 4.2520 \n",
            "[19/25][148/391] Loss_D: 0.6567 Loss_G: 2.4020 \n",
            "[19/25][149/391] Loss_D: 0.6015 Loss_G: 1.7939 \n",
            "[19/25][150/391] Loss_D: 0.6305 Loss_G: 3.5178 \n",
            "[19/25][151/391] Loss_D: 0.6007 Loss_G: 2.1567 \n",
            "[19/25][152/391] Loss_D: 0.5505 Loss_G: 1.9486 \n",
            "[19/25][153/391] Loss_D: 0.7685 Loss_G: 3.0764 \n",
            "[19/25][154/391] Loss_D: 0.7969 Loss_G: 1.5366 \n",
            "[19/25][155/391] Loss_D: 0.7049 Loss_G: 3.3248 \n",
            "[19/25][156/391] Loss_D: 0.2997 Loss_G: 3.5349 \n",
            "[19/25][157/391] Loss_D: 0.5630 Loss_G: 1.7481 \n",
            "[19/25][158/391] Loss_D: 0.6748 Loss_G: 3.4652 \n",
            "[19/25][159/391] Loss_D: 0.5827 Loss_G: 2.2104 \n",
            "[19/25][160/391] Loss_D: 0.5792 Loss_G: 2.4241 \n",
            "[19/25][161/391] Loss_D: 0.4996 Loss_G: 3.1721 \n",
            "[19/25][162/391] Loss_D: 0.6415 Loss_G: 2.1130 \n",
            "[19/25][163/391] Loss_D: 0.5224 Loss_G: 2.4340 \n",
            "[19/25][164/391] Loss_D: 0.3476 Loss_G: 3.7728 \n",
            "[19/25][165/391] Loss_D: 0.5290 Loss_G: 2.3497 \n",
            "[19/25][166/391] Loss_D: 0.3482 Loss_G: 2.1324 \n",
            "[19/25][167/391] Loss_D: 0.4052 Loss_G: 2.9054 \n",
            "[19/25][168/391] Loss_D: 0.4120 Loss_G: 2.6763 \n",
            "[19/25][169/391] Loss_D: 0.4789 Loss_G: 1.9906 \n",
            "[19/25][170/391] Loss_D: 0.6622 Loss_G: 3.6841 \n",
            "[19/25][171/391] Loss_D: 0.8649 Loss_G: 1.5527 \n",
            "[19/25][172/391] Loss_D: 0.4942 Loss_G: 3.1905 \n",
            "[19/25][173/391] Loss_D: 0.3256 Loss_G: 3.3596 \n",
            "[19/25][174/391] Loss_D: 0.7309 Loss_G: 1.4410 \n",
            "[19/25][175/391] Loss_D: 0.5502 Loss_G: 3.3555 \n",
            "[19/25][176/391] Loss_D: 0.5336 Loss_G: 1.8945 \n",
            "[19/25][177/391] Loss_D: 0.4227 Loss_G: 2.7548 \n",
            "[19/25][178/391] Loss_D: 0.5333 Loss_G: 2.6488 \n",
            "[19/25][179/391] Loss_D: 0.4375 Loss_G: 2.1566 \n",
            "[19/25][180/391] Loss_D: 0.4618 Loss_G: 2.6071 \n",
            "[19/25][181/391] Loss_D: 0.4422 Loss_G: 3.3471 \n",
            "[19/25][182/391] Loss_D: 0.5146 Loss_G: 1.9187 \n",
            "[19/25][183/391] Loss_D: 0.6608 Loss_G: 3.3141 \n",
            "[19/25][184/391] Loss_D: 0.5767 Loss_G: 1.9300 \n",
            "[19/25][185/391] Loss_D: 0.5981 Loss_G: 2.5050 \n",
            "[19/25][186/391] Loss_D: 0.3268 Loss_G: 3.3474 \n",
            "[19/25][187/391] Loss_D: 0.6132 Loss_G: 1.5855 \n",
            "[19/25][188/391] Loss_D: 0.6871 Loss_G: 3.9050 \n",
            "[19/25][189/391] Loss_D: 0.4993 Loss_G: 2.5643 \n",
            "[19/25][190/391] Loss_D: 0.3957 Loss_G: 2.3844 \n",
            "[19/25][191/391] Loss_D: 0.3804 Loss_G: 2.9009 \n",
            "[19/25][192/391] Loss_D: 0.3984 Loss_G: 2.1991 \n",
            "[19/25][193/391] Loss_D: 0.3890 Loss_G: 3.1254 \n",
            "[19/25][194/391] Loss_D: 0.4931 Loss_G: 3.0034 \n",
            "[19/25][195/391] Loss_D: 0.6612 Loss_G: 1.3742 \n",
            "[19/25][196/391] Loss_D: 0.5780 Loss_G: 3.3746 \n",
            "[19/25][197/391] Loss_D: 0.4308 Loss_G: 2.6875 \n",
            "[19/25][198/391] Loss_D: 0.3789 Loss_G: 2.3668 \n",
            "[19/25][199/391] Loss_D: 0.5771 Loss_G: 3.3100 \n",
            "[19/25][200/391] Loss_D: 0.6456 Loss_G: 1.3751 \n",
            "saving the output\n",
            "[19/25][201/391] Loss_D: 0.5831 Loss_G: 3.3733 \n",
            "[19/25][202/391] Loss_D: 0.3769 Loss_G: 2.6527 \n",
            "[19/25][203/391] Loss_D: 0.2953 Loss_G: 2.8078 \n",
            "[19/25][204/391] Loss_D: 0.4096 Loss_G: 2.3010 \n",
            "[19/25][205/391] Loss_D: 0.4054 Loss_G: 2.7430 \n",
            "[19/25][206/391] Loss_D: 0.4058 Loss_G: 2.1192 \n",
            "[19/25][207/391] Loss_D: 0.5498 Loss_G: 2.9358 \n",
            "[19/25][208/391] Loss_D: 0.5882 Loss_G: 2.1099 \n",
            "[19/25][209/391] Loss_D: 0.3884 Loss_G: 2.3911 \n",
            "[19/25][210/391] Loss_D: 0.5578 Loss_G: 3.7236 \n",
            "[19/25][211/391] Loss_D: 0.5615 Loss_G: 2.0875 \n",
            "[19/25][212/391] Loss_D: 0.4119 Loss_G: 2.2353 \n",
            "[19/25][213/391] Loss_D: 0.4914 Loss_G: 3.3502 \n",
            "[19/25][214/391] Loss_D: 0.3645 Loss_G: 2.6803 \n",
            "[19/25][215/391] Loss_D: 0.3041 Loss_G: 2.7785 \n",
            "[19/25][216/391] Loss_D: 0.5504 Loss_G: 1.6148 \n",
            "[19/25][217/391] Loss_D: 0.6790 Loss_G: 3.6931 \n",
            "[19/25][218/391] Loss_D: 0.4803 Loss_G: 2.1156 \n",
            "[19/25][219/391] Loss_D: 0.4161 Loss_G: 2.6515 \n",
            "[19/25][220/391] Loss_D: 0.4770 Loss_G: 2.3319 \n",
            "[19/25][221/391] Loss_D: 0.3520 Loss_G: 2.9924 \n",
            "[19/25][222/391] Loss_D: 0.7184 Loss_G: 1.5412 \n",
            "[19/25][223/391] Loss_D: 0.4326 Loss_G: 2.8454 \n",
            "[19/25][224/391] Loss_D: 0.4098 Loss_G: 2.5582 \n",
            "[19/25][225/391] Loss_D: 0.4396 Loss_G: 3.3675 \n",
            "[19/25][226/391] Loss_D: 0.4690 Loss_G: 2.1340 \n",
            "[19/25][227/391] Loss_D: 0.4542 Loss_G: 2.0984 \n",
            "[19/25][228/391] Loss_D: 0.6083 Loss_G: 3.1270 \n",
            "[19/25][229/391] Loss_D: 0.5068 Loss_G: 2.0616 \n",
            "[19/25][230/391] Loss_D: 0.3022 Loss_G: 3.0070 \n",
            "[19/25][231/391] Loss_D: 0.3878 Loss_G: 2.6044 \n",
            "[19/25][232/391] Loss_D: 0.3324 Loss_G: 2.5089 \n",
            "[19/25][233/391] Loss_D: 0.5897 Loss_G: 2.3445 \n",
            "[19/25][234/391] Loss_D: 0.3668 Loss_G: 2.8746 \n",
            "[19/25][235/391] Loss_D: 0.5804 Loss_G: 1.5162 \n",
            "[19/25][236/391] Loss_D: 0.6419 Loss_G: 3.9633 \n",
            "[19/25][237/391] Loss_D: 0.7167 Loss_G: 1.4248 \n",
            "[19/25][238/391] Loss_D: 0.5007 Loss_G: 3.3971 \n",
            "[19/25][239/391] Loss_D: 0.4775 Loss_G: 1.9745 \n",
            "[19/25][240/391] Loss_D: 0.3494 Loss_G: 2.9813 \n",
            "[19/25][241/391] Loss_D: 0.4719 Loss_G: 2.4145 \n",
            "[19/25][242/391] Loss_D: 0.5925 Loss_G: 2.5052 \n",
            "[19/25][243/391] Loss_D: 0.4522 Loss_G: 2.0954 \n",
            "[19/25][244/391] Loss_D: 0.5205 Loss_G: 2.6538 \n",
            "[19/25][245/391] Loss_D: 0.4205 Loss_G: 2.6670 \n",
            "[19/25][246/391] Loss_D: 0.3547 Loss_G: 2.6651 \n",
            "[19/25][247/391] Loss_D: 0.3590 Loss_G: 2.8607 \n",
            "[19/25][248/391] Loss_D: 0.4807 Loss_G: 1.7577 \n",
            "[19/25][249/391] Loss_D: 0.5381 Loss_G: 3.6945 \n",
            "[19/25][250/391] Loss_D: 0.4531 Loss_G: 2.1568 \n",
            "[19/25][251/391] Loss_D: 0.4748 Loss_G: 2.6126 \n",
            "[19/25][252/391] Loss_D: 0.4472 Loss_G: 2.6620 \n",
            "[19/25][253/391] Loss_D: 0.4109 Loss_G: 2.8177 \n",
            "[19/25][254/391] Loss_D: 0.4484 Loss_G: 2.0027 \n",
            "[19/25][255/391] Loss_D: 0.4554 Loss_G: 3.2290 \n",
            "[19/25][256/391] Loss_D: 0.2647 Loss_G: 3.1834 \n",
            "[19/25][257/391] Loss_D: 0.4587 Loss_G: 1.9280 \n",
            "[19/25][258/391] Loss_D: 0.5030 Loss_G: 3.0943 \n",
            "[19/25][259/391] Loss_D: 0.2829 Loss_G: 2.9643 \n",
            "[19/25][260/391] Loss_D: 0.3037 Loss_G: 2.4681 \n",
            "[19/25][261/391] Loss_D: 0.3530 Loss_G: 2.8065 \n",
            "[19/25][262/391] Loss_D: 0.3923 Loss_G: 2.3232 \n",
            "[19/25][263/391] Loss_D: 0.3457 Loss_G: 2.3012 \n",
            "[19/25][264/391] Loss_D: 0.6034 Loss_G: 3.0374 \n",
            "[19/25][265/391] Loss_D: 0.4836 Loss_G: 1.7110 \n",
            "[19/25][266/391] Loss_D: 0.5754 Loss_G: 3.7385 \n",
            "[19/25][267/391] Loss_D: 0.2870 Loss_G: 3.2747 \n",
            "[19/25][268/391] Loss_D: 0.3874 Loss_G: 1.8043 \n",
            "[19/25][269/391] Loss_D: 0.4827 Loss_G: 3.4761 \n",
            "[19/25][270/391] Loss_D: 0.4496 Loss_G: 2.3944 \n",
            "[19/25][271/391] Loss_D: 0.4432 Loss_G: 2.3409 \n",
            "[19/25][272/391] Loss_D: 0.3462 Loss_G: 3.4167 \n",
            "[19/25][273/391] Loss_D: 0.6172 Loss_G: 1.3782 \n",
            "[19/25][274/391] Loss_D: 0.5748 Loss_G: 3.1713 \n",
            "[19/25][275/391] Loss_D: 0.6455 Loss_G: 2.8856 \n",
            "[19/25][276/391] Loss_D: 0.5059 Loss_G: 1.5371 \n",
            "[19/25][277/391] Loss_D: 0.6309 Loss_G: 3.8836 \n",
            "[19/25][278/391] Loss_D: 0.5414 Loss_G: 1.7405 \n",
            "[19/25][279/391] Loss_D: 0.4697 Loss_G: 3.0900 \n",
            "[19/25][280/391] Loss_D: 0.3685 Loss_G: 2.9074 \n",
            "[19/25][281/391] Loss_D: 0.6523 Loss_G: 1.6763 \n",
            "[19/25][282/391] Loss_D: 0.5175 Loss_G: 2.9590 \n",
            "[19/25][283/391] Loss_D: 0.4702 Loss_G: 2.5425 \n",
            "[19/25][284/391] Loss_D: 0.4434 Loss_G: 2.2992 \n",
            "[19/25][285/391] Loss_D: 0.5511 Loss_G: 2.9210 \n",
            "[19/25][286/391] Loss_D: 0.4982 Loss_G: 1.8716 \n",
            "[19/25][287/391] Loss_D: 0.4665 Loss_G: 2.6792 \n",
            "[19/25][288/391] Loss_D: 0.3663 Loss_G: 2.6528 \n",
            "[19/25][289/391] Loss_D: 0.2881 Loss_G: 2.7664 \n",
            "[19/25][290/391] Loss_D: 0.2762 Loss_G: 2.7831 \n",
            "[19/25][291/391] Loss_D: 0.6322 Loss_G: 1.6481 \n",
            "[19/25][292/391] Loss_D: 0.4652 Loss_G: 3.7261 \n",
            "[19/25][293/391] Loss_D: 0.5034 Loss_G: 2.0186 \n",
            "[19/25][294/391] Loss_D: 0.4440 Loss_G: 2.1407 \n",
            "[19/25][295/391] Loss_D: 0.4245 Loss_G: 3.7489 \n",
            "[19/25][296/391] Loss_D: 0.4209 Loss_G: 2.2244 \n",
            "[19/25][297/391] Loss_D: 0.3123 Loss_G: 2.1732 \n",
            "[19/25][298/391] Loss_D: 0.6186 Loss_G: 3.7283 \n",
            "[19/25][299/391] Loss_D: 0.4409 Loss_G: 2.2003 \n",
            "[19/25][300/391] Loss_D: 0.5133 Loss_G: 1.7367 \n",
            "saving the output\n",
            "[19/25][301/391] Loss_D: 0.5761 Loss_G: 4.2734 \n",
            "[19/25][302/391] Loss_D: 0.5469 Loss_G: 1.3978 \n",
            "[19/25][303/391] Loss_D: 0.7043 Loss_G: 5.2142 \n",
            "[19/25][304/391] Loss_D: 0.9392 Loss_G: 0.8132 \n",
            "[19/25][305/391] Loss_D: 1.2369 Loss_G: 6.1648 \n",
            "[19/25][306/391] Loss_D: 1.4002 Loss_G: 1.0504 \n",
            "[19/25][307/391] Loss_D: 0.9390 Loss_G: 4.6251 \n",
            "[19/25][308/391] Loss_D: 0.8969 Loss_G: 1.1282 \n",
            "[19/25][309/391] Loss_D: 1.0790 Loss_G: 4.8048 \n",
            "[19/25][310/391] Loss_D: 0.8492 Loss_G: 1.2506 \n",
            "[19/25][311/391] Loss_D: 1.0600 Loss_G: 4.2326 \n",
            "[19/25][312/391] Loss_D: 1.0730 Loss_G: 0.6520 \n",
            "[19/25][313/391] Loss_D: 1.1746 Loss_G: 5.1513 \n",
            "[19/25][314/391] Loss_D: 0.8168 Loss_G: 1.5490 \n",
            "[19/25][315/391] Loss_D: 0.7303 Loss_G: 4.3529 \n",
            "[19/25][316/391] Loss_D: 0.7252 Loss_G: 1.9352 \n",
            "[19/25][317/391] Loss_D: 0.4509 Loss_G: 1.8616 \n",
            "[19/25][318/391] Loss_D: 0.8604 Loss_G: 5.0502 \n",
            "[19/25][319/391] Loss_D: 1.3726 Loss_G: 0.9362 \n",
            "[19/25][320/391] Loss_D: 0.9882 Loss_G: 4.2417 \n",
            "[19/25][321/391] Loss_D: 0.7073 Loss_G: 1.5245 \n",
            "[19/25][322/391] Loss_D: 0.5854 Loss_G: 3.7197 \n",
            "[19/25][323/391] Loss_D: 0.5279 Loss_G: 2.1566 \n",
            "[19/25][324/391] Loss_D: 0.5878 Loss_G: 2.6758 \n",
            "[19/25][325/391] Loss_D: 0.3762 Loss_G: 3.2926 \n",
            "[19/25][326/391] Loss_D: 0.5953 Loss_G: 1.6568 \n",
            "[19/25][327/391] Loss_D: 0.4874 Loss_G: 3.3494 \n",
            "[19/25][328/391] Loss_D: 0.5282 Loss_G: 2.3406 \n",
            "[19/25][329/391] Loss_D: 0.4491 Loss_G: 2.8978 \n",
            "[19/25][330/391] Loss_D: 0.4763 Loss_G: 2.4434 \n",
            "[19/25][331/391] Loss_D: 0.5822 Loss_G: 2.6512 \n",
            "[19/25][332/391] Loss_D: 0.5335 Loss_G: 1.9218 \n",
            "[19/25][333/391] Loss_D: 0.3568 Loss_G: 3.0547 \n",
            "[19/25][334/391] Loss_D: 0.3243 Loss_G: 2.9701 \n",
            "[19/25][335/391] Loss_D: 0.4912 Loss_G: 2.1310 \n",
            "[19/25][336/391] Loss_D: 0.4780 Loss_G: 3.2855 \n",
            "[19/25][337/391] Loss_D: 0.5612 Loss_G: 2.0487 \n",
            "[19/25][338/391] Loss_D: 0.5271 Loss_G: 3.1422 \n",
            "[19/25][339/391] Loss_D: 0.4248 Loss_G: 2.5978 \n",
            "[19/25][340/391] Loss_D: 0.3600 Loss_G: 3.0050 \n",
            "[19/25][341/391] Loss_D: 0.4862 Loss_G: 1.9014 \n",
            "[19/25][342/391] Loss_D: 0.5075 Loss_G: 3.3001 \n",
            "[19/25][343/391] Loss_D: 0.5775 Loss_G: 1.7834 \n",
            "[19/25][344/391] Loss_D: 0.5233 Loss_G: 4.2940 \n",
            "[19/25][345/391] Loss_D: 0.6895 Loss_G: 1.5506 \n",
            "[19/25][346/391] Loss_D: 0.6746 Loss_G: 3.8608 \n",
            "[19/25][347/391] Loss_D: 0.5335 Loss_G: 2.0350 \n",
            "[19/25][348/391] Loss_D: 0.4518 Loss_G: 2.9707 \n",
            "[19/25][349/391] Loss_D: 0.4061 Loss_G: 2.7250 \n",
            "[19/25][350/391] Loss_D: 0.5150 Loss_G: 1.9841 \n",
            "[19/25][351/391] Loss_D: 0.5643 Loss_G: 4.1628 \n",
            "[19/25][352/391] Loss_D: 0.7757 Loss_G: 1.3710 \n",
            "[19/25][353/391] Loss_D: 0.7820 Loss_G: 4.0697 \n",
            "[19/25][354/391] Loss_D: 0.4752 Loss_G: 2.9829 \n",
            "[19/25][355/391] Loss_D: 0.4987 Loss_G: 1.4100 \n",
            "[19/25][356/391] Loss_D: 0.6890 Loss_G: 4.0872 \n",
            "[19/25][357/391] Loss_D: 0.6433 Loss_G: 1.8186 \n",
            "[19/25][358/391] Loss_D: 0.6157 Loss_G: 2.9383 \n",
            "[19/25][359/391] Loss_D: 0.5830 Loss_G: 1.9125 \n",
            "[19/25][360/391] Loss_D: 0.3596 Loss_G: 3.1311 \n",
            "[19/25][361/391] Loss_D: 0.5477 Loss_G: 2.6288 \n",
            "[19/25][362/391] Loss_D: 0.3889 Loss_G: 2.6519 \n",
            "[19/25][363/391] Loss_D: 0.4752 Loss_G: 2.4007 \n",
            "[19/25][364/391] Loss_D: 0.4424 Loss_G: 2.9589 \n",
            "[19/25][365/391] Loss_D: 0.3130 Loss_G: 3.0354 \n",
            "[19/25][366/391] Loss_D: 0.4681 Loss_G: 2.7769 \n",
            "[19/25][367/391] Loss_D: 0.4117 Loss_G: 2.2167 \n",
            "[19/25][368/391] Loss_D: 0.4907 Loss_G: 1.8829 \n",
            "[19/25][369/391] Loss_D: 0.6531 Loss_G: 3.1927 \n",
            "[19/25][370/391] Loss_D: 0.3974 Loss_G: 2.5629 \n",
            "[19/25][371/391] Loss_D: 0.4002 Loss_G: 2.1361 \n",
            "[19/25][372/391] Loss_D: 0.6642 Loss_G: 3.9884 \n",
            "[19/25][373/391] Loss_D: 0.8153 Loss_G: 0.8012 \n",
            "[19/25][374/391] Loss_D: 0.7550 Loss_G: 4.4160 \n",
            "[19/25][375/391] Loss_D: 0.8299 Loss_G: 0.8545 \n",
            "[19/25][376/391] Loss_D: 1.1739 Loss_G: 4.8852 \n",
            "[19/25][377/391] Loss_D: 1.1640 Loss_G: 0.4128 \n",
            "[19/25][378/391] Loss_D: 2.2285 Loss_G: 7.1751 \n",
            "[19/25][379/391] Loss_D: 2.4154 Loss_G: 0.1124 \n",
            "[19/25][380/391] Loss_D: 3.0539 Loss_G: 8.3581 \n",
            "[19/25][381/391] Loss_D: 4.1116 Loss_G: 0.0321 \n",
            "[19/25][382/391] Loss_D: 3.9109 Loss_G: 8.8585 \n",
            "[19/25][383/391] Loss_D: 6.5297 Loss_G: 0.1130 \n",
            "[19/25][384/391] Loss_D: 3.5765 Loss_G: 6.1591 \n",
            "[19/25][385/391] Loss_D: 3.6509 Loss_G: 0.4310 \n",
            "[19/25][386/391] Loss_D: 2.8892 Loss_G: 3.6984 \n",
            "[19/25][387/391] Loss_D: 1.6925 Loss_G: 1.5469 \n",
            "[19/25][388/391] Loss_D: 1.0489 Loss_G: 2.3803 \n",
            "[19/25][389/391] Loss_D: 1.1486 Loss_G: 3.0880 \n",
            "[19/25][390/391] Loss_D: 1.0934 Loss_G: 1.9586 \n",
            "[20/25][0/391] Loss_D: 1.2287 Loss_G: 3.0592 \n",
            "saving the output\n",
            "[20/25][1/391] Loss_D: 1.3026 Loss_G: 0.9368 \n",
            "[20/25][2/391] Loss_D: 1.2894 Loss_G: 4.8339 \n",
            "[20/25][3/391] Loss_D: 0.9441 Loss_G: 2.1450 \n",
            "[20/25][4/391] Loss_D: 1.0104 Loss_G: 2.8251 \n",
            "[20/25][5/391] Loss_D: 0.7418 Loss_G: 2.4636 \n",
            "[20/25][6/391] Loss_D: 0.7330 Loss_G: 2.4925 \n",
            "[20/25][7/391] Loss_D: 0.8079 Loss_G: 3.8561 \n",
            "[20/25][8/391] Loss_D: 0.9922 Loss_G: 1.3554 \n",
            "[20/25][9/391] Loss_D: 1.2047 Loss_G: 4.6363 \n",
            "[20/25][10/391] Loss_D: 0.6089 Loss_G: 3.0192 \n",
            "[20/25][11/391] Loss_D: 0.7230 Loss_G: 1.3399 \n",
            "[20/25][12/391] Loss_D: 0.9301 Loss_G: 3.8506 \n",
            "[20/25][13/391] Loss_D: 0.5558 Loss_G: 3.1114 \n",
            "[20/25][14/391] Loss_D: 0.6395 Loss_G: 1.8510 \n",
            "[20/25][15/391] Loss_D: 0.7637 Loss_G: 1.8099 \n",
            "[20/25][16/391] Loss_D: 0.6448 Loss_G: 3.8910 \n",
            "[20/25][17/391] Loss_D: 0.7346 Loss_G: 1.9021 \n",
            "[20/25][18/391] Loss_D: 0.5620 Loss_G: 3.0100 \n",
            "[20/25][19/391] Loss_D: 0.7064 Loss_G: 2.5206 \n",
            "[20/25][20/391] Loss_D: 0.7045 Loss_G: 1.7013 \n",
            "[20/25][21/391] Loss_D: 0.6005 Loss_G: 3.7355 \n",
            "[20/25][22/391] Loss_D: 0.5989 Loss_G: 2.2385 \n",
            "[20/25][23/391] Loss_D: 0.4740 Loss_G: 2.4135 \n",
            "[20/25][24/391] Loss_D: 0.6400 Loss_G: 3.4090 \n",
            "[20/25][25/391] Loss_D: 0.6285 Loss_G: 2.2707 \n",
            "[20/25][26/391] Loss_D: 0.5117 Loss_G: 2.4443 \n",
            "[20/25][27/391] Loss_D: 0.4312 Loss_G: 2.8667 \n",
            "[20/25][28/391] Loss_D: 0.5500 Loss_G: 2.8459 \n",
            "[20/25][29/391] Loss_D: 0.5469 Loss_G: 1.8900 \n",
            "[20/25][30/391] Loss_D: 0.5167 Loss_G: 3.0811 \n",
            "[20/25][31/391] Loss_D: 0.4272 Loss_G: 2.7968 \n",
            "[20/25][32/391] Loss_D: 0.5539 Loss_G: 1.6330 \n",
            "[20/25][33/391] Loss_D: 0.5627 Loss_G: 3.2285 \n",
            "[20/25][34/391] Loss_D: 0.4552 Loss_G: 2.7236 \n",
            "[20/25][35/391] Loss_D: 0.4491 Loss_G: 2.1589 \n",
            "[20/25][36/391] Loss_D: 0.4351 Loss_G: 2.3728 \n",
            "[20/25][37/391] Loss_D: 0.3671 Loss_G: 2.7684 \n",
            "[20/25][38/391] Loss_D: 0.5128 Loss_G: 2.3420 \n",
            "[20/25][39/391] Loss_D: 0.3613 Loss_G: 2.7795 \n",
            "[20/25][40/391] Loss_D: 0.5277 Loss_G: 2.0121 \n",
            "[20/25][41/391] Loss_D: 0.5461 Loss_G: 2.7070 \n",
            "[20/25][42/391] Loss_D: 0.3527 Loss_G: 2.7670 \n",
            "[20/25][43/391] Loss_D: 0.4164 Loss_G: 2.8000 \n",
            "[20/25][44/391] Loss_D: 0.4909 Loss_G: 2.2183 \n",
            "[20/25][45/391] Loss_D: 0.5907 Loss_G: 2.5075 \n",
            "[20/25][46/391] Loss_D: 0.4010 Loss_G: 2.5365 \n",
            "[20/25][47/391] Loss_D: 0.4294 Loss_G: 2.2665 \n",
            "[20/25][48/391] Loss_D: 0.4388 Loss_G: 2.3688 \n",
            "[20/25][49/391] Loss_D: 0.4173 Loss_G: 2.7295 \n",
            "[20/25][50/391] Loss_D: 0.3707 Loss_G: 2.7951 \n",
            "[20/25][51/391] Loss_D: 0.3716 Loss_G: 2.8398 \n",
            "[20/25][52/391] Loss_D: 0.4057 Loss_G: 2.5947 \n",
            "[20/25][53/391] Loss_D: 0.3721 Loss_G: 2.5157 \n",
            "[20/25][54/391] Loss_D: 0.5191 Loss_G: 2.6037 \n",
            "[20/25][55/391] Loss_D: 0.5503 Loss_G: 2.0286 \n",
            "[20/25][56/391] Loss_D: 0.4866 Loss_G: 2.7341 \n",
            "[20/25][57/391] Loss_D: 0.3294 Loss_G: 2.7583 \n",
            "[20/25][58/391] Loss_D: 0.4276 Loss_G: 2.1778 \n",
            "[20/25][59/391] Loss_D: 0.4396 Loss_G: 2.9034 \n",
            "[20/25][60/391] Loss_D: 0.3745 Loss_G: 2.7879 \n",
            "[20/25][61/391] Loss_D: 0.5770 Loss_G: 1.3893 \n",
            "[20/25][62/391] Loss_D: 0.5844 Loss_G: 3.2088 \n",
            "[20/25][63/391] Loss_D: 0.4155 Loss_G: 2.5959 \n",
            "[20/25][64/391] Loss_D: 0.4762 Loss_G: 2.2193 \n",
            "[20/25][65/391] Loss_D: 0.4115 Loss_G: 2.4131 \n",
            "[20/25][66/391] Loss_D: 0.4813 Loss_G: 2.7596 \n",
            "[20/25][67/391] Loss_D: 0.4197 Loss_G: 2.3646 \n",
            "[20/25][68/391] Loss_D: 0.4361 Loss_G: 2.0844 \n",
            "[20/25][69/391] Loss_D: 0.4413 Loss_G: 3.0922 \n",
            "[20/25][70/391] Loss_D: 0.3011 Loss_G: 2.7996 \n",
            "[20/25][71/391] Loss_D: 0.4081 Loss_G: 2.0124 \n",
            "[20/25][72/391] Loss_D: 0.4116 Loss_G: 2.3900 \n",
            "[20/25][73/391] Loss_D: 0.4715 Loss_G: 3.0811 \n",
            "[20/25][74/391] Loss_D: 0.4830 Loss_G: 2.1129 \n",
            "[20/25][75/391] Loss_D: 0.4666 Loss_G: 2.7996 \n",
            "[20/25][76/391] Loss_D: 0.5242 Loss_G: 1.9226 \n",
            "[20/25][77/391] Loss_D: 0.4572 Loss_G: 2.3279 \n",
            "[20/25][78/391] Loss_D: 0.4616 Loss_G: 2.7059 \n",
            "[20/25][79/391] Loss_D: 0.4290 Loss_G: 2.8562 \n",
            "[20/25][80/391] Loss_D: 0.2616 Loss_G: 2.9080 \n",
            "[20/25][81/391] Loss_D: 0.3618 Loss_G: 2.2573 \n",
            "[20/25][82/391] Loss_D: 0.5713 Loss_G: 1.8646 \n",
            "[20/25][83/391] Loss_D: 0.4742 Loss_G: 2.4187 \n",
            "[20/25][84/391] Loss_D: 0.3728 Loss_G: 2.9224 \n",
            "[20/25][85/391] Loss_D: 0.4755 Loss_G: 2.2128 \n",
            "[20/25][86/391] Loss_D: 0.5142 Loss_G: 1.6830 \n",
            "[20/25][87/391] Loss_D: 0.4234 Loss_G: 2.7675 \n",
            "[20/25][88/391] Loss_D: 0.4450 Loss_G: 2.5120 \n",
            "[20/25][89/391] Loss_D: 0.4813 Loss_G: 2.5186 \n",
            "[20/25][90/391] Loss_D: 0.2030 Loss_G: 3.4267 \n",
            "[20/25][91/391] Loss_D: 0.5278 Loss_G: 1.8077 \n",
            "[20/25][92/391] Loss_D: 0.4070 Loss_G: 2.9009 \n",
            "[20/25][93/391] Loss_D: 0.5025 Loss_G: 2.6971 \n",
            "[20/25][94/391] Loss_D: 0.4372 Loss_G: 1.9359 \n",
            "[20/25][95/391] Loss_D: 0.6555 Loss_G: 3.4274 \n",
            "[20/25][96/391] Loss_D: 0.5277 Loss_G: 2.0725 \n",
            "[20/25][97/391] Loss_D: 0.4343 Loss_G: 1.9104 \n",
            "[20/25][98/391] Loss_D: 0.5239 Loss_G: 3.4908 \n",
            "[20/25][99/391] Loss_D: 0.7921 Loss_G: 1.3622 \n",
            "[20/25][100/391] Loss_D: 0.4862 Loss_G: 3.2655 \n",
            "saving the output\n",
            "[20/25][101/391] Loss_D: 0.3953 Loss_G: 2.9889 \n",
            "[20/25][102/391] Loss_D: 0.4717 Loss_G: 2.2208 \n",
            "[20/25][103/391] Loss_D: 0.4923 Loss_G: 2.1370 \n",
            "[20/25][104/391] Loss_D: 0.3992 Loss_G: 3.3248 \n",
            "[20/25][105/391] Loss_D: 0.4191 Loss_G: 2.5138 \n",
            "[20/25][106/391] Loss_D: 0.4210 Loss_G: 1.7953 \n",
            "[20/25][107/391] Loss_D: 0.6921 Loss_G: 4.2296 \n",
            "[20/25][108/391] Loss_D: 0.6556 Loss_G: 1.2286 \n",
            "[20/25][109/391] Loss_D: 0.7382 Loss_G: 3.4266 \n",
            "[20/25][110/391] Loss_D: 0.2675 Loss_G: 3.8124 \n",
            "[20/25][111/391] Loss_D: 1.0886 Loss_G: 0.4845 \n",
            "[20/25][112/391] Loss_D: 1.6215 Loss_G: 4.8812 \n",
            "[20/25][113/391] Loss_D: 0.7768 Loss_G: 1.0950 \n",
            "[20/25][114/391] Loss_D: 0.8837 Loss_G: 4.3363 \n",
            "[20/25][115/391] Loss_D: 1.2447 Loss_G: 0.4014 \n",
            "[20/25][116/391] Loss_D: 1.7061 Loss_G: 5.7756 \n",
            "[20/25][117/391] Loss_D: 1.1414 Loss_G: 1.4910 \n",
            "[20/25][118/391] Loss_D: 0.7181 Loss_G: 3.3977 \n",
            "[20/25][119/391] Loss_D: 0.5712 Loss_G: 2.2546 \n",
            "[20/25][120/391] Loss_D: 0.4982 Loss_G: 2.2099 \n",
            "[20/25][121/391] Loss_D: 0.6531 Loss_G: 2.9306 \n",
            "[20/25][122/391] Loss_D: 0.5441 Loss_G: 2.1768 \n",
            "[20/25][123/391] Loss_D: 0.5551 Loss_G: 2.8937 \n",
            "[20/25][124/391] Loss_D: 0.4767 Loss_G: 2.2329 \n",
            "[20/25][125/391] Loss_D: 0.5976 Loss_G: 3.5946 \n",
            "[20/25][126/391] Loss_D: 0.7379 Loss_G: 1.2835 \n",
            "[20/25][127/391] Loss_D: 0.6912 Loss_G: 3.2636 \n",
            "[20/25][128/391] Loss_D: 0.4302 Loss_G: 2.6799 \n",
            "[20/25][129/391] Loss_D: 0.4315 Loss_G: 3.2323 \n",
            "[20/25][130/391] Loss_D: 0.7266 Loss_G: 1.0312 \n",
            "[20/25][131/391] Loss_D: 0.8344 Loss_G: 4.3685 \n",
            "[20/25][132/391] Loss_D: 0.7115 Loss_G: 1.0364 \n",
            "[20/25][133/391] Loss_D: 0.7586 Loss_G: 5.6848 \n",
            "[20/25][134/391] Loss_D: 1.4420 Loss_G: 1.1818 \n",
            "[20/25][135/391] Loss_D: 0.8694 Loss_G: 2.9860 \n",
            "[20/25][136/391] Loss_D: 0.4497 Loss_G: 2.6206 \n",
            "[20/25][137/391] Loss_D: 0.5064 Loss_G: 3.0944 \n",
            "[20/25][138/391] Loss_D: 0.5888 Loss_G: 1.7819 \n",
            "[20/25][139/391] Loss_D: 0.4637 Loss_G: 2.5559 \n",
            "[20/25][140/391] Loss_D: 0.3848 Loss_G: 3.4425 \n",
            "[20/25][141/391] Loss_D: 0.5833 Loss_G: 1.7525 \n",
            "[20/25][142/391] Loss_D: 0.5581 Loss_G: 2.9025 \n",
            "[20/25][143/391] Loss_D: 0.4990 Loss_G: 2.9661 \n",
            "[20/25][144/391] Loss_D: 0.5223 Loss_G: 1.7529 \n",
            "[20/25][145/391] Loss_D: 0.4482 Loss_G: 3.3581 \n",
            "[20/25][146/391] Loss_D: 0.3801 Loss_G: 2.7821 \n",
            "[20/25][147/391] Loss_D: 0.4030 Loss_G: 2.3464 \n",
            "[20/25][148/391] Loss_D: 0.3410 Loss_G: 3.0450 \n",
            "[20/25][149/391] Loss_D: 0.4334 Loss_G: 2.4480 \n",
            "[20/25][150/391] Loss_D: 0.4930 Loss_G: 2.2442 \n",
            "[20/25][151/391] Loss_D: 0.3941 Loss_G: 2.7360 \n",
            "[20/25][152/391] Loss_D: 0.3483 Loss_G: 3.2227 \n",
            "[20/25][153/391] Loss_D: 0.4086 Loss_G: 2.2713 \n",
            "[20/25][154/391] Loss_D: 0.4292 Loss_G: 2.3399 \n",
            "[20/25][155/391] Loss_D: 0.4326 Loss_G: 3.1195 \n",
            "[20/25][156/391] Loss_D: 0.4158 Loss_G: 2.5274 \n",
            "[20/25][157/391] Loss_D: 0.4301 Loss_G: 2.3967 \n",
            "[20/25][158/391] Loss_D: 0.4746 Loss_G: 2.8231 \n",
            "[20/25][159/391] Loss_D: 0.3753 Loss_G: 3.2823 \n",
            "[20/25][160/391] Loss_D: 0.5275 Loss_G: 1.7127 \n",
            "[20/25][161/391] Loss_D: 0.5284 Loss_G: 3.2402 \n",
            "[20/25][162/391] Loss_D: 0.5847 Loss_G: 2.3308 \n",
            "[20/25][163/391] Loss_D: 0.4227 Loss_G: 2.5587 \n",
            "[20/25][164/391] Loss_D: 0.3914 Loss_G: 3.2098 \n",
            "[20/25][165/391] Loss_D: 0.4264 Loss_G: 2.3131 \n",
            "[20/25][166/391] Loss_D: 0.4027 Loss_G: 2.6251 \n",
            "[20/25][167/391] Loss_D: 0.4339 Loss_G: 2.6771 \n",
            "[20/25][168/391] Loss_D: 0.5115 Loss_G: 3.1292 \n",
            "[20/25][169/391] Loss_D: 0.4202 Loss_G: 2.1710 \n",
            "[20/25][170/391] Loss_D: 0.5106 Loss_G: 3.4084 \n",
            "[20/25][171/391] Loss_D: 0.7886 Loss_G: 1.1823 \n",
            "[20/25][172/391] Loss_D: 0.7114 Loss_G: 3.7588 \n",
            "[20/25][173/391] Loss_D: 0.4434 Loss_G: 2.5744 \n",
            "[20/25][174/391] Loss_D: 0.3210 Loss_G: 2.4835 \n",
            "[20/25][175/391] Loss_D: 0.6652 Loss_G: 1.9229 \n",
            "[20/25][176/391] Loss_D: 0.5592 Loss_G: 2.5598 \n",
            "[20/25][177/391] Loss_D: 0.3839 Loss_G: 3.7104 \n",
            "[20/25][178/391] Loss_D: 0.7008 Loss_G: 1.3676 \n",
            "[20/25][179/391] Loss_D: 0.7199 Loss_G: 3.2933 \n",
            "[20/25][180/391] Loss_D: 0.5296 Loss_G: 2.0514 \n",
            "[20/25][181/391] Loss_D: 0.4308 Loss_G: 3.8952 \n",
            "[20/25][182/391] Loss_D: 0.7526 Loss_G: 1.1124 \n",
            "[20/25][183/391] Loss_D: 0.9470 Loss_G: 4.3469 \n",
            "[20/25][184/391] Loss_D: 0.6861 Loss_G: 1.7881 \n",
            "[20/25][185/391] Loss_D: 0.6100 Loss_G: 2.6831 \n",
            "[20/25][186/391] Loss_D: 0.3128 Loss_G: 3.7130 \n",
            "[20/25][187/391] Loss_D: 0.4784 Loss_G: 2.0985 \n",
            "[20/25][188/391] Loss_D: 0.4463 Loss_G: 3.0272 \n",
            "[20/25][189/391] Loss_D: 0.5821 Loss_G: 1.6465 \n",
            "[20/25][190/391] Loss_D: 0.5008 Loss_G: 3.0120 \n",
            "[20/25][191/391] Loss_D: 0.4709 Loss_G: 2.8118 \n",
            "[20/25][192/391] Loss_D: 0.4926 Loss_G: 1.9715 \n",
            "[20/25][193/391] Loss_D: 0.4422 Loss_G: 3.4298 \n",
            "[20/25][194/391] Loss_D: 0.4721 Loss_G: 1.9198 \n",
            "[20/25][195/391] Loss_D: 0.5286 Loss_G: 3.1167 \n",
            "[20/25][196/391] Loss_D: 0.5298 Loss_G: 2.5232 \n",
            "[20/25][197/391] Loss_D: 0.4241 Loss_G: 2.1219 \n",
            "[20/25][198/391] Loss_D: 0.4676 Loss_G: 2.8954 \n",
            "[20/25][199/391] Loss_D: 0.5678 Loss_G: 2.1831 \n",
            "[20/25][200/391] Loss_D: 0.5198 Loss_G: 2.7370 \n",
            "saving the output\n",
            "[20/25][201/391] Loss_D: 0.4231 Loss_G: 2.5505 \n",
            "[20/25][202/391] Loss_D: 0.3497 Loss_G: 3.0419 \n",
            "[20/25][203/391] Loss_D: 0.3399 Loss_G: 2.7886 \n",
            "[20/25][204/391] Loss_D: 0.4283 Loss_G: 2.0271 \n",
            "[20/25][205/391] Loss_D: 0.5462 Loss_G: 3.4166 \n",
            "[20/25][206/391] Loss_D: 0.4625 Loss_G: 2.2851 \n",
            "[20/25][207/391] Loss_D: 0.2413 Loss_G: 2.9494 \n",
            "[20/25][208/391] Loss_D: 0.3218 Loss_G: 3.0986 \n",
            "[20/25][209/391] Loss_D: 0.4281 Loss_G: 2.0609 \n",
            "[20/25][210/391] Loss_D: 0.5065 Loss_G: 3.6217 \n",
            "[20/25][211/391] Loss_D: 0.6353 Loss_G: 1.5857 \n",
            "[20/25][212/391] Loss_D: 0.5545 Loss_G: 3.3004 \n",
            "[20/25][213/391] Loss_D: 0.5269 Loss_G: 1.7345 \n",
            "[20/25][214/391] Loss_D: 0.4977 Loss_G: 3.9870 \n",
            "[20/25][215/391] Loss_D: 0.6667 Loss_G: 1.5926 \n",
            "[20/25][216/391] Loss_D: 0.6442 Loss_G: 2.5684 \n",
            "[20/25][217/391] Loss_D: 0.4242 Loss_G: 3.1098 \n",
            "[20/25][218/391] Loss_D: 0.3382 Loss_G: 2.7322 \n",
            "[20/25][219/391] Loss_D: 0.5120 Loss_G: 1.8440 \n",
            "[20/25][220/391] Loss_D: 0.4109 Loss_G: 3.4309 \n",
            "[20/25][221/391] Loss_D: 0.4750 Loss_G: 2.0553 \n",
            "[20/25][222/391] Loss_D: 0.3321 Loss_G: 2.6393 \n",
            "[20/25][223/391] Loss_D: 0.3620 Loss_G: 3.0096 \n",
            "[20/25][224/391] Loss_D: 0.3482 Loss_G: 2.7196 \n",
            "[20/25][225/391] Loss_D: 0.4870 Loss_G: 2.4687 \n",
            "[20/25][226/391] Loss_D: 0.5305 Loss_G: 1.6167 \n",
            "[20/25][227/391] Loss_D: 0.4062 Loss_G: 3.0817 \n",
            "[20/25][228/391] Loss_D: 0.4258 Loss_G: 3.3086 \n",
            "[20/25][229/391] Loss_D: 0.4966 Loss_G: 2.0226 \n",
            "[20/25][230/391] Loss_D: 0.4205 Loss_G: 2.3988 \n",
            "[20/25][231/391] Loss_D: 0.3872 Loss_G: 2.8858 \n",
            "[20/25][232/391] Loss_D: 0.4521 Loss_G: 2.0560 \n",
            "[20/25][233/391] Loss_D: 0.4077 Loss_G: 2.7228 \n",
            "[20/25][234/391] Loss_D: 0.2910 Loss_G: 3.0432 \n",
            "[20/25][235/391] Loss_D: 0.3832 Loss_G: 2.3507 \n",
            "[20/25][236/391] Loss_D: 0.3662 Loss_G: 2.1733 \n",
            "[20/25][237/391] Loss_D: 0.4692 Loss_G: 3.3663 \n",
            "[20/25][238/391] Loss_D: 0.4698 Loss_G: 1.9602 \n",
            "[20/25][239/391] Loss_D: 0.3480 Loss_G: 2.8986 \n",
            "[20/25][240/391] Loss_D: 0.3416 Loss_G: 3.1529 \n",
            "[20/25][241/391] Loss_D: 0.3359 Loss_G: 2.5660 \n",
            "[20/25][242/391] Loss_D: 0.4658 Loss_G: 1.9348 \n",
            "[20/25][243/391] Loss_D: 0.4755 Loss_G: 3.3644 \n",
            "[20/25][244/391] Loss_D: 0.3724 Loss_G: 2.6134 \n",
            "[20/25][245/391] Loss_D: 0.4323 Loss_G: 1.6636 \n",
            "[20/25][246/391] Loss_D: 0.6441 Loss_G: 4.1151 \n",
            "[20/25][247/391] Loss_D: 0.3917 Loss_G: 2.6117 \n",
            "[20/25][248/391] Loss_D: 0.3764 Loss_G: 1.9422 \n",
            "[20/25][249/391] Loss_D: 0.4324 Loss_G: 3.8073 \n",
            "[20/25][250/391] Loss_D: 0.3680 Loss_G: 2.4904 \n",
            "[20/25][251/391] Loss_D: 0.2943 Loss_G: 2.6470 \n",
            "[20/25][252/391] Loss_D: 0.5120 Loss_G: 2.0825 \n",
            "[20/25][253/391] Loss_D: 0.3908 Loss_G: 3.4943 \n",
            "[20/25][254/391] Loss_D: 0.4903 Loss_G: 1.8598 \n",
            "[20/25][255/391] Loss_D: 0.5076 Loss_G: 2.5475 \n",
            "[20/25][256/391] Loss_D: 0.5789 Loss_G: 2.1389 \n",
            "[20/25][257/391] Loss_D: 0.4342 Loss_G: 2.9394 \n",
            "[20/25][258/391] Loss_D: 0.4243 Loss_G: 3.8066 \n",
            "[20/25][259/391] Loss_D: 0.4203 Loss_G: 2.1154 \n",
            "[20/25][260/391] Loss_D: 0.4931 Loss_G: 2.7345 \n",
            "[20/25][261/391] Loss_D: 0.3224 Loss_G: 3.4011 \n",
            "[20/25][262/391] Loss_D: 0.3388 Loss_G: 2.5113 \n",
            "[20/25][263/391] Loss_D: 0.4120 Loss_G: 2.8075 \n",
            "[20/25][264/391] Loss_D: 0.6297 Loss_G: 1.9308 \n",
            "[20/25][265/391] Loss_D: 0.4186 Loss_G: 2.9871 \n",
            "[20/25][266/391] Loss_D: 0.3981 Loss_G: 2.5671 \n",
            "[20/25][267/391] Loss_D: 0.5792 Loss_G: 1.8264 \n",
            "[20/25][268/391] Loss_D: 0.6969 Loss_G: 4.6857 \n",
            "[20/25][269/391] Loss_D: 0.6417 Loss_G: 1.2883 \n",
            "[20/25][270/391] Loss_D: 1.0018 Loss_G: 4.3999 \n",
            "[20/25][271/391] Loss_D: 0.6768 Loss_G: 1.7309 \n",
            "[20/25][272/391] Loss_D: 0.6664 Loss_G: 3.5332 \n",
            "[20/25][273/391] Loss_D: 0.8436 Loss_G: 1.0339 \n",
            "[20/25][274/391] Loss_D: 0.7436 Loss_G: 3.9148 \n",
            "[20/25][275/391] Loss_D: 0.5714 Loss_G: 1.8037 \n",
            "[20/25][276/391] Loss_D: 0.6167 Loss_G: 3.1188 \n",
            "[20/25][277/391] Loss_D: 0.5818 Loss_G: 1.6400 \n",
            "[20/25][278/391] Loss_D: 0.6214 Loss_G: 4.2617 \n",
            "[20/25][279/391] Loss_D: 0.6527 Loss_G: 1.4267 \n",
            "[20/25][280/391] Loss_D: 0.5371 Loss_G: 3.5542 \n",
            "[20/25][281/391] Loss_D: 0.4935 Loss_G: 2.3140 \n",
            "[20/25][282/391] Loss_D: 0.4677 Loss_G: 2.2513 \n",
            "[20/25][283/391] Loss_D: 0.4437 Loss_G: 3.6972 \n",
            "[20/25][284/391] Loss_D: 0.3728 Loss_G: 2.5849 \n",
            "[20/25][285/391] Loss_D: 0.4204 Loss_G: 2.2400 \n",
            "[20/25][286/391] Loss_D: 0.4649 Loss_G: 1.8428 \n",
            "[20/25][287/391] Loss_D: 0.6734 Loss_G: 3.8968 \n",
            "[20/25][288/391] Loss_D: 0.6643 Loss_G: 1.5047 \n",
            "[20/25][289/391] Loss_D: 0.6504 Loss_G: 4.2694 \n",
            "[20/25][290/391] Loss_D: 0.4857 Loss_G: 1.8787 \n",
            "[20/25][291/391] Loss_D: 0.4008 Loss_G: 3.5144 \n",
            "[20/25][292/391] Loss_D: 0.3897 Loss_G: 2.5323 \n",
            "[20/25][293/391] Loss_D: 0.4875 Loss_G: 2.6528 \n",
            "[20/25][294/391] Loss_D: 0.4579 Loss_G: 1.6354 \n",
            "[20/25][295/391] Loss_D: 0.5796 Loss_G: 4.8224 \n",
            "[20/25][296/391] Loss_D: 0.8286 Loss_G: 1.3478 \n",
            "[20/25][297/391] Loss_D: 0.8011 Loss_G: 4.1018 \n",
            "[20/25][298/391] Loss_D: 0.5695 Loss_G: 2.3261 \n",
            "[20/25][299/391] Loss_D: 0.6340 Loss_G: 2.6594 \n",
            "[20/25][300/391] Loss_D: 0.4278 Loss_G: 2.5840 \n",
            "saving the output\n",
            "[20/25][301/391] Loss_D: 0.4500 Loss_G: 2.8384 \n",
            "[20/25][302/391] Loss_D: 0.4308 Loss_G: 2.6469 \n",
            "[20/25][303/391] Loss_D: 0.5109 Loss_G: 1.3334 \n",
            "[20/25][304/391] Loss_D: 0.6526 Loss_G: 4.1040 \n",
            "[20/25][305/391] Loss_D: 0.5490 Loss_G: 2.2666 \n",
            "[20/25][306/391] Loss_D: 0.3946 Loss_G: 2.5341 \n",
            "[20/25][307/391] Loss_D: 0.4847 Loss_G: 3.0367 \n",
            "[20/25][308/391] Loss_D: 0.3947 Loss_G: 2.0427 \n",
            "[20/25][309/391] Loss_D: 0.5268 Loss_G: 3.4128 \n",
            "[20/25][310/391] Loss_D: 0.5252 Loss_G: 1.7396 \n",
            "[20/25][311/391] Loss_D: 0.5286 Loss_G: 3.8165 \n",
            "[20/25][312/391] Loss_D: 0.5312 Loss_G: 1.8625 \n",
            "[20/25][313/391] Loss_D: 0.4425 Loss_G: 2.7172 \n",
            "[20/25][314/391] Loss_D: 0.2897 Loss_G: 3.2946 \n",
            "[20/25][315/391] Loss_D: 0.3014 Loss_G: 3.0604 \n",
            "[20/25][316/391] Loss_D: 0.4991 Loss_G: 1.6248 \n",
            "[20/25][317/391] Loss_D: 0.5642 Loss_G: 3.5092 \n",
            "[20/25][318/391] Loss_D: 0.4887 Loss_G: 2.1375 \n",
            "[20/25][319/391] Loss_D: 0.4511 Loss_G: 3.5398 \n",
            "[20/25][320/391] Loss_D: 0.4763 Loss_G: 1.9887 \n",
            "[20/25][321/391] Loss_D: 0.3762 Loss_G: 3.2973 \n",
            "[20/25][322/391] Loss_D: 0.2194 Loss_G: 3.5695 \n",
            "[20/25][323/391] Loss_D: 0.4217 Loss_G: 2.3507 \n",
            "[20/25][324/391] Loss_D: 0.3248 Loss_G: 2.9359 \n",
            "[20/25][325/391] Loss_D: 0.3539 Loss_G: 3.4508 \n",
            "[20/25][326/391] Loss_D: 0.4937 Loss_G: 2.1308 \n",
            "[20/25][327/391] Loss_D: 0.5872 Loss_G: 3.3472 \n",
            "[20/25][328/391] Loss_D: 0.5296 Loss_G: 2.1317 \n",
            "[20/25][329/391] Loss_D: 0.3021 Loss_G: 3.4114 \n",
            "[20/25][330/391] Loss_D: 0.6472 Loss_G: 1.6847 \n",
            "[20/25][331/391] Loss_D: 0.5006 Loss_G: 3.9088 \n",
            "[20/25][332/391] Loss_D: 0.6662 Loss_G: 1.5381 \n",
            "[20/25][333/391] Loss_D: 0.5389 Loss_G: 3.0286 \n",
            "[20/25][334/391] Loss_D: 0.4582 Loss_G: 2.6754 \n",
            "[20/25][335/391] Loss_D: 0.4941 Loss_G: 2.1280 \n",
            "[20/25][336/391] Loss_D: 0.4487 Loss_G: 3.3024 \n",
            "[20/25][337/391] Loss_D: 0.5355 Loss_G: 1.8440 \n",
            "[20/25][338/391] Loss_D: 0.5048 Loss_G: 4.0033 \n",
            "[20/25][339/391] Loss_D: 0.5170 Loss_G: 2.0974 \n",
            "[20/25][340/391] Loss_D: 0.4113 Loss_G: 2.9496 \n",
            "[20/25][341/391] Loss_D: 0.4409 Loss_G: 2.7805 \n",
            "[20/25][342/391] Loss_D: 0.3617 Loss_G: 2.6925 \n",
            "[20/25][343/391] Loss_D: 0.4889 Loss_G: 2.1804 \n",
            "[20/25][344/391] Loss_D: 0.4781 Loss_G: 3.2981 \n",
            "[20/25][345/391] Loss_D: 0.6142 Loss_G: 2.1249 \n",
            "[20/25][346/391] Loss_D: 0.4484 Loss_G: 2.5077 \n",
            "[20/25][347/391] Loss_D: 0.3412 Loss_G: 3.1826 \n",
            "[20/25][348/391] Loss_D: 0.3345 Loss_G: 3.0041 \n",
            "[20/25][349/391] Loss_D: 0.5363 Loss_G: 2.1987 \n",
            "[20/25][350/391] Loss_D: 0.3276 Loss_G: 3.1647 \n",
            "[20/25][351/391] Loss_D: 0.4518 Loss_G: 2.3799 \n",
            "[20/25][352/391] Loss_D: 0.4076 Loss_G: 2.7591 \n",
            "[20/25][353/391] Loss_D: 0.3901 Loss_G: 2.9748 \n",
            "[20/25][354/391] Loss_D: 0.3119 Loss_G: 2.7529 \n",
            "[20/25][355/391] Loss_D: 0.5765 Loss_G: 1.5937 \n",
            "[20/25][356/391] Loss_D: 0.6256 Loss_G: 4.3541 \n",
            "[20/25][357/391] Loss_D: 0.7191 Loss_G: 0.8599 \n",
            "[20/25][358/391] Loss_D: 0.9879 Loss_G: 5.3478 \n",
            "[20/25][359/391] Loss_D: 0.9543 Loss_G: 0.2596 \n",
            "[20/25][360/391] Loss_D: 2.5543 Loss_G: 8.6057 \n",
            "[20/25][361/391] Loss_D: 4.4695 Loss_G: 0.1968 \n",
            "[20/25][362/391] Loss_D: 2.9047 Loss_G: 7.7108 \n",
            "[20/25][363/391] Loss_D: 4.2199 Loss_G: 0.0030 \n",
            "[20/25][364/391] Loss_D: 8.4582 Loss_G: 3.4140 \n",
            "[20/25][365/391] Loss_D: 1.9332 Loss_G: 1.6215 \n",
            "[20/25][366/391] Loss_D: 1.9146 Loss_G: 3.1110 \n",
            "[20/25][367/391] Loss_D: 3.6267 Loss_G: 0.7975 \n",
            "[20/25][368/391] Loss_D: 2.2384 Loss_G: 4.7612 \n",
            "[20/25][369/391] Loss_D: 2.0206 Loss_G: 0.4866 \n",
            "[20/25][370/391] Loss_D: 2.1740 Loss_G: 4.3016 \n",
            "[20/25][371/391] Loss_D: 1.3999 Loss_G: 1.3904 \n",
            "[20/25][372/391] Loss_D: 0.9789 Loss_G: 3.2405 \n",
            "[20/25][373/391] Loss_D: 0.8636 Loss_G: 1.5073 \n",
            "[20/25][374/391] Loss_D: 1.1001 Loss_G: 4.2603 \n",
            "[20/25][375/391] Loss_D: 1.3129 Loss_G: 0.9205 \n",
            "[20/25][376/391] Loss_D: 1.1174 Loss_G: 3.5113 \n",
            "[20/25][377/391] Loss_D: 0.6756 Loss_G: 4.7003 \n",
            "[20/25][378/391] Loss_D: 1.2365 Loss_G: 0.7656 \n",
            "[20/25][379/391] Loss_D: 2.6717 Loss_G: 4.8086 \n",
            "[20/25][380/391] Loss_D: 1.6865 Loss_G: 0.6396 \n",
            "[20/25][381/391] Loss_D: 1.5448 Loss_G: 4.6300 \n",
            "[20/25][382/391] Loss_D: 1.4740 Loss_G: 1.1799 \n",
            "[20/25][383/391] Loss_D: 0.9057 Loss_G: 3.5844 \n",
            "[20/25][384/391] Loss_D: 0.6627 Loss_G: 2.5305 \n",
            "[20/25][385/391] Loss_D: 0.7225 Loss_G: 1.5694 \n",
            "[20/25][386/391] Loss_D: 0.7119 Loss_G: 3.7822 \n",
            "[20/25][387/391] Loss_D: 0.5775 Loss_G: 3.0560 \n",
            "[20/25][388/391] Loss_D: 0.9931 Loss_G: 0.7507 \n",
            "[20/25][389/391] Loss_D: 1.5136 Loss_G: 4.9290 \n",
            "[20/25][390/391] Loss_D: 1.3851 Loss_G: 1.0825 \n",
            "[21/25][0/391] Loss_D: 0.8784 Loss_G: 4.6156 \n",
            "saving the output\n",
            "[21/25][1/391] Loss_D: 0.7906 Loss_G: 1.4731 \n",
            "[21/25][2/391] Loss_D: 0.6409 Loss_G: 3.4467 \n",
            "[21/25][3/391] Loss_D: 0.6433 Loss_G: 2.6792 \n",
            "[21/25][4/391] Loss_D: 0.5735 Loss_G: 2.0852 \n",
            "[21/25][5/391] Loss_D: 0.4944 Loss_G: 3.4405 \n",
            "[21/25][6/391] Loss_D: 0.6703 Loss_G: 2.0807 \n",
            "[21/25][7/391] Loss_D: 0.3873 Loss_G: 2.7990 \n",
            "[21/25][8/391] Loss_D: 0.4865 Loss_G: 3.0049 \n",
            "[21/25][9/391] Loss_D: 0.5663 Loss_G: 1.9999 \n",
            "[21/25][10/391] Loss_D: 0.6400 Loss_G: 3.0952 \n",
            "[21/25][11/391] Loss_D: 0.4557 Loss_G: 3.6486 \n",
            "[21/25][12/391] Loss_D: 0.6562 Loss_G: 1.5505 \n",
            "[21/25][13/391] Loss_D: 0.8886 Loss_G: 3.8041 \n",
            "[21/25][14/391] Loss_D: 0.6029 Loss_G: 1.7906 \n",
            "[21/25][15/391] Loss_D: 0.7039 Loss_G: 4.6493 \n",
            "[21/25][16/391] Loss_D: 0.5807 Loss_G: 2.5459 \n",
            "[21/25][17/391] Loss_D: 0.5991 Loss_G: 1.9560 \n",
            "[21/25][18/391] Loss_D: 0.5859 Loss_G: 2.8644 \n",
            "[21/25][19/391] Loss_D: 0.4533 Loss_G: 3.2769 \n",
            "[21/25][20/391] Loss_D: 0.5038 Loss_G: 1.8697 \n",
            "[21/25][21/391] Loss_D: 0.6638 Loss_G: 3.2279 \n",
            "[21/25][22/391] Loss_D: 0.4482 Loss_G: 2.7044 \n",
            "[21/25][23/391] Loss_D: 0.4533 Loss_G: 2.3460 \n",
            "[21/25][24/391] Loss_D: 0.5311 Loss_G: 3.1739 \n",
            "[21/25][25/391] Loss_D: 0.4317 Loss_G: 2.6995 \n",
            "[21/25][26/391] Loss_D: 0.4780 Loss_G: 2.2956 \n",
            "[21/25][27/391] Loss_D: 0.4543 Loss_G: 2.7824 \n",
            "[21/25][28/391] Loss_D: 0.4403 Loss_G: 2.4720 \n",
            "[21/25][29/391] Loss_D: 0.4532 Loss_G: 2.4223 \n",
            "[21/25][30/391] Loss_D: 0.4849 Loss_G: 2.3037 \n",
            "[21/25][31/391] Loss_D: 0.4690 Loss_G: 2.8429 \n",
            "[21/25][32/391] Loss_D: 0.5208 Loss_G: 2.1207 \n",
            "[21/25][33/391] Loss_D: 0.4117 Loss_G: 2.8275 \n",
            "[21/25][34/391] Loss_D: 0.2988 Loss_G: 3.0316 \n",
            "[21/25][35/391] Loss_D: 0.4310 Loss_G: 2.6397 \n",
            "[21/25][36/391] Loss_D: 0.2482 Loss_G: 2.8475 \n",
            "[21/25][37/391] Loss_D: 0.4461 Loss_G: 2.3830 \n",
            "[21/25][38/391] Loss_D: 0.4119 Loss_G: 2.6656 \n",
            "[21/25][39/391] Loss_D: 0.4933 Loss_G: 1.9017 \n",
            "[21/25][40/391] Loss_D: 0.7317 Loss_G: 2.9907 \n",
            "[21/25][41/391] Loss_D: 0.6552 Loss_G: 1.4888 \n",
            "[21/25][42/391] Loss_D: 0.8296 Loss_G: 3.3856 \n",
            "[21/25][43/391] Loss_D: 0.4817 Loss_G: 2.3629 \n",
            "[21/25][44/391] Loss_D: 0.4527 Loss_G: 2.3727 \n",
            "[21/25][45/391] Loss_D: 0.4690 Loss_G: 2.9152 \n",
            "[21/25][46/391] Loss_D: 0.4237 Loss_G: 2.4411 \n",
            "[21/25][47/391] Loss_D: 0.2888 Loss_G: 3.0077 \n",
            "[21/25][48/391] Loss_D: 0.3372 Loss_G: 2.9064 \n",
            "[21/25][49/391] Loss_D: 0.3963 Loss_G: 2.3126 \n",
            "[21/25][50/391] Loss_D: 0.4000 Loss_G: 2.7692 \n",
            "[21/25][51/391] Loss_D: 0.5076 Loss_G: 2.6149 \n",
            "[21/25][52/391] Loss_D: 0.6570 Loss_G: 1.2319 \n",
            "[21/25][53/391] Loss_D: 0.7119 Loss_G: 4.3874 \n",
            "[21/25][54/391] Loss_D: 0.5224 Loss_G: 2.2070 \n",
            "[21/25][55/391] Loss_D: 0.3892 Loss_G: 2.0139 \n",
            "[21/25][56/391] Loss_D: 0.6426 Loss_G: 4.5447 \n",
            "[21/25][57/391] Loss_D: 0.7460 Loss_G: 2.2191 \n",
            "[21/25][58/391] Loss_D: 0.3297 Loss_G: 2.4169 \n",
            "[21/25][59/391] Loss_D: 0.4437 Loss_G: 3.0905 \n",
            "[21/25][60/391] Loss_D: 0.4821 Loss_G: 2.3181 \n",
            "[21/25][61/391] Loss_D: 0.3569 Loss_G: 2.9647 \n",
            "[21/25][62/391] Loss_D: 0.3727 Loss_G: 3.0355 \n",
            "[21/25][63/391] Loss_D: 0.5580 Loss_G: 1.8642 \n",
            "[21/25][64/391] Loss_D: 0.3881 Loss_G: 3.3699 \n",
            "[21/25][65/391] Loss_D: 0.4298 Loss_G: 2.6104 \n",
            "[21/25][66/391] Loss_D: 0.4020 Loss_G: 2.9531 \n",
            "[21/25][67/391] Loss_D: 0.4633 Loss_G: 1.9636 \n",
            "[21/25][68/391] Loss_D: 0.5398 Loss_G: 3.4049 \n",
            "[21/25][69/391] Loss_D: 0.3805 Loss_G: 2.9048 \n",
            "[21/25][70/391] Loss_D: 0.4366 Loss_G: 2.0323 \n",
            "[21/25][71/391] Loss_D: 0.4791 Loss_G: 2.1115 \n",
            "[21/25][72/391] Loss_D: 0.5424 Loss_G: 4.0286 \n",
            "[21/25][73/391] Loss_D: 0.7397 Loss_G: 1.0851 \n",
            "[21/25][74/391] Loss_D: 0.7145 Loss_G: 3.5146 \n",
            "[21/25][75/391] Loss_D: 0.5232 Loss_G: 1.0425 \n",
            "[21/25][76/391] Loss_D: 0.9756 Loss_G: 5.3334 \n",
            "[21/25][77/391] Loss_D: 1.0407 Loss_G: 1.3192 \n",
            "[21/25][78/391] Loss_D: 0.6153 Loss_G: 2.3985 \n",
            "[21/25][79/391] Loss_D: 0.6279 Loss_G: 2.5540 \n",
            "[21/25][80/391] Loss_D: 0.4576 Loss_G: 2.9073 \n",
            "[21/25][81/391] Loss_D: 0.4605 Loss_G: 2.0735 \n",
            "[21/25][82/391] Loss_D: 0.3654 Loss_G: 3.0486 \n",
            "[21/25][83/391] Loss_D: 0.3604 Loss_G: 3.2991 \n",
            "[21/25][84/391] Loss_D: 0.4454 Loss_G: 2.0258 \n",
            "[21/25][85/391] Loss_D: 0.4579 Loss_G: 2.4544 \n",
            "[21/25][86/391] Loss_D: 0.4461 Loss_G: 3.2542 \n",
            "[21/25][87/391] Loss_D: 0.7217 Loss_G: 1.5185 \n",
            "[21/25][88/391] Loss_D: 0.4233 Loss_G: 3.0022 \n",
            "[21/25][89/391] Loss_D: 0.4188 Loss_G: 2.8191 \n",
            "[21/25][90/391] Loss_D: 0.4617 Loss_G: 1.8214 \n",
            "[21/25][91/391] Loss_D: 0.5843 Loss_G: 3.5560 \n",
            "[21/25][92/391] Loss_D: 0.7936 Loss_G: 1.3243 \n",
            "[21/25][93/391] Loss_D: 0.5002 Loss_G: 2.6412 \n",
            "[21/25][94/391] Loss_D: 0.3773 Loss_G: 3.1917 \n",
            "[21/25][95/391] Loss_D: 0.5438 Loss_G: 1.7598 \n",
            "[21/25][96/391] Loss_D: 0.4131 Loss_G: 2.5799 \n",
            "[21/25][97/391] Loss_D: 0.4200 Loss_G: 2.8816 \n",
            "[21/25][98/391] Loss_D: 0.3927 Loss_G: 2.6274 \n",
            "[21/25][99/391] Loss_D: 0.4525 Loss_G: 1.9099 \n",
            "[21/25][100/391] Loss_D: 0.5282 Loss_G: 2.9144 \n",
            "saving the output\n",
            "[21/25][101/391] Loss_D: 0.3832 Loss_G: 2.8496 \n",
            "[21/25][102/391] Loss_D: 0.4151 Loss_G: 2.2317 \n",
            "[21/25][103/391] Loss_D: 0.4544 Loss_G: 2.4311 \n",
            "[21/25][104/391] Loss_D: 0.4914 Loss_G: 3.7311 \n",
            "[21/25][105/391] Loss_D: 0.6598 Loss_G: 1.4496 \n",
            "[21/25][106/391] Loss_D: 0.5435 Loss_G: 3.2543 \n",
            "[21/25][107/391] Loss_D: 0.2986 Loss_G: 3.4264 \n",
            "[21/25][108/391] Loss_D: 0.4521 Loss_G: 1.8713 \n",
            "[21/25][109/391] Loss_D: 0.5100 Loss_G: 3.0179 \n",
            "[21/25][110/391] Loss_D: 0.5025 Loss_G: 2.1210 \n",
            "[21/25][111/391] Loss_D: 0.4417 Loss_G: 3.5245 \n",
            "[21/25][112/391] Loss_D: 0.4995 Loss_G: 1.8972 \n",
            "[21/25][113/391] Loss_D: 0.4609 Loss_G: 2.6221 \n",
            "[21/25][114/391] Loss_D: 0.3788 Loss_G: 3.2669 \n",
            "[21/25][115/391] Loss_D: 0.4656 Loss_G: 2.0055 \n",
            "[21/25][116/391] Loss_D: 0.4839 Loss_G: 2.4784 \n",
            "[21/25][117/391] Loss_D: 0.4913 Loss_G: 2.5773 \n",
            "[21/25][118/391] Loss_D: 0.3202 Loss_G: 2.7558 \n",
            "[21/25][119/391] Loss_D: 0.3898 Loss_G: 2.4872 \n",
            "[21/25][120/391] Loss_D: 0.3687 Loss_G: 3.3313 \n",
            "[21/25][121/391] Loss_D: 0.5010 Loss_G: 1.9237 \n",
            "[21/25][122/391] Loss_D: 0.7021 Loss_G: 3.3853 \n",
            "[21/25][123/391] Loss_D: 0.5984 Loss_G: 1.8480 \n",
            "[21/25][124/391] Loss_D: 0.4653 Loss_G: 2.7558 \n",
            "[21/25][125/391] Loss_D: 0.2845 Loss_G: 3.2320 \n",
            "[21/25][126/391] Loss_D: 0.3031 Loss_G: 2.6677 \n",
            "[21/25][127/391] Loss_D: 0.3487 Loss_G: 2.1537 \n",
            "[21/25][128/391] Loss_D: 0.4379 Loss_G: 3.0847 \n",
            "[21/25][129/391] Loss_D: 0.5676 Loss_G: 1.5272 \n",
            "[21/25][130/391] Loss_D: 0.5926 Loss_G: 3.9411 \n",
            "[21/25][131/391] Loss_D: 0.6520 Loss_G: 1.6855 \n",
            "[21/25][132/391] Loss_D: 0.4118 Loss_G: 2.7855 \n",
            "[21/25][133/391] Loss_D: 0.4028 Loss_G: 3.0201 \n",
            "[21/25][134/391] Loss_D: 0.4156 Loss_G: 2.3733 \n",
            "[21/25][135/391] Loss_D: 0.3090 Loss_G: 2.4904 \n",
            "[21/25][136/391] Loss_D: 0.4087 Loss_G: 2.8545 \n",
            "[21/25][137/391] Loss_D: 0.2307 Loss_G: 3.5250 \n",
            "[21/25][138/391] Loss_D: 0.6543 Loss_G: 1.2896 \n",
            "[21/25][139/391] Loss_D: 0.7103 Loss_G: 3.5743 \n",
            "[21/25][140/391] Loss_D: 0.5231 Loss_G: 2.3273 \n",
            "[21/25][141/391] Loss_D: 0.3005 Loss_G: 2.9335 \n",
            "[21/25][142/391] Loss_D: 0.3681 Loss_G: 2.8553 \n",
            "[21/25][143/391] Loss_D: 0.3448 Loss_G: 2.2635 \n",
            "[21/25][144/391] Loss_D: 0.4117 Loss_G: 3.3363 \n",
            "[21/25][145/391] Loss_D: 0.5040 Loss_G: 1.9756 \n",
            "[21/25][146/391] Loss_D: 0.4421 Loss_G: 2.8512 \n",
            "[21/25][147/391] Loss_D: 0.4236 Loss_G: 2.5024 \n",
            "[21/25][148/391] Loss_D: 0.4741 Loss_G: 3.0087 \n",
            "[21/25][149/391] Loss_D: 0.3711 Loss_G: 2.6383 \n",
            "[21/25][150/391] Loss_D: 0.4681 Loss_G: 1.6919 \n",
            "[21/25][151/391] Loss_D: 0.5078 Loss_G: 3.7149 \n",
            "[21/25][152/391] Loss_D: 0.5465 Loss_G: 1.7978 \n",
            "[21/25][153/391] Loss_D: 0.4543 Loss_G: 2.8133 \n",
            "[21/25][154/391] Loss_D: 0.3403 Loss_G: 2.8698 \n",
            "[21/25][155/391] Loss_D: 0.3517 Loss_G: 2.6502 \n",
            "[21/25][156/391] Loss_D: 0.4871 Loss_G: 1.8224 \n",
            "[21/25][157/391] Loss_D: 0.4896 Loss_G: 3.4375 \n",
            "[21/25][158/391] Loss_D: 0.3930 Loss_G: 2.5582 \n",
            "[21/25][159/391] Loss_D: 0.3173 Loss_G: 3.1816 \n",
            "[21/25][160/391] Loss_D: 0.7081 Loss_G: 0.9673 \n",
            "[21/25][161/391] Loss_D: 0.9119 Loss_G: 4.7843 \n",
            "[21/25][162/391] Loss_D: 0.9539 Loss_G: 0.7079 \n",
            "[21/25][163/391] Loss_D: 1.2300 Loss_G: 4.8928 \n",
            "[21/25][164/391] Loss_D: 0.6322 Loss_G: 0.6106 \n",
            "[21/25][165/391] Loss_D: 1.4978 Loss_G: 7.5722 \n",
            "[21/25][166/391] Loss_D: 3.0198 Loss_G: 0.4281 \n",
            "[21/25][167/391] Loss_D: 1.4466 Loss_G: 5.4935 \n",
            "[21/25][168/391] Loss_D: 1.8951 Loss_G: 0.3608 \n",
            "[21/25][169/391] Loss_D: 1.7284 Loss_G: 5.1149 \n",
            "[21/25][170/391] Loss_D: 0.9489 Loss_G: 1.5906 \n",
            "[21/25][171/391] Loss_D: 0.7170 Loss_G: 2.8734 \n",
            "[21/25][172/391] Loss_D: 0.7243 Loss_G: 2.2224 \n",
            "[21/25][173/391] Loss_D: 0.7668 Loss_G: 1.5637 \n",
            "[21/25][174/391] Loss_D: 0.8576 Loss_G: 3.7111 \n",
            "[21/25][175/391] Loss_D: 0.8535 Loss_G: 1.6969 \n",
            "[21/25][176/391] Loss_D: 0.7163 Loss_G: 1.6772 \n",
            "[21/25][177/391] Loss_D: 0.6967 Loss_G: 4.2017 \n",
            "[21/25][178/391] Loss_D: 1.0189 Loss_G: 1.1825 \n",
            "[21/25][179/391] Loss_D: 0.6679 Loss_G: 3.6504 \n",
            "[21/25][180/391] Loss_D: 0.4271 Loss_G: 3.5652 \n",
            "[21/25][181/391] Loss_D: 0.6672 Loss_G: 1.4725 \n",
            "[21/25][182/391] Loss_D: 0.7256 Loss_G: 3.7780 \n",
            "[21/25][183/391] Loss_D: 0.6358 Loss_G: 2.0358 \n",
            "[21/25][184/391] Loss_D: 0.4309 Loss_G: 3.0075 \n",
            "[21/25][185/391] Loss_D: 0.4261 Loss_G: 2.4375 \n",
            "[21/25][186/391] Loss_D: 0.8384 Loss_G: 4.7757 \n",
            "[21/25][187/391] Loss_D: 0.8470 Loss_G: 1.0146 \n",
            "[21/25][188/391] Loss_D: 0.9191 Loss_G: 4.2268 \n",
            "[21/25][189/391] Loss_D: 0.6316 Loss_G: 2.0461 \n",
            "[21/25][190/391] Loss_D: 0.4155 Loss_G: 2.7510 \n",
            "[21/25][191/391] Loss_D: 0.5217 Loss_G: 3.7822 \n",
            "[21/25][192/391] Loss_D: 0.4493 Loss_G: 2.5627 \n",
            "[21/25][193/391] Loss_D: 0.4742 Loss_G: 3.1282 \n",
            "[21/25][194/391] Loss_D: 0.9906 Loss_G: 0.7614 \n",
            "[21/25][195/391] Loss_D: 1.4760 Loss_G: 5.1304 \n",
            "[21/25][196/391] Loss_D: 1.0261 Loss_G: 1.9421 \n",
            "[21/25][197/391] Loss_D: 0.4141 Loss_G: 2.7064 \n",
            "[21/25][198/391] Loss_D: 0.4729 Loss_G: 3.5667 \n",
            "[21/25][199/391] Loss_D: 0.7738 Loss_G: 1.0475 \n",
            "[21/25][200/391] Loss_D: 1.0429 Loss_G: 4.9026 \n",
            "saving the output\n",
            "[21/25][201/391] Loss_D: 0.8775 Loss_G: 1.7650 \n",
            "[21/25][202/391] Loss_D: 0.6805 Loss_G: 2.8991 \n",
            "[21/25][203/391] Loss_D: 0.6692 Loss_G: 2.0146 \n",
            "[21/25][204/391] Loss_D: 0.7185 Loss_G: 3.1467 \n",
            "[21/25][205/391] Loss_D: 0.4339 Loss_G: 2.2903 \n",
            "[21/25][206/391] Loss_D: 0.5845 Loss_G: 2.9664 \n",
            "[21/25][207/391] Loss_D: 0.3532 Loss_G: 3.4851 \n",
            "[21/25][208/391] Loss_D: 0.6776 Loss_G: 1.4576 \n",
            "[21/25][209/391] Loss_D: 0.7733 Loss_G: 3.7678 \n",
            "[21/25][210/391] Loss_D: 0.8123 Loss_G: 1.6079 \n",
            "[21/25][211/391] Loss_D: 0.5939 Loss_G: 3.4881 \n",
            "[21/25][212/391] Loss_D: 0.4129 Loss_G: 3.5377 \n",
            "[21/25][213/391] Loss_D: 0.5590 Loss_G: 2.0226 \n",
            "[21/25][214/391] Loss_D: 0.6191 Loss_G: 3.2704 \n",
            "[21/25][215/391] Loss_D: 0.3853 Loss_G: 2.9204 \n",
            "[21/25][216/391] Loss_D: 0.3899 Loss_G: 2.8435 \n",
            "[21/25][217/391] Loss_D: 0.5082 Loss_G: 2.3130 \n",
            "[21/25][218/391] Loss_D: 0.3340 Loss_G: 3.2053 \n",
            "[21/25][219/391] Loss_D: 0.3369 Loss_G: 3.3765 \n",
            "[21/25][220/391] Loss_D: 0.6288 Loss_G: 1.3659 \n",
            "[21/25][221/391] Loss_D: 0.8656 Loss_G: 4.1587 \n",
            "[21/25][222/391] Loss_D: 0.8527 Loss_G: 1.6743 \n",
            "[21/25][223/391] Loss_D: 0.6583 Loss_G: 3.9538 \n",
            "[21/25][224/391] Loss_D: 0.5234 Loss_G: 2.2285 \n",
            "[21/25][225/391] Loss_D: 0.6271 Loss_G: 1.8914 \n",
            "[21/25][226/391] Loss_D: 0.6057 Loss_G: 3.6271 \n",
            "[21/25][227/391] Loss_D: 0.6619 Loss_G: 1.8592 \n",
            "[21/25][228/391] Loss_D: 0.4568 Loss_G: 2.2883 \n",
            "[21/25][229/391] Loss_D: 0.5026 Loss_G: 4.3494 \n",
            "[21/25][230/391] Loss_D: 0.5933 Loss_G: 2.1603 \n",
            "[21/25][231/391] Loss_D: 0.4636 Loss_G: 1.8595 \n",
            "[21/25][232/391] Loss_D: 0.7309 Loss_G: 4.7466 \n",
            "[21/25][233/391] Loss_D: 0.9156 Loss_G: 1.3783 \n",
            "[21/25][234/391] Loss_D: 0.5808 Loss_G: 3.4511 \n",
            "[21/25][235/391] Loss_D: 0.3903 Loss_G: 3.3258 \n",
            "[21/25][236/391] Loss_D: 0.3608 Loss_G: 2.2371 \n",
            "[21/25][237/391] Loss_D: 0.4571 Loss_G: 2.9299 \n",
            "[21/25][238/391] Loss_D: 0.5126 Loss_G: 2.9541 \n",
            "[21/25][239/391] Loss_D: 0.5498 Loss_G: 1.7129 \n",
            "[21/25][240/391] Loss_D: 0.6925 Loss_G: 3.6580 \n",
            "[21/25][241/391] Loss_D: 0.5343 Loss_G: 2.6744 \n",
            "[21/25][242/391] Loss_D: 0.3766 Loss_G: 2.3484 \n",
            "[21/25][243/391] Loss_D: 0.3772 Loss_G: 2.9558 \n",
            "[21/25][244/391] Loss_D: 0.4376 Loss_G: 2.5056 \n",
            "[21/25][245/391] Loss_D: 0.4489 Loss_G: 2.3235 \n",
            "[21/25][246/391] Loss_D: 0.3661 Loss_G: 3.0965 \n",
            "[21/25][247/391] Loss_D: 0.7244 Loss_G: 1.4717 \n",
            "[21/25][248/391] Loss_D: 0.5846 Loss_G: 3.4595 \n",
            "[21/25][249/391] Loss_D: 0.4788 Loss_G: 2.3808 \n",
            "[21/25][250/391] Loss_D: 0.3658 Loss_G: 2.5715 \n",
            "[21/25][251/391] Loss_D: 0.3729 Loss_G: 2.5588 \n",
            "[21/25][252/391] Loss_D: 0.4597 Loss_G: 2.8183 \n",
            "[21/25][253/391] Loss_D: 0.3467 Loss_G: 2.6790 \n",
            "[21/25][254/391] Loss_D: 0.3345 Loss_G: 3.1569 \n",
            "[21/25][255/391] Loss_D: 0.4699 Loss_G: 1.9774 \n",
            "[21/25][256/391] Loss_D: 0.3935 Loss_G: 2.8725 \n",
            "[21/25][257/391] Loss_D: 0.3509 Loss_G: 2.8667 \n",
            "[21/25][258/391] Loss_D: 0.5093 Loss_G: 1.7633 \n",
            "[21/25][259/391] Loss_D: 0.4978 Loss_G: 2.4222 \n",
            "[21/25][260/391] Loss_D: 0.4344 Loss_G: 3.1244 \n",
            "[21/25][261/391] Loss_D: 0.5042 Loss_G: 1.6594 \n",
            "[21/25][262/391] Loss_D: 0.4067 Loss_G: 3.0630 \n",
            "[21/25][263/391] Loss_D: 0.4341 Loss_G: 2.6579 \n",
            "[21/25][264/391] Loss_D: 0.4501 Loss_G: 1.8147 \n",
            "[21/25][265/391] Loss_D: 0.6095 Loss_G: 4.0678 \n",
            "[21/25][266/391] Loss_D: 0.4251 Loss_G: 2.6016 \n",
            "[21/25][267/391] Loss_D: 0.3205 Loss_G: 2.7826 \n",
            "[21/25][268/391] Loss_D: 0.3792 Loss_G: 2.9321 \n",
            "[21/25][269/391] Loss_D: 0.5930 Loss_G: 1.3747 \n",
            "[21/25][270/391] Loss_D: 0.6799 Loss_G: 3.7500 \n",
            "[21/25][271/391] Loss_D: 0.6132 Loss_G: 1.7875 \n",
            "[21/25][272/391] Loss_D: 0.5314 Loss_G: 2.9605 \n",
            "[21/25][273/391] Loss_D: 0.2943 Loss_G: 2.9187 \n",
            "[21/25][274/391] Loss_D: 0.3617 Loss_G: 2.6550 \n",
            "[21/25][275/391] Loss_D: 0.3662 Loss_G: 2.2553 \n",
            "[21/25][276/391] Loss_D: 0.3508 Loss_G: 3.2302 \n",
            "[21/25][277/391] Loss_D: 0.3662 Loss_G: 3.0137 \n",
            "[21/25][278/391] Loss_D: 0.5678 Loss_G: 1.3608 \n",
            "[21/25][279/391] Loss_D: 0.8394 Loss_G: 4.4212 \n",
            "[21/25][280/391] Loss_D: 1.1572 Loss_G: 1.1742 \n",
            "[21/25][281/391] Loss_D: 0.5609 Loss_G: 3.4652 \n",
            "[21/25][282/391] Loss_D: 0.4883 Loss_G: 2.7868 \n",
            "[21/25][283/391] Loss_D: 0.3455 Loss_G: 2.8266 \n",
            "[21/25][284/391] Loss_D: 0.3771 Loss_G: 2.2668 \n",
            "[21/25][285/391] Loss_D: 0.6297 Loss_G: 3.4523 \n",
            "[21/25][286/391] Loss_D: 0.3098 Loss_G: 3.0715 \n",
            "[21/25][287/391] Loss_D: 0.5664 Loss_G: 1.3411 \n",
            "[21/25][288/391] Loss_D: 0.5660 Loss_G: 3.8786 \n",
            "[21/25][289/391] Loss_D: 0.3941 Loss_G: 2.7337 \n",
            "[21/25][290/391] Loss_D: 0.4663 Loss_G: 2.4234 \n",
            "[21/25][291/391] Loss_D: 0.3702 Loss_G: 3.3716 \n",
            "[21/25][292/391] Loss_D: 0.3162 Loss_G: 2.5647 \n",
            "[21/25][293/391] Loss_D: 0.4553 Loss_G: 2.1624 \n",
            "[21/25][294/391] Loss_D: 0.4266 Loss_G: 3.8297 \n",
            "[21/25][295/391] Loss_D: 0.5796 Loss_G: 1.7251 \n",
            "[21/25][296/391] Loss_D: 0.5780 Loss_G: 2.8922 \n",
            "[21/25][297/391] Loss_D: 0.4980 Loss_G: 2.0215 \n",
            "[21/25][298/391] Loss_D: 0.3573 Loss_G: 3.0072 \n",
            "[21/25][299/391] Loss_D: 0.3685 Loss_G: 3.5329 \n",
            "[21/25][300/391] Loss_D: 0.5118 Loss_G: 1.6140 \n",
            "saving the output\n",
            "[21/25][301/391] Loss_D: 0.4146 Loss_G: 2.8343 \n",
            "[21/25][302/391] Loss_D: 0.4916 Loss_G: 2.1974 \n",
            "[21/25][303/391] Loss_D: 0.5145 Loss_G: 2.7499 \n",
            "[21/25][304/391] Loss_D: 0.3539 Loss_G: 2.6956 \n",
            "[21/25][305/391] Loss_D: 0.4322 Loss_G: 2.1059 \n",
            "[21/25][306/391] Loss_D: 0.4385 Loss_G: 3.2342 \n",
            "[21/25][307/391] Loss_D: 0.6814 Loss_G: 1.5616 \n",
            "[21/25][308/391] Loss_D: 0.3779 Loss_G: 3.7605 \n",
            "[21/25][309/391] Loss_D: 0.5199 Loss_G: 2.0498 \n",
            "[21/25][310/391] Loss_D: 0.4203 Loss_G: 2.5428 \n",
            "[21/25][311/391] Loss_D: 0.3724 Loss_G: 3.0772 \n",
            "[21/25][312/391] Loss_D: 0.3097 Loss_G: 2.7634 \n",
            "[21/25][313/391] Loss_D: 0.4224 Loss_G: 1.7610 \n",
            "[21/25][314/391] Loss_D: 0.6375 Loss_G: 3.7083 \n",
            "[21/25][315/391] Loss_D: 0.6939 Loss_G: 1.3785 \n",
            "[21/25][316/391] Loss_D: 0.5704 Loss_G: 3.0760 \n",
            "[21/25][317/391] Loss_D: 0.2750 Loss_G: 3.3082 \n",
            "[21/25][318/391] Loss_D: 0.4478 Loss_G: 1.8168 \n",
            "[21/25][319/391] Loss_D: 0.6344 Loss_G: 3.2504 \n",
            "[21/25][320/391] Loss_D: 0.4614 Loss_G: 2.2455 \n",
            "[21/25][321/391] Loss_D: 0.4319 Loss_G: 2.8900 \n",
            "[21/25][322/391] Loss_D: 0.3403 Loss_G: 2.6350 \n",
            "[21/25][323/391] Loss_D: 0.4299 Loss_G: 2.6030 \n",
            "[21/25][324/391] Loss_D: 0.4042 Loss_G: 2.7298 \n",
            "[21/25][325/391] Loss_D: 0.4232 Loss_G: 2.2442 \n",
            "[21/25][326/391] Loss_D: 0.5093 Loss_G: 2.0383 \n",
            "[21/25][327/391] Loss_D: 0.3876 Loss_G: 3.3836 \n",
            "[21/25][328/391] Loss_D: 0.2310 Loss_G: 3.4351 \n",
            "[21/25][329/391] Loss_D: 0.5174 Loss_G: 1.4323 \n",
            "[21/25][330/391] Loss_D: 0.7111 Loss_G: 3.9126 \n",
            "[21/25][331/391] Loss_D: 0.6943 Loss_G: 1.7417 \n",
            "[21/25][332/391] Loss_D: 0.4100 Loss_G: 2.8304 \n",
            "[21/25][333/391] Loss_D: 0.4132 Loss_G: 2.7395 \n",
            "[21/25][334/391] Loss_D: 0.3077 Loss_G: 3.2719 \n",
            "[21/25][335/391] Loss_D: 0.5427 Loss_G: 1.5537 \n",
            "[21/25][336/391] Loss_D: 0.6232 Loss_G: 2.8379 \n",
            "[21/25][337/391] Loss_D: 0.4024 Loss_G: 2.5191 \n",
            "[21/25][338/391] Loss_D: 0.4002 Loss_G: 3.0139 \n",
            "[21/25][339/391] Loss_D: 0.4031 Loss_G: 2.4230 \n",
            "[21/25][340/391] Loss_D: 0.3866 Loss_G: 3.2395 \n",
            "[21/25][341/391] Loss_D: 0.2903 Loss_G: 2.9007 \n",
            "[21/25][342/391] Loss_D: 0.3326 Loss_G: 2.1596 \n",
            "[21/25][343/391] Loss_D: 0.4924 Loss_G: 3.1684 \n",
            "[21/25][344/391] Loss_D: 0.4021 Loss_G: 2.8813 \n",
            "[21/25][345/391] Loss_D: 0.3718 Loss_G: 2.1862 \n",
            "[21/25][346/391] Loss_D: 0.4388 Loss_G: 3.9695 \n",
            "[21/25][347/391] Loss_D: 0.7129 Loss_G: 0.5970 \n",
            "[21/25][348/391] Loss_D: 1.1194 Loss_G: 3.3577 \n",
            "[21/25][349/391] Loss_D: 0.5532 Loss_G: 0.9206 \n",
            "[21/25][350/391] Loss_D: 1.1476 Loss_G: 6.0485 \n",
            "[21/25][351/391] Loss_D: 2.9714 Loss_G: 0.3395 \n",
            "[21/25][352/391] Loss_D: 1.9029 Loss_G: 7.1914 \n",
            "[21/25][353/391] Loss_D: 2.6733 Loss_G: 0.2046 \n",
            "[21/25][354/391] Loss_D: 2.7118 Loss_G: 5.4566 \n",
            "[21/25][355/391] Loss_D: 1.9182 Loss_G: 0.1779 \n",
            "[21/25][356/391] Loss_D: 3.1638 Loss_G: 6.2350 \n",
            "[21/25][357/391] Loss_D: 1.8880 Loss_G: 0.6731 \n",
            "[21/25][358/391] Loss_D: 2.0141 Loss_G: 4.5487 \n",
            "[21/25][359/391] Loss_D: 1.2434 Loss_G: 0.8882 \n",
            "[21/25][360/391] Loss_D: 1.4066 Loss_G: 4.8631 \n",
            "[21/25][361/391] Loss_D: 1.4933 Loss_G: 1.2009 \n",
            "[21/25][362/391] Loss_D: 1.4592 Loss_G: 3.2552 \n",
            "[21/25][363/391] Loss_D: 0.8271 Loss_G: 2.3281 \n",
            "[21/25][364/391] Loss_D: 0.5466 Loss_G: 2.9737 \n",
            "[21/25][365/391] Loss_D: 0.5431 Loss_G: 2.6701 \n",
            "[21/25][366/391] Loss_D: 0.6052 Loss_G: 2.7992 \n",
            "[21/25][367/391] Loss_D: 0.7996 Loss_G: 2.1119 \n",
            "[21/25][368/391] Loss_D: 0.8192 Loss_G: 4.3402 \n",
            "[21/25][369/391] Loss_D: 0.9560 Loss_G: 1.3983 \n",
            "[21/25][370/391] Loss_D: 0.7392 Loss_G: 2.8504 \n",
            "[21/25][371/391] Loss_D: 0.7180 Loss_G: 3.9650 \n",
            "[21/25][372/391] Loss_D: 0.8792 Loss_G: 1.2508 \n",
            "[21/25][373/391] Loss_D: 0.7004 Loss_G: 4.1006 \n",
            "[21/25][374/391] Loss_D: 0.5205 Loss_G: 2.2668 \n",
            "[21/25][375/391] Loss_D: 0.5287 Loss_G: 2.5640 \n",
            "[21/25][376/391] Loss_D: 0.5587 Loss_G: 3.0863 \n",
            "[21/25][377/391] Loss_D: 0.6528 Loss_G: 1.7871 \n",
            "[21/25][378/391] Loss_D: 0.5192 Loss_G: 3.4938 \n",
            "[21/25][379/391] Loss_D: 0.6392 Loss_G: 1.5886 \n",
            "[21/25][380/391] Loss_D: 0.5901 Loss_G: 3.1459 \n",
            "[21/25][381/391] Loss_D: 0.4754 Loss_G: 2.5211 \n",
            "[21/25][382/391] Loss_D: 0.5193 Loss_G: 2.3952 \n",
            "[21/25][383/391] Loss_D: 0.4039 Loss_G: 2.5845 \n",
            "[21/25][384/391] Loss_D: 0.6115 Loss_G: 2.7526 \n",
            "[21/25][385/391] Loss_D: 0.3830 Loss_G: 2.8727 \n",
            "[21/25][386/391] Loss_D: 0.4364 Loss_G: 2.2715 \n",
            "[21/25][387/391] Loss_D: 0.4981 Loss_G: 2.4390 \n",
            "[21/25][388/391] Loss_D: 0.4791 Loss_G: 2.4744 \n",
            "[21/25][389/391] Loss_D: 0.4811 Loss_G: 2.3770 \n",
            "[21/25][390/391] Loss_D: 0.4700 Loss_G: 3.8331 \n",
            "[22/25][0/391] Loss_D: 0.5141 Loss_G: 2.1322 \n",
            "saving the output\n",
            "[22/25][1/391] Loss_D: 0.3914 Loss_G: 2.2126 \n",
            "[22/25][2/391] Loss_D: 0.5610 Loss_G: 4.1094 \n",
            "[22/25][3/391] Loss_D: 0.6740 Loss_G: 2.0263 \n",
            "[22/25][4/391] Loss_D: 0.4287 Loss_G: 2.4594 \n",
            "[22/25][5/391] Loss_D: 0.5164 Loss_G: 3.6100 \n",
            "[22/25][6/391] Loss_D: 0.6738 Loss_G: 1.8495 \n",
            "[22/25][7/391] Loss_D: 0.3985 Loss_G: 2.8135 \n",
            "[22/25][8/391] Loss_D: 0.4244 Loss_G: 3.7317 \n",
            "[22/25][9/391] Loss_D: 0.4171 Loss_G: 2.7081 \n",
            "[22/25][10/391] Loss_D: 0.4788 Loss_G: 1.4931 \n",
            "[22/25][11/391] Loss_D: 0.6481 Loss_G: 3.8620 \n",
            "[22/25][12/391] Loss_D: 0.4245 Loss_G: 2.0087 \n",
            "[22/25][13/391] Loss_D: 0.5250 Loss_G: 2.9314 \n",
            "[22/25][14/391] Loss_D: 0.7064 Loss_G: 1.3658 \n",
            "[22/25][15/391] Loss_D: 0.7220 Loss_G: 4.0460 \n",
            "[22/25][16/391] Loss_D: 0.9206 Loss_G: 1.2423 \n",
            "[22/25][17/391] Loss_D: 1.0074 Loss_G: 4.1436 \n",
            "[22/25][18/391] Loss_D: 0.7710 Loss_G: 2.0325 \n",
            "[22/25][19/391] Loss_D: 0.5721 Loss_G: 3.1577 \n",
            "[22/25][20/391] Loss_D: 0.4435 Loss_G: 2.6408 \n",
            "[22/25][21/391] Loss_D: 0.4005 Loss_G: 2.7238 \n",
            "[22/25][22/391] Loss_D: 0.4782 Loss_G: 2.8368 \n",
            "[22/25][23/391] Loss_D: 0.4918 Loss_G: 2.2296 \n",
            "[22/25][24/391] Loss_D: 0.5765 Loss_G: 2.3367 \n",
            "[22/25][25/391] Loss_D: 0.3821 Loss_G: 2.6861 \n",
            "[22/25][26/391] Loss_D: 0.3858 Loss_G: 3.0647 \n",
            "[22/25][27/391] Loss_D: 0.4623 Loss_G: 1.9167 \n",
            "[22/25][28/391] Loss_D: 0.4278 Loss_G: 3.1107 \n",
            "[22/25][29/391] Loss_D: 0.3974 Loss_G: 2.6855 \n",
            "[22/25][30/391] Loss_D: 0.2985 Loss_G: 3.0072 \n",
            "[22/25][31/391] Loss_D: 0.4994 Loss_G: 1.9867 \n",
            "[22/25][32/391] Loss_D: 0.4947 Loss_G: 3.4286 \n",
            "[22/25][33/391] Loss_D: 0.7540 Loss_G: 1.3725 \n",
            "[22/25][34/391] Loss_D: 0.6664 Loss_G: 3.0345 \n",
            "[22/25][35/391] Loss_D: 0.5057 Loss_G: 2.1194 \n",
            "[22/25][36/391] Loss_D: 0.4339 Loss_G: 3.0381 \n",
            "[22/25][37/391] Loss_D: 0.3508 Loss_G: 2.7756 \n",
            "[22/25][38/391] Loss_D: 0.3336 Loss_G: 2.2888 \n",
            "[22/25][39/391] Loss_D: 0.4027 Loss_G: 2.6833 \n",
            "[22/25][40/391] Loss_D: 0.3789 Loss_G: 2.7908 \n",
            "[22/25][41/391] Loss_D: 0.4273 Loss_G: 2.1847 \n",
            "[22/25][42/391] Loss_D: 0.3943 Loss_G: 2.6305 \n",
            "[22/25][43/391] Loss_D: 0.4882 Loss_G: 2.1421 \n",
            "[22/25][44/391] Loss_D: 0.3655 Loss_G: 2.9478 \n",
            "[22/25][45/391] Loss_D: 0.3483 Loss_G: 3.1211 \n",
            "[22/25][46/391] Loss_D: 0.4862 Loss_G: 1.7458 \n",
            "[22/25][47/391] Loss_D: 0.4039 Loss_G: 3.2128 \n",
            "[22/25][48/391] Loss_D: 0.4352 Loss_G: 2.4829 \n",
            "[22/25][49/391] Loss_D: 0.4527 Loss_G: 1.8164 \n",
            "[22/25][50/391] Loss_D: 0.3760 Loss_G: 2.9221 \n",
            "[22/25][51/391] Loss_D: 0.3043 Loss_G: 3.0240 \n",
            "[22/25][52/391] Loss_D: 0.3346 Loss_G: 2.6944 \n",
            "[22/25][53/391] Loss_D: 0.5994 Loss_G: 1.2106 \n",
            "[22/25][54/391] Loss_D: 0.8143 Loss_G: 4.6870 \n",
            "[22/25][55/391] Loss_D: 0.7083 Loss_G: 1.2635 \n",
            "[22/25][56/391] Loss_D: 0.7359 Loss_G: 4.2859 \n",
            "[22/25][57/391] Loss_D: 0.5873 Loss_G: 1.9155 \n",
            "[22/25][58/391] Loss_D: 0.3667 Loss_G: 2.5727 \n",
            "[22/25][59/391] Loss_D: 0.4527 Loss_G: 3.0677 \n",
            "[22/25][60/391] Loss_D: 0.5153 Loss_G: 1.6907 \n",
            "[22/25][61/391] Loss_D: 0.6625 Loss_G: 4.6867 \n",
            "[22/25][62/391] Loss_D: 0.7320 Loss_G: 1.8249 \n",
            "[22/25][63/391] Loss_D: 0.4666 Loss_G: 3.2336 \n",
            "[22/25][64/391] Loss_D: 0.3988 Loss_G: 2.8607 \n",
            "[22/25][65/391] Loss_D: 0.4125 Loss_G: 1.9267 \n",
            "[22/25][66/391] Loss_D: 0.5961 Loss_G: 3.1358 \n",
            "[22/25][67/391] Loss_D: 0.5218 Loss_G: 2.1979 \n",
            "[22/25][68/391] Loss_D: 0.4173 Loss_G: 2.2612 \n",
            "[22/25][69/391] Loss_D: 0.3595 Loss_G: 3.2341 \n",
            "[22/25][70/391] Loss_D: 0.4042 Loss_G: 2.6264 \n",
            "[22/25][71/391] Loss_D: 0.3286 Loss_G: 2.4179 \n",
            "[22/25][72/391] Loss_D: 0.3735 Loss_G: 2.7723 \n",
            "[22/25][73/391] Loss_D: 0.3951 Loss_G: 2.5313 \n",
            "[22/25][74/391] Loss_D: 0.4996 Loss_G: 2.3001 \n",
            "[22/25][75/391] Loss_D: 0.4647 Loss_G: 2.5328 \n",
            "[22/25][76/391] Loss_D: 0.3656 Loss_G: 3.0744 \n",
            "[22/25][77/391] Loss_D: 0.3632 Loss_G: 3.1557 \n",
            "[22/25][78/391] Loss_D: 0.4504 Loss_G: 1.8161 \n",
            "[22/25][79/391] Loss_D: 0.5574 Loss_G: 3.6860 \n",
            "[22/25][80/391] Loss_D: 0.7098 Loss_G: 1.1607 \n",
            "[22/25][81/391] Loss_D: 0.7340 Loss_G: 4.3872 \n",
            "[22/25][82/391] Loss_D: 0.6444 Loss_G: 1.7150 \n",
            "[22/25][83/391] Loss_D: 0.4713 Loss_G: 3.0811 \n",
            "[22/25][84/391] Loss_D: 0.3977 Loss_G: 2.9168 \n",
            "[22/25][85/391] Loss_D: 0.5803 Loss_G: 1.4355 \n",
            "[22/25][86/391] Loss_D: 0.7567 Loss_G: 3.6430 \n",
            "[22/25][87/391] Loss_D: 0.4510 Loss_G: 2.5849 \n",
            "[22/25][88/391] Loss_D: 0.3553 Loss_G: 1.9605 \n",
            "[22/25][89/391] Loss_D: 0.5155 Loss_G: 4.0244 \n",
            "[22/25][90/391] Loss_D: 0.4678 Loss_G: 2.3382 \n",
            "[22/25][91/391] Loss_D: 0.3632 Loss_G: 2.3205 \n",
            "[22/25][92/391] Loss_D: 0.4781 Loss_G: 3.4800 \n",
            "[22/25][93/391] Loss_D: 0.4362 Loss_G: 2.0461 \n",
            "[22/25][94/391] Loss_D: 0.4129 Loss_G: 2.9560 \n",
            "[22/25][95/391] Loss_D: 0.3460 Loss_G: 3.1318 \n",
            "[22/25][96/391] Loss_D: 0.4256 Loss_G: 2.1277 \n",
            "[22/25][97/391] Loss_D: 0.4423 Loss_G: 3.7134 \n",
            "[22/25][98/391] Loss_D: 0.3010 Loss_G: 3.1951 \n",
            "[22/25][99/391] Loss_D: 0.5421 Loss_G: 1.2598 \n",
            "[22/25][100/391] Loss_D: 0.8850 Loss_G: 4.9344 \n",
            "saving the output\n",
            "[22/25][101/391] Loss_D: 0.7938 Loss_G: 1.1407 \n",
            "[22/25][102/391] Loss_D: 0.7205 Loss_G: 4.6603 \n",
            "[22/25][103/391] Loss_D: 0.7327 Loss_G: 1.4106 \n",
            "[22/25][104/391] Loss_D: 0.5502 Loss_G: 3.5284 \n",
            "[22/25][105/391] Loss_D: 0.4414 Loss_G: 2.6433 \n",
            "[22/25][106/391] Loss_D: 0.3688 Loss_G: 2.1863 \n",
            "[22/25][107/391] Loss_D: 0.5159 Loss_G: 3.5522 \n",
            "[22/25][108/391] Loss_D: 0.5728 Loss_G: 1.6584 \n",
            "[22/25][109/391] Loss_D: 0.4616 Loss_G: 2.6762 \n",
            "[22/25][110/391] Loss_D: 0.3297 Loss_G: 3.0818 \n",
            "[22/25][111/391] Loss_D: 0.4563 Loss_G: 2.0203 \n",
            "[22/25][112/391] Loss_D: 0.4863 Loss_G: 3.2608 \n",
            "[22/25][113/391] Loss_D: 0.4755 Loss_G: 2.4006 \n",
            "[22/25][114/391] Loss_D: 0.4185 Loss_G: 2.0373 \n",
            "[22/25][115/391] Loss_D: 0.4962 Loss_G: 3.2598 \n",
            "[22/25][116/391] Loss_D: 0.6054 Loss_G: 1.8030 \n",
            "[22/25][117/391] Loss_D: 0.4423 Loss_G: 2.7784 \n",
            "[22/25][118/391] Loss_D: 0.4245 Loss_G: 3.5915 \n",
            "[22/25][119/391] Loss_D: 0.6742 Loss_G: 0.9070 \n",
            "[22/25][120/391] Loss_D: 0.9109 Loss_G: 4.8168 \n",
            "[22/25][121/391] Loss_D: 0.7864 Loss_G: 1.1262 \n",
            "[22/25][122/391] Loss_D: 0.9106 Loss_G: 4.1321 \n",
            "[22/25][123/391] Loss_D: 0.6489 Loss_G: 2.1021 \n",
            "[22/25][124/391] Loss_D: 0.4559 Loss_G: 2.5998 \n",
            "[22/25][125/391] Loss_D: 0.4546 Loss_G: 2.8621 \n",
            "[22/25][126/391] Loss_D: 0.4908 Loss_G: 2.5098 \n",
            "[22/25][127/391] Loss_D: 0.4070 Loss_G: 2.7063 \n",
            "[22/25][128/391] Loss_D: 0.4121 Loss_G: 2.1107 \n",
            "[22/25][129/391] Loss_D: 0.4549 Loss_G: 3.1743 \n",
            "[22/25][130/391] Loss_D: 0.4808 Loss_G: 2.2433 \n",
            "[22/25][131/391] Loss_D: 0.4378 Loss_G: 2.1323 \n",
            "[22/25][132/391] Loss_D: 0.4927 Loss_G: 3.6019 \n",
            "[22/25][133/391] Loss_D: 0.3865 Loss_G: 2.5462 \n",
            "[22/25][134/391] Loss_D: 0.3746 Loss_G: 2.0783 \n",
            "[22/25][135/391] Loss_D: 0.5808 Loss_G: 3.1711 \n",
            "[22/25][136/391] Loss_D: 0.5047 Loss_G: 2.1453 \n",
            "[22/25][137/391] Loss_D: 0.3824 Loss_G: 2.5702 \n",
            "[22/25][138/391] Loss_D: 0.3618 Loss_G: 2.9429 \n",
            "[22/25][139/391] Loss_D: 0.4398 Loss_G: 1.9253 \n",
            "[22/25][140/391] Loss_D: 0.5345 Loss_G: 2.6900 \n",
            "[22/25][141/391] Loss_D: 0.3955 Loss_G: 3.1695 \n",
            "[22/25][142/391] Loss_D: 0.4917 Loss_G: 1.9610 \n",
            "[22/25][143/391] Loss_D: 0.4565 Loss_G: 3.1456 \n",
            "[22/25][144/391] Loss_D: 0.3975 Loss_G: 2.4346 \n",
            "[22/25][145/391] Loss_D: 0.4080 Loss_G: 2.4182 \n",
            "[22/25][146/391] Loss_D: 0.4451 Loss_G: 3.6175 \n",
            "[22/25][147/391] Loss_D: 0.6778 Loss_G: 1.1709 \n",
            "[22/25][148/391] Loss_D: 0.5146 Loss_G: 3.0683 \n",
            "[22/25][149/391] Loss_D: 0.3465 Loss_G: 3.3548 \n",
            "[22/25][150/391] Loss_D: 0.4590 Loss_G: 1.5490 \n",
            "[22/25][151/391] Loss_D: 0.6269 Loss_G: 3.9483 \n",
            "[22/25][152/391] Loss_D: 0.4485 Loss_G: 1.8742 \n",
            "[22/25][153/391] Loss_D: 0.7112 Loss_G: 4.9922 \n",
            "[22/25][154/391] Loss_D: 1.1666 Loss_G: 0.4965 \n",
            "[22/25][155/391] Loss_D: 1.0281 Loss_G: 5.0170 \n",
            "[22/25][156/391] Loss_D: 0.6390 Loss_G: 1.7280 \n",
            "[22/25][157/391] Loss_D: 0.5422 Loss_G: 2.5854 \n",
            "[22/25][158/391] Loss_D: 0.4495 Loss_G: 3.5095 \n",
            "[22/25][159/391] Loss_D: 0.4408 Loss_G: 1.8880 \n",
            "[22/25][160/391] Loss_D: 0.5156 Loss_G: 3.3878 \n",
            "[22/25][161/391] Loss_D: 0.5397 Loss_G: 1.5971 \n",
            "[22/25][162/391] Loss_D: 0.7925 Loss_G: 4.8940 \n",
            "[22/25][163/391] Loss_D: 0.7683 Loss_G: 0.5458 \n",
            "[22/25][164/391] Loss_D: 1.4507 Loss_G: 6.2671 \n",
            "[22/25][165/391] Loss_D: 1.4148 Loss_G: 0.0785 \n",
            "[22/25][166/391] Loss_D: 3.0694 Loss_G: 7.6001 \n",
            "[22/25][167/391] Loss_D: 4.5761 Loss_G: 0.0270 \n",
            "[22/25][168/391] Loss_D: 5.1527 Loss_G: 6.8732 \n",
            "[22/25][169/391] Loss_D: 4.0286 Loss_G: 0.3630 \n",
            "[22/25][170/391] Loss_D: 2.3129 Loss_G: 3.8786 \n",
            "[22/25][171/391] Loss_D: 1.5732 Loss_G: 1.0848 \n",
            "[22/25][172/391] Loss_D: 0.8405 Loss_G: 2.7325 \n",
            "[22/25][173/391] Loss_D: 0.8852 Loss_G: 2.1616 \n",
            "[22/25][174/391] Loss_D: 0.9208 Loss_G: 1.6820 \n",
            "[22/25][175/391] Loss_D: 0.8719 Loss_G: 2.3937 \n",
            "[22/25][176/391] Loss_D: 0.8420 Loss_G: 3.4126 \n",
            "[22/25][177/391] Loss_D: 0.7011 Loss_G: 1.7518 \n",
            "[22/25][178/391] Loss_D: 0.6765 Loss_G: 3.4434 \n",
            "[22/25][179/391] Loss_D: 0.8064 Loss_G: 1.0583 \n",
            "[22/25][180/391] Loss_D: 0.8973 Loss_G: 3.9859 \n",
            "[22/25][181/391] Loss_D: 0.9043 Loss_G: 1.0678 \n",
            "[22/25][182/391] Loss_D: 1.1479 Loss_G: 4.0452 \n",
            "[22/25][183/391] Loss_D: 0.9452 Loss_G: 1.6244 \n",
            "[22/25][184/391] Loss_D: 0.6072 Loss_G: 3.9679 \n",
            "[22/25][185/391] Loss_D: 0.9517 Loss_G: 1.3572 \n",
            "[22/25][186/391] Loss_D: 0.7268 Loss_G: 3.3660 \n",
            "[22/25][187/391] Loss_D: 0.4620 Loss_G: 3.0379 \n",
            "[22/25][188/391] Loss_D: 0.5507 Loss_G: 1.8291 \n",
            "[22/25][189/391] Loss_D: 0.7913 Loss_G: 3.4360 \n",
            "[22/25][190/391] Loss_D: 0.9619 Loss_G: 1.3063 \n",
            "[22/25][191/391] Loss_D: 0.5624 Loss_G: 3.1446 \n",
            "[22/25][192/391] Loss_D: 0.5082 Loss_G: 3.2877 \n",
            "[22/25][193/391] Loss_D: 0.4433 Loss_G: 2.3748 \n",
            "[22/25][194/391] Loss_D: 0.4388 Loss_G: 2.5120 \n",
            "[22/25][195/391] Loss_D: 0.5093 Loss_G: 3.1397 \n",
            "[22/25][196/391] Loss_D: 0.4272 Loss_G: 3.3845 \n",
            "[22/25][197/391] Loss_D: 0.8128 Loss_G: 1.1387 \n",
            "[22/25][198/391] Loss_D: 0.7817 Loss_G: 4.2167 \n",
            "[22/25][199/391] Loss_D: 0.4646 Loss_G: 2.9980 \n",
            "[22/25][200/391] Loss_D: 0.5319 Loss_G: 1.6191 \n",
            "saving the output\n",
            "[22/25][201/391] Loss_D: 0.5108 Loss_G: 4.0547 \n",
            "[22/25][202/391] Loss_D: 0.8107 Loss_G: 0.9900 \n",
            "[22/25][203/391] Loss_D: 1.1374 Loss_G: 4.6900 \n",
            "[22/25][204/391] Loss_D: 0.8203 Loss_G: 1.9067 \n",
            "[22/25][205/391] Loss_D: 0.6590 Loss_G: 3.0018 \n",
            "[22/25][206/391] Loss_D: 0.5270 Loss_G: 2.6655 \n",
            "[22/25][207/391] Loss_D: 0.4829 Loss_G: 1.9209 \n",
            "[22/25][208/391] Loss_D: 0.7400 Loss_G: 4.4374 \n",
            "[22/25][209/391] Loss_D: 0.8308 Loss_G: 1.6220 \n",
            "[22/25][210/391] Loss_D: 0.5748 Loss_G: 3.1240 \n",
            "[22/25][211/391] Loss_D: 0.3957 Loss_G: 3.1935 \n",
            "[22/25][212/391] Loss_D: 0.4641 Loss_G: 2.3919 \n",
            "[22/25][213/391] Loss_D: 0.5138 Loss_G: 2.2732 \n",
            "[22/25][214/391] Loss_D: 0.4781 Loss_G: 3.4248 \n",
            "[22/25][215/391] Loss_D: 0.6135 Loss_G: 1.7561 \n",
            "[22/25][216/391] Loss_D: 0.5553 Loss_G: 3.5987 \n",
            "[22/25][217/391] Loss_D: 0.6093 Loss_G: 2.2169 \n",
            "[22/25][218/391] Loss_D: 0.6470 Loss_G: 2.9981 \n",
            "[22/25][219/391] Loss_D: 0.3782 Loss_G: 3.0621 \n",
            "[22/25][220/391] Loss_D: 0.4514 Loss_G: 2.2389 \n",
            "[22/25][221/391] Loss_D: 0.5233 Loss_G: 2.4030 \n",
            "[22/25][222/391] Loss_D: 0.5116 Loss_G: 2.7624 \n",
            "[22/25][223/391] Loss_D: 0.5219 Loss_G: 2.8869 \n",
            "[22/25][224/391] Loss_D: 0.4433 Loss_G: 2.0802 \n",
            "[22/25][225/391] Loss_D: 0.5598 Loss_G: 3.3363 \n",
            "[22/25][226/391] Loss_D: 0.3039 Loss_G: 3.2894 \n",
            "[22/25][227/391] Loss_D: 0.5327 Loss_G: 1.7345 \n",
            "[22/25][228/391] Loss_D: 0.4538 Loss_G: 3.0662 \n",
            "[22/25][229/391] Loss_D: 0.4935 Loss_G: 2.9020 \n",
            "[22/25][230/391] Loss_D: 0.2770 Loss_G: 3.1217 \n",
            "[22/25][231/391] Loss_D: 0.5996 Loss_G: 1.5916 \n",
            "[22/25][232/391] Loss_D: 0.5079 Loss_G: 2.9633 \n",
            "[22/25][233/391] Loss_D: 0.4432 Loss_G: 2.6409 \n",
            "[22/25][234/391] Loss_D: 0.3935 Loss_G: 2.7786 \n",
            "[22/25][235/391] Loss_D: 0.3588 Loss_G: 3.1929 \n",
            "[22/25][236/391] Loss_D: 0.5479 Loss_G: 2.0225 \n",
            "[22/25][237/391] Loss_D: 0.5176 Loss_G: 2.6383 \n",
            "[22/25][238/391] Loss_D: 0.3657 Loss_G: 2.9310 \n",
            "[22/25][239/391] Loss_D: 0.4930 Loss_G: 2.8239 \n",
            "[22/25][240/391] Loss_D: 0.3198 Loss_G: 2.6899 \n",
            "[22/25][241/391] Loss_D: 0.3441 Loss_G: 2.6509 \n",
            "[22/25][242/391] Loss_D: 0.4853 Loss_G: 2.4289 \n",
            "[22/25][243/391] Loss_D: 0.5120 Loss_G: 2.6452 \n",
            "[22/25][244/391] Loss_D: 0.4627 Loss_G: 3.1950 \n",
            "[22/25][245/391] Loss_D: 0.4438 Loss_G: 2.2941 \n",
            "[22/25][246/391] Loss_D: 0.4421 Loss_G: 2.5109 \n",
            "[22/25][247/391] Loss_D: 0.5200 Loss_G: 1.9233 \n",
            "[22/25][248/391] Loss_D: 0.4981 Loss_G: 3.5086 \n",
            "[22/25][249/391] Loss_D: 0.4356 Loss_G: 2.6812 \n",
            "[22/25][250/391] Loss_D: 0.3422 Loss_G: 2.6837 \n",
            "[22/25][251/391] Loss_D: 0.3445 Loss_G: 2.7700 \n",
            "[22/25][252/391] Loss_D: 0.4524 Loss_G: 2.4394 \n",
            "[22/25][253/391] Loss_D: 0.5042 Loss_G: 2.6872 \n",
            "[22/25][254/391] Loss_D: 0.5265 Loss_G: 2.4387 \n",
            "[22/25][255/391] Loss_D: 0.3289 Loss_G: 2.8177 \n",
            "[22/25][256/391] Loss_D: 0.4063 Loss_G: 2.6210 \n",
            "[22/25][257/391] Loss_D: 0.4871 Loss_G: 2.0245 \n",
            "[22/25][258/391] Loss_D: 0.4450 Loss_G: 3.1559 \n",
            "[22/25][259/391] Loss_D: 0.4057 Loss_G: 2.6173 \n",
            "[22/25][260/391] Loss_D: 0.3902 Loss_G: 2.4282 \n",
            "[22/25][261/391] Loss_D: 0.5317 Loss_G: 3.5147 \n",
            "[22/25][262/391] Loss_D: 0.7310 Loss_G: 0.8548 \n",
            "[22/25][263/391] Loss_D: 1.0484 Loss_G: 4.9725 \n",
            "[22/25][264/391] Loss_D: 0.7603 Loss_G: 0.6915 \n",
            "[22/25][265/391] Loss_D: 1.4990 Loss_G: 6.3255 \n",
            "[22/25][266/391] Loss_D: 2.2079 Loss_G: 0.3870 \n",
            "[22/25][267/391] Loss_D: 1.6108 Loss_G: 5.3948 \n",
            "[22/25][268/391] Loss_D: 1.1629 Loss_G: 1.5582 \n",
            "[22/25][269/391] Loss_D: 0.7715 Loss_G: 2.7827 \n",
            "[22/25][270/391] Loss_D: 0.7481 Loss_G: 1.7461 \n",
            "[22/25][271/391] Loss_D: 0.6424 Loss_G: 3.0012 \n",
            "[22/25][272/391] Loss_D: 0.6174 Loss_G: 1.9776 \n",
            "[22/25][273/391] Loss_D: 0.6371 Loss_G: 2.7749 \n",
            "[22/25][274/391] Loss_D: 0.6947 Loss_G: 1.9454 \n",
            "[22/25][275/391] Loss_D: 0.6461 Loss_G: 2.8648 \n",
            "[22/25][276/391] Loss_D: 0.4878 Loss_G: 3.5622 \n",
            "[22/25][277/391] Loss_D: 0.6562 Loss_G: 1.2328 \n",
            "[22/25][278/391] Loss_D: 0.8494 Loss_G: 4.2838 \n",
            "[22/25][279/391] Loss_D: 0.1883 Loss_G: 4.1140 \n",
            "[22/25][280/391] Loss_D: 0.8558 Loss_G: 0.5130 \n",
            "[22/25][281/391] Loss_D: 1.3415 Loss_G: 5.3114 \n",
            "[22/25][282/391] Loss_D: 0.7838 Loss_G: 2.6020 \n",
            "[22/25][283/391] Loss_D: 0.4292 Loss_G: 2.0451 \n",
            "[22/25][284/391] Loss_D: 0.5108 Loss_G: 4.0325 \n",
            "[22/25][285/391] Loss_D: 0.7436 Loss_G: 1.4826 \n",
            "[22/25][286/391] Loss_D: 0.7482 Loss_G: 3.2087 \n",
            "[22/25][287/391] Loss_D: 0.4900 Loss_G: 2.9393 \n",
            "[22/25][288/391] Loss_D: 0.6912 Loss_G: 1.7567 \n",
            "[22/25][289/391] Loss_D: 0.5619 Loss_G: 3.1122 \n",
            "[22/25][290/391] Loss_D: 0.3166 Loss_G: 3.7626 \n",
            "[22/25][291/391] Loss_D: 0.7684 Loss_G: 1.1660 \n",
            "[22/25][292/391] Loss_D: 0.8207 Loss_G: 4.3450 \n",
            "[22/25][293/391] Loss_D: 1.0056 Loss_G: 1.1811 \n",
            "[22/25][294/391] Loss_D: 0.8370 Loss_G: 4.0495 \n",
            "[22/25][295/391] Loss_D: 0.4553 Loss_G: 2.9170 \n",
            "[22/25][296/391] Loss_D: 0.4400 Loss_G: 1.9726 \n",
            "[22/25][297/391] Loss_D: 0.6615 Loss_G: 4.0807 \n",
            "[22/25][298/391] Loss_D: 0.5055 Loss_G: 2.3620 \n",
            "[22/25][299/391] Loss_D: 0.4798 Loss_G: 2.1009 \n",
            "[22/25][300/391] Loss_D: 0.4839 Loss_G: 3.5991 \n",
            "saving the output\n",
            "[22/25][301/391] Loss_D: 0.5843 Loss_G: 2.0083 \n",
            "[22/25][302/391] Loss_D: 0.4516 Loss_G: 2.4170 \n",
            "[22/25][303/391] Loss_D: 0.4218 Loss_G: 4.1830 \n",
            "[22/25][304/391] Loss_D: 1.0560 Loss_G: 1.2016 \n",
            "[22/25][305/391] Loss_D: 0.7937 Loss_G: 3.3985 \n",
            "[22/25][306/391] Loss_D: 0.4053 Loss_G: 3.5245 \n",
            "[22/25][307/391] Loss_D: 0.5265 Loss_G: 1.9135 \n",
            "[22/25][308/391] Loss_D: 0.4172 Loss_G: 3.0414 \n",
            "[22/25][309/391] Loss_D: 0.4382 Loss_G: 2.6705 \n",
            "[22/25][310/391] Loss_D: 0.4588 Loss_G: 2.2604 \n",
            "[22/25][311/391] Loss_D: 0.4876 Loss_G: 3.5352 \n",
            "[22/25][312/391] Loss_D: 0.5624 Loss_G: 1.7141 \n",
            "[22/25][313/391] Loss_D: 0.6008 Loss_G: 2.9284 \n",
            "[22/25][314/391] Loss_D: 0.4378 Loss_G: 2.8695 \n",
            "[22/25][315/391] Loss_D: 0.4187 Loss_G: 2.3553 \n",
            "[22/25][316/391] Loss_D: 0.4521 Loss_G: 2.6604 \n",
            "[22/25][317/391] Loss_D: 0.4032 Loss_G: 2.6543 \n",
            "[22/25][318/391] Loss_D: 0.3236 Loss_G: 2.8201 \n",
            "[22/25][319/391] Loss_D: 0.4948 Loss_G: 2.0449 \n",
            "[22/25][320/391] Loss_D: 0.5002 Loss_G: 3.5679 \n",
            "[22/25][321/391] Loss_D: 0.5122 Loss_G: 2.3140 \n",
            "[22/25][322/391] Loss_D: 0.3771 Loss_G: 2.7275 \n",
            "[22/25][323/391] Loss_D: 0.4769 Loss_G: 2.6121 \n",
            "[22/25][324/391] Loss_D: 0.3678 Loss_G: 2.7839 \n",
            "[22/25][325/391] Loss_D: 0.4587 Loss_G: 2.7082 \n",
            "[22/25][326/391] Loss_D: 0.4220 Loss_G: 2.2365 \n",
            "[22/25][327/391] Loss_D: 0.4454 Loss_G: 2.7402 \n",
            "[22/25][328/391] Loss_D: 0.4155 Loss_G: 2.4892 \n",
            "[22/25][329/391] Loss_D: 0.3456 Loss_G: 2.5041 \n",
            "[22/25][330/391] Loss_D: 0.2953 Loss_G: 3.3868 \n",
            "[22/25][331/391] Loss_D: 0.5589 Loss_G: 1.7625 \n",
            "[22/25][332/391] Loss_D: 0.5458 Loss_G: 2.1616 \n",
            "[22/25][333/391] Loss_D: 0.4951 Loss_G: 3.4066 \n",
            "[22/25][334/391] Loss_D: 0.5916 Loss_G: 1.8925 \n",
            "[22/25][335/391] Loss_D: 0.4887 Loss_G: 3.0099 \n",
            "[22/25][336/391] Loss_D: 0.3639 Loss_G: 2.8387 \n",
            "[22/25][337/391] Loss_D: 0.2362 Loss_G: 3.0792 \n",
            "[22/25][338/391] Loss_D: 0.5316 Loss_G: 1.6885 \n",
            "[22/25][339/391] Loss_D: 0.6068 Loss_G: 3.5605 \n",
            "[22/25][340/391] Loss_D: 0.3040 Loss_G: 3.0560 \n",
            "[22/25][341/391] Loss_D: 0.3582 Loss_G: 2.0684 \n",
            "[22/25][342/391] Loss_D: 0.4958 Loss_G: 3.0470 \n",
            "[22/25][343/391] Loss_D: 0.3875 Loss_G: 2.6352 \n",
            "[22/25][344/391] Loss_D: 0.3625 Loss_G: 2.7696 \n",
            "[22/25][345/391] Loss_D: 0.4611 Loss_G: 2.1513 \n",
            "[22/25][346/391] Loss_D: 0.4274 Loss_G: 3.8018 \n",
            "[22/25][347/391] Loss_D: 0.6884 Loss_G: 1.5328 \n",
            "[22/25][348/391] Loss_D: 0.5087 Loss_G: 3.0933 \n",
            "[22/25][349/391] Loss_D: 0.3552 Loss_G: 2.6916 \n",
            "[22/25][350/391] Loss_D: 0.4160 Loss_G: 2.5962 \n",
            "[22/25][351/391] Loss_D: 0.4347 Loss_G: 2.7217 \n",
            "[22/25][352/391] Loss_D: 0.5044 Loss_G: 1.9217 \n",
            "[22/25][353/391] Loss_D: 0.4337 Loss_G: 2.7973 \n",
            "[22/25][354/391] Loss_D: 0.3698 Loss_G: 2.9709 \n",
            "[22/25][355/391] Loss_D: 0.3107 Loss_G: 2.7140 \n",
            "[22/25][356/391] Loss_D: 0.4676 Loss_G: 2.1033 \n",
            "[22/25][357/391] Loss_D: 0.4030 Loss_G: 2.7291 \n",
            "[22/25][358/391] Loss_D: 0.3507 Loss_G: 2.9390 \n",
            "[22/25][359/391] Loss_D: 0.4133 Loss_G: 2.3221 \n",
            "[22/25][360/391] Loss_D: 0.3520 Loss_G: 3.4698 \n",
            "[22/25][361/391] Loss_D: 0.5232 Loss_G: 1.9248 \n",
            "[22/25][362/391] Loss_D: 0.4274 Loss_G: 3.0342 \n",
            "[22/25][363/391] Loss_D: 0.4380 Loss_G: 2.0149 \n",
            "[22/25][364/391] Loss_D: 0.4992 Loss_G: 2.8396 \n",
            "[22/25][365/391] Loss_D: 0.3124 Loss_G: 2.9361 \n",
            "[22/25][366/391] Loss_D: 0.3935 Loss_G: 2.0584 \n",
            "[22/25][367/391] Loss_D: 0.3835 Loss_G: 3.2271 \n",
            "[22/25][368/391] Loss_D: 0.3682 Loss_G: 2.8612 \n",
            "[22/25][369/391] Loss_D: 0.5517 Loss_G: 1.7178 \n",
            "[22/25][370/391] Loss_D: 0.5287 Loss_G: 3.6625 \n",
            "[22/25][371/391] Loss_D: 0.5843 Loss_G: 1.5905 \n",
            "[22/25][372/391] Loss_D: 0.4250 Loss_G: 3.6165 \n",
            "[22/25][373/391] Loss_D: 0.3732 Loss_G: 2.3850 \n",
            "[22/25][374/391] Loss_D: 0.4635 Loss_G: 3.7031 \n",
            "[22/25][375/391] Loss_D: 0.4975 Loss_G: 1.9902 \n",
            "[22/25][376/391] Loss_D: 0.4445 Loss_G: 2.7507 \n",
            "[22/25][377/391] Loss_D: 0.5148 Loss_G: 3.4164 \n",
            "[22/25][378/391] Loss_D: 0.4367 Loss_G: 1.8911 \n",
            "[22/25][379/391] Loss_D: 0.5962 Loss_G: 3.9787 \n",
            "[22/25][380/391] Loss_D: 0.6737 Loss_G: 1.3307 \n",
            "[22/25][381/391] Loss_D: 0.6937 Loss_G: 4.1494 \n",
            "[22/25][382/391] Loss_D: 0.5768 Loss_G: 1.9573 \n",
            "[22/25][383/391] Loss_D: 0.5537 Loss_G: 2.4968 \n",
            "[22/25][384/391] Loss_D: 0.3488 Loss_G: 3.2163 \n",
            "[22/25][385/391] Loss_D: 0.3337 Loss_G: 2.9089 \n",
            "[22/25][386/391] Loss_D: 0.3337 Loss_G: 2.5019 \n",
            "[22/25][387/391] Loss_D: 0.4341 Loss_G: 1.9185 \n",
            "[22/25][388/391] Loss_D: 0.5301 Loss_G: 3.7384 \n",
            "[22/25][389/391] Loss_D: 0.7993 Loss_G: 1.2636 \n",
            "[22/25][390/391] Loss_D: 0.6551 Loss_G: 4.7096 \n",
            "[23/25][0/391] Loss_D: 0.2856 Loss_G: 3.4120 \n",
            "saving the output\n",
            "[23/25][1/391] Loss_D: 0.4405 Loss_G: 1.2112 \n",
            "[23/25][2/391] Loss_D: 0.6614 Loss_G: 4.6990 \n",
            "[23/25][3/391] Loss_D: 1.0499 Loss_G: 0.7946 \n",
            "[23/25][4/391] Loss_D: 1.0749 Loss_G: 4.7267 \n",
            "[23/25][5/391] Loss_D: 0.9976 Loss_G: 0.9965 \n",
            "[23/25][6/391] Loss_D: 0.9004 Loss_G: 4.4386 \n",
            "[23/25][7/391] Loss_D: 0.7379 Loss_G: 1.6965 \n",
            "[23/25][8/391] Loss_D: 0.5633 Loss_G: 2.9273 \n",
            "[23/25][9/391] Loss_D: 0.4104 Loss_G: 3.1438 \n",
            "[23/25][10/391] Loss_D: 0.3472 Loss_G: 2.4443 \n",
            "[23/25][11/391] Loss_D: 0.5490 Loss_G: 2.6697 \n",
            "[23/25][12/391] Loss_D: 0.2540 Loss_G: 4.1345 \n",
            "[23/25][13/391] Loss_D: 0.4315 Loss_G: 2.2086 \n",
            "[23/25][14/391] Loss_D: 0.5143 Loss_G: 3.9491 \n",
            "[23/25][15/391] Loss_D: 0.4464 Loss_G: 2.1584 \n",
            "[23/25][16/391] Loss_D: 0.4495 Loss_G: 2.7939 \n",
            "[23/25][17/391] Loss_D: 0.4252 Loss_G: 2.6478 \n",
            "[23/25][18/391] Loss_D: 0.2611 Loss_G: 3.2684 \n",
            "[23/25][19/391] Loss_D: 0.4790 Loss_G: 2.0545 \n",
            "[23/25][20/391] Loss_D: 0.3760 Loss_G: 3.1434 \n",
            "[23/25][21/391] Loss_D: 0.3656 Loss_G: 2.8682 \n",
            "[23/25][22/391] Loss_D: 0.3372 Loss_G: 2.8677 \n",
            "[23/25][23/391] Loss_D: 0.1707 Loss_G: 3.7573 \n",
            "[23/25][24/391] Loss_D: 0.4759 Loss_G: 1.8698 \n",
            "[23/25][25/391] Loss_D: 0.3406 Loss_G: 2.8453 \n",
            "[23/25][26/391] Loss_D: 0.3118 Loss_G: 3.3213 \n",
            "[23/25][27/391] Loss_D: 0.4753 Loss_G: 2.0086 \n",
            "[23/25][28/391] Loss_D: 0.3672 Loss_G: 3.0440 \n",
            "[23/25][29/391] Loss_D: 0.3843 Loss_G: 2.1521 \n",
            "[23/25][30/391] Loss_D: 0.6053 Loss_G: 3.6484 \n",
            "[23/25][31/391] Loss_D: 0.7203 Loss_G: 1.2555 \n",
            "[23/25][32/391] Loss_D: 0.8716 Loss_G: 4.8152 \n",
            "[23/25][33/391] Loss_D: 0.6745 Loss_G: 1.9980 \n",
            "[23/25][34/391] Loss_D: 0.4631 Loss_G: 3.3774 \n",
            "[23/25][35/391] Loss_D: 0.4968 Loss_G: 1.9215 \n",
            "[23/25][36/391] Loss_D: 0.6759 Loss_G: 4.1828 \n",
            "[23/25][37/391] Loss_D: 0.5982 Loss_G: 1.7359 \n",
            "[23/25][38/391] Loss_D: 0.6658 Loss_G: 4.1608 \n",
            "[23/25][39/391] Loss_D: 0.6668 Loss_G: 1.7902 \n",
            "[23/25][40/391] Loss_D: 0.5367 Loss_G: 3.8661 \n",
            "[23/25][41/391] Loss_D: 0.4261 Loss_G: 2.3924 \n",
            "[23/25][42/391] Loss_D: 0.4674 Loss_G: 2.5305 \n",
            "[23/25][43/391] Loss_D: 0.5066 Loss_G: 2.3243 \n",
            "[23/25][44/391] Loss_D: 0.5472 Loss_G: 2.3643 \n",
            "[23/25][45/391] Loss_D: 0.5064 Loss_G: 3.4456 \n",
            "[23/25][46/391] Loss_D: 0.4287 Loss_G: 2.3822 \n",
            "[23/25][47/391] Loss_D: 0.4205 Loss_G: 2.6400 \n",
            "[23/25][48/391] Loss_D: 0.3016 Loss_G: 3.4819 \n",
            "[23/25][49/391] Loss_D: 0.2534 Loss_G: 3.3333 \n",
            "[23/25][50/391] Loss_D: 0.2306 Loss_G: 3.0285 \n",
            "[23/25][51/391] Loss_D: 0.2996 Loss_G: 2.2845 \n",
            "[23/25][52/391] Loss_D: 0.4752 Loss_G: 2.1560 \n",
            "[23/25][53/391] Loss_D: 0.4207 Loss_G: 3.7238 \n",
            "[23/25][54/391] Loss_D: 0.3544 Loss_G: 2.7188 \n",
            "[23/25][55/391] Loss_D: 0.4171 Loss_G: 1.6762 \n",
            "[23/25][56/391] Loss_D: 0.5323 Loss_G: 3.7853 \n",
            "[23/25][57/391] Loss_D: 0.4907 Loss_G: 2.4164 \n",
            "[23/25][58/391] Loss_D: 0.3995 Loss_G: 1.9292 \n",
            "[23/25][59/391] Loss_D: 0.5168 Loss_G: 4.0883 \n",
            "[23/25][60/391] Loss_D: 0.3982 Loss_G: 1.9852 \n",
            "[23/25][61/391] Loss_D: 0.3940 Loss_G: 3.1414 \n",
            "[23/25][62/391] Loss_D: 0.4535 Loss_G: 2.3845 \n",
            "[23/25][63/391] Loss_D: 0.3146 Loss_G: 3.0766 \n",
            "[23/25][64/391] Loss_D: 0.3740 Loss_G: 2.6295 \n",
            "[23/25][65/391] Loss_D: 0.4128 Loss_G: 3.1194 \n",
            "[23/25][66/391] Loss_D: 0.4420 Loss_G: 2.2262 \n",
            "[23/25][67/391] Loss_D: 0.3998 Loss_G: 2.2027 \n",
            "[23/25][68/391] Loss_D: 0.4614 Loss_G: 3.2895 \n",
            "[23/25][69/391] Loss_D: 0.3721 Loss_G: 2.6291 \n",
            "[23/25][70/391] Loss_D: 0.2587 Loss_G: 2.9293 \n",
            "[23/25][71/391] Loss_D: 0.4776 Loss_G: 2.7146 \n",
            "[23/25][72/391] Loss_D: 0.4017 Loss_G: 2.3153 \n",
            "[23/25][73/391] Loss_D: 0.4357 Loss_G: 3.6875 \n",
            "[23/25][74/391] Loss_D: 0.5279 Loss_G: 1.7790 \n",
            "[23/25][75/391] Loss_D: 0.5423 Loss_G: 3.1602 \n",
            "[23/25][76/391] Loss_D: 0.3510 Loss_G: 3.0780 \n",
            "[23/25][77/391] Loss_D: 0.3752 Loss_G: 2.0735 \n",
            "[23/25][78/391] Loss_D: 0.5672 Loss_G: 3.6319 \n",
            "[23/25][79/391] Loss_D: 0.4586 Loss_G: 2.2345 \n",
            "[23/25][80/391] Loss_D: 0.4689 Loss_G: 2.6896 \n",
            "[23/25][81/391] Loss_D: 0.3303 Loss_G: 3.5218 \n",
            "[23/25][82/391] Loss_D: 0.5004 Loss_G: 1.7654 \n",
            "[23/25][83/391] Loss_D: 0.7131 Loss_G: 4.5496 \n",
            "[23/25][84/391] Loss_D: 0.8041 Loss_G: 0.8585 \n",
            "[23/25][85/391] Loss_D: 0.9323 Loss_G: 5.3168 \n",
            "[23/25][86/391] Loss_D: 1.0672 Loss_G: 0.2917 \n",
            "[23/25][87/391] Loss_D: 1.8852 Loss_G: 6.2362 \n",
            "[23/25][88/391] Loss_D: 2.0134 Loss_G: 0.1202 \n",
            "[23/25][89/391] Loss_D: 2.9309 Loss_G: 7.6019 \n",
            "[23/25][90/391] Loss_D: 4.5494 Loss_G: 0.4659 \n",
            "[23/25][91/391] Loss_D: 1.6661 Loss_G: 3.9835 \n",
            "[23/25][92/391] Loss_D: 1.0717 Loss_G: 1.2464 \n",
            "[23/25][93/391] Loss_D: 1.1639 Loss_G: 4.0958 \n",
            "[23/25][94/391] Loss_D: 2.0204 Loss_G: 0.2977 \n",
            "[23/25][95/391] Loss_D: 2.5026 Loss_G: 5.0371 \n",
            "[23/25][96/391] Loss_D: 1.5091 Loss_G: 1.5153 \n",
            "[23/25][97/391] Loss_D: 0.9537 Loss_G: 2.6119 \n",
            "[23/25][98/391] Loss_D: 0.6727 Loss_G: 2.5713 \n",
            "[23/25][99/391] Loss_D: 0.7088 Loss_G: 1.8754 \n",
            "[23/25][100/391] Loss_D: 0.8270 Loss_G: 2.9588 \n",
            "saving the output\n",
            "[23/25][101/391] Loss_D: 0.8284 Loss_G: 1.0507 \n",
            "[23/25][102/391] Loss_D: 1.0920 Loss_G: 4.9895 \n",
            "[23/25][103/391] Loss_D: 1.3017 Loss_G: 0.6154 \n",
            "[23/25][104/391] Loss_D: 1.3833 Loss_G: 4.2158 \n",
            "[23/25][105/391] Loss_D: 0.6204 Loss_G: 2.7895 \n",
            "[23/25][106/391] Loss_D: 0.4429 Loss_G: 1.9303 \n",
            "[23/25][107/391] Loss_D: 0.7094 Loss_G: 2.8608 \n",
            "[23/25][108/391] Loss_D: 0.5850 Loss_G: 2.5511 \n",
            "[23/25][109/391] Loss_D: 0.6659 Loss_G: 2.5547 \n",
            "[23/25][110/391] Loss_D: 0.5430 Loss_G: 2.7677 \n",
            "[23/25][111/391] Loss_D: 0.8909 Loss_G: 1.8137 \n",
            "[23/25][112/391] Loss_D: 0.4712 Loss_G: 3.6958 \n",
            "[23/25][113/391] Loss_D: 0.7506 Loss_G: 1.3504 \n",
            "[23/25][114/391] Loss_D: 0.8331 Loss_G: 3.7958 \n",
            "[23/25][115/391] Loss_D: 0.6065 Loss_G: 2.2858 \n",
            "[23/25][116/391] Loss_D: 0.5062 Loss_G: 2.3047 \n",
            "[23/25][117/391] Loss_D: 0.4431 Loss_G: 3.3318 \n",
            "[23/25][118/391] Loss_D: 0.4167 Loss_G: 2.5187 \n",
            "[23/25][119/391] Loss_D: 0.7298 Loss_G: 2.1306 \n",
            "[23/25][120/391] Loss_D: 0.6424 Loss_G: 3.1297 \n",
            "[23/25][121/391] Loss_D: 0.5456 Loss_G: 2.1015 \n",
            "[23/25][122/391] Loss_D: 0.4103 Loss_G: 3.1558 \n",
            "[23/25][123/391] Loss_D: 0.5766 Loss_G: 2.2351 \n",
            "[23/25][124/391] Loss_D: 0.4440 Loss_G: 3.2499 \n",
            "[23/25][125/391] Loss_D: 0.3159 Loss_G: 2.8281 \n",
            "[23/25][126/391] Loss_D: 0.5388 Loss_G: 2.4180 \n",
            "[23/25][127/391] Loss_D: 0.4613 Loss_G: 2.3150 \n",
            "[23/25][128/391] Loss_D: 0.5094 Loss_G: 3.1935 \n",
            "[23/25][129/391] Loss_D: 0.4344 Loss_G: 2.2525 \n",
            "[23/25][130/391] Loss_D: 0.3977 Loss_G: 3.1515 \n",
            "[23/25][131/391] Loss_D: 0.3739 Loss_G: 3.1082 \n",
            "[23/25][132/391] Loss_D: 0.4112 Loss_G: 2.1411 \n",
            "[23/25][133/391] Loss_D: 0.6162 Loss_G: 3.1819 \n",
            "[23/25][134/391] Loss_D: 0.4615 Loss_G: 2.5766 \n",
            "[23/25][135/391] Loss_D: 0.5925 Loss_G: 1.6130 \n",
            "[23/25][136/391] Loss_D: 0.4869 Loss_G: 3.0002 \n",
            "[23/25][137/391] Loss_D: 0.4354 Loss_G: 2.6640 \n",
            "[23/25][138/391] Loss_D: 0.4951 Loss_G: 2.1462 \n",
            "[23/25][139/391] Loss_D: 0.5449 Loss_G: 3.0238 \n",
            "[23/25][140/391] Loss_D: 0.3785 Loss_G: 3.1982 \n",
            "[23/25][141/391] Loss_D: 0.3125 Loss_G: 2.5911 \n",
            "[23/25][142/391] Loss_D: 0.4106 Loss_G: 1.5997 \n",
            "[23/25][143/391] Loss_D: 0.5350 Loss_G: 4.0594 \n",
            "[23/25][144/391] Loss_D: 0.6516 Loss_G: 1.6145 \n",
            "[23/25][145/391] Loss_D: 0.6477 Loss_G: 3.9003 \n",
            "[23/25][146/391] Loss_D: 0.4740 Loss_G: 1.7088 \n",
            "[23/25][147/391] Loss_D: 0.5562 Loss_G: 3.9737 \n",
            "[23/25][148/391] Loss_D: 0.9728 Loss_G: 0.5913 \n",
            "[23/25][149/391] Loss_D: 1.2995 Loss_G: 5.1039 \n",
            "[23/25][150/391] Loss_D: 0.8946 Loss_G: 1.0715 \n",
            "[23/25][151/391] Loss_D: 0.8152 Loss_G: 4.4907 \n",
            "[23/25][152/391] Loss_D: 0.4728 Loss_G: 2.4810 \n",
            "[23/25][153/391] Loss_D: 0.4089 Loss_G: 2.1522 \n",
            "[23/25][154/391] Loss_D: 0.5659 Loss_G: 2.4085 \n",
            "[23/25][155/391] Loss_D: 0.4591 Loss_G: 2.9105 \n",
            "[23/25][156/391] Loss_D: 0.5872 Loss_G: 2.3229 \n",
            "[23/25][157/391] Loss_D: 0.3646 Loss_G: 3.2554 \n",
            "[23/25][158/391] Loss_D: 0.4607 Loss_G: 2.1698 \n",
            "[23/25][159/391] Loss_D: 0.4118 Loss_G: 3.0039 \n",
            "[23/25][160/391] Loss_D: 0.4446 Loss_G: 2.1616 \n",
            "[23/25][161/391] Loss_D: 0.5005 Loss_G: 2.7907 \n",
            "[23/25][162/391] Loss_D: 0.4123 Loss_G: 2.9970 \n",
            "[23/25][163/391] Loss_D: 0.6151 Loss_G: 1.3059 \n",
            "[23/25][164/391] Loss_D: 0.7288 Loss_G: 3.9530 \n",
            "[23/25][165/391] Loss_D: 0.6050 Loss_G: 1.8326 \n",
            "[23/25][166/391] Loss_D: 0.5963 Loss_G: 3.7780 \n",
            "[23/25][167/391] Loss_D: 0.5171 Loss_G: 2.0512 \n",
            "[23/25][168/391] Loss_D: 0.4119 Loss_G: 2.3478 \n",
            "[23/25][169/391] Loss_D: 0.4918 Loss_G: 3.2070 \n",
            "[23/25][170/391] Loss_D: 0.5390 Loss_G: 1.9357 \n",
            "[23/25][171/391] Loss_D: 0.5760 Loss_G: 4.1180 \n",
            "[23/25][172/391] Loss_D: 0.6305 Loss_G: 2.0653 \n",
            "[23/25][173/391] Loss_D: 0.4535 Loss_G: 1.8128 \n",
            "[23/25][174/391] Loss_D: 0.5769 Loss_G: 3.7663 \n",
            "[23/25][175/391] Loss_D: 0.5029 Loss_G: 2.1101 \n",
            "[23/25][176/391] Loss_D: 0.5947 Loss_G: 3.5751 \n",
            "[23/25][177/391] Loss_D: 0.6066 Loss_G: 1.9673 \n",
            "[23/25][178/391] Loss_D: 0.5163 Loss_G: 2.4542 \n",
            "[23/25][179/391] Loss_D: 0.4548 Loss_G: 3.5416 \n",
            "[23/25][180/391] Loss_D: 0.4463 Loss_G: 2.3902 \n",
            "[23/25][181/391] Loss_D: 0.3282 Loss_G: 2.7236 \n",
            "[23/25][182/391] Loss_D: 0.4284 Loss_G: 2.1706 \n",
            "[23/25][183/391] Loss_D: 0.4430 Loss_G: 3.6994 \n",
            "[23/25][184/391] Loss_D: 0.6251 Loss_G: 1.7380 \n",
            "[23/25][185/391] Loss_D: 0.4194 Loss_G: 3.4617 \n",
            "[23/25][186/391] Loss_D: 0.4896 Loss_G: 2.4967 \n",
            "[23/25][187/391] Loss_D: 0.5593 Loss_G: 1.7253 \n",
            "[23/25][188/391] Loss_D: 0.4559 Loss_G: 3.4597 \n",
            "[23/25][189/391] Loss_D: 0.4186 Loss_G: 2.6911 \n",
            "[23/25][190/391] Loss_D: 0.4013 Loss_G: 2.2659 \n",
            "[23/25][191/391] Loss_D: 0.4135 Loss_G: 2.5285 \n",
            "[23/25][192/391] Loss_D: 0.3911 Loss_G: 2.9317 \n",
            "[23/25][193/391] Loss_D: 0.3991 Loss_G: 2.5876 \n",
            "[23/25][194/391] Loss_D: 0.4057 Loss_G: 2.3811 \n",
            "[23/25][195/391] Loss_D: 0.4259 Loss_G: 3.0647 \n",
            "[23/25][196/391] Loss_D: 0.3260 Loss_G: 2.7810 \n",
            "[23/25][197/391] Loss_D: 0.3878 Loss_G: 1.9175 \n",
            "[23/25][198/391] Loss_D: 0.4225 Loss_G: 3.2091 \n",
            "[23/25][199/391] Loss_D: 0.4465 Loss_G: 2.3532 \n",
            "[23/25][200/391] Loss_D: 0.3928 Loss_G: 2.9968 \n",
            "saving the output\n",
            "[23/25][201/391] Loss_D: 0.3194 Loss_G: 2.9370 \n",
            "[23/25][202/391] Loss_D: 0.3425 Loss_G: 2.4578 \n",
            "[23/25][203/391] Loss_D: 0.5464 Loss_G: 2.8362 \n",
            "[23/25][204/391] Loss_D: 0.5546 Loss_G: 1.6688 \n",
            "[23/25][205/391] Loss_D: 0.7325 Loss_G: 3.6971 \n",
            "[23/25][206/391] Loss_D: 0.4986 Loss_G: 1.9461 \n",
            "[23/25][207/391] Loss_D: 0.3343 Loss_G: 3.2532 \n",
            "[23/25][208/391] Loss_D: 0.4260 Loss_G: 2.5970 \n",
            "[23/25][209/391] Loss_D: 0.3935 Loss_G: 2.8454 \n",
            "[23/25][210/391] Loss_D: 0.5175 Loss_G: 1.6679 \n",
            "[23/25][211/391] Loss_D: 0.4207 Loss_G: 3.2376 \n",
            "[23/25][212/391] Loss_D: 0.3857 Loss_G: 3.0227 \n",
            "[23/25][213/391] Loss_D: 0.5624 Loss_G: 1.7859 \n",
            "[23/25][214/391] Loss_D: 0.6092 Loss_G: 3.5022 \n",
            "[23/25][215/391] Loss_D: 0.3718 Loss_G: 2.6793 \n",
            "[23/25][216/391] Loss_D: 0.4131 Loss_G: 2.3009 \n",
            "[23/25][217/391] Loss_D: 0.2896 Loss_G: 3.3477 \n",
            "[23/25][218/391] Loss_D: 0.4437 Loss_G: 2.3699 \n",
            "[23/25][219/391] Loss_D: 0.3930 Loss_G: 2.8923 \n",
            "[23/25][220/391] Loss_D: 0.4816 Loss_G: 1.9836 \n",
            "[23/25][221/391] Loss_D: 0.5786 Loss_G: 4.0981 \n",
            "[23/25][222/391] Loss_D: 0.4479 Loss_G: 2.5342 \n",
            "[23/25][223/391] Loss_D: 0.4735 Loss_G: 1.3551 \n",
            "[23/25][224/391] Loss_D: 0.7555 Loss_G: 4.8590 \n",
            "[23/25][225/391] Loss_D: 0.7524 Loss_G: 0.4308 \n",
            "[23/25][226/391] Loss_D: 1.8639 Loss_G: 6.5787 \n",
            "[23/25][227/391] Loss_D: 1.8038 Loss_G: 0.4483 \n",
            "[23/25][228/391] Loss_D: 1.7268 Loss_G: 6.6964 \n",
            "[23/25][229/391] Loss_D: 3.4335 Loss_G: 0.0618 \n",
            "[23/25][230/391] Loss_D: 3.8107 Loss_G: 5.4755 \n",
            "[23/25][231/391] Loss_D: 1.3951 Loss_G: 1.2807 \n",
            "[23/25][232/391] Loss_D: 1.2694 Loss_G: 3.7834 \n",
            "[23/25][233/391] Loss_D: 1.0290 Loss_G: 1.0691 \n",
            "[23/25][234/391] Loss_D: 1.0730 Loss_G: 4.3958 \n",
            "[23/25][235/391] Loss_D: 1.0972 Loss_G: 1.0237 \n",
            "[23/25][236/391] Loss_D: 0.8412 Loss_G: 3.3829 \n",
            "[23/25][237/391] Loss_D: 0.7515 Loss_G: 2.1186 \n",
            "[23/25][238/391] Loss_D: 0.7775 Loss_G: 2.3403 \n",
            "[23/25][239/391] Loss_D: 0.4093 Loss_G: 2.9749 \n",
            "[23/25][240/391] Loss_D: 0.8356 Loss_G: 1.2561 \n",
            "[23/25][241/391] Loss_D: 0.8527 Loss_G: 4.8668 \n",
            "[23/25][242/391] Loss_D: 1.2316 Loss_G: 0.7262 \n",
            "[23/25][243/391] Loss_D: 1.2990 Loss_G: 5.1599 \n",
            "[23/25][244/391] Loss_D: 1.2199 Loss_G: 0.3999 \n",
            "[23/25][245/391] Loss_D: 1.9316 Loss_G: 5.5885 \n",
            "[23/25][246/391] Loss_D: 2.3092 Loss_G: 0.9300 \n",
            "[23/25][247/391] Loss_D: 1.3074 Loss_G: 3.1389 \n",
            "[23/25][248/391] Loss_D: 0.8042 Loss_G: 1.9482 \n",
            "[23/25][249/391] Loss_D: 1.0010 Loss_G: 2.5423 \n",
            "[23/25][250/391] Loss_D: 0.7358 Loss_G: 2.2293 \n",
            "[23/25][251/391] Loss_D: 0.9245 Loss_G: 1.8773 \n",
            "[23/25][252/391] Loss_D: 0.8611 Loss_G: 3.0573 \n",
            "[23/25][253/391] Loss_D: 0.6523 Loss_G: 2.3042 \n",
            "[23/25][254/391] Loss_D: 0.6512 Loss_G: 3.6358 \n",
            "[23/25][255/391] Loss_D: 0.5364 Loss_G: 2.2592 \n",
            "[23/25][256/391] Loss_D: 0.8605 Loss_G: 2.1413 \n",
            "[23/25][257/391] Loss_D: 0.6980 Loss_G: 3.7790 \n",
            "[23/25][258/391] Loss_D: 0.7574 Loss_G: 1.2506 \n",
            "[23/25][259/391] Loss_D: 1.0001 Loss_G: 4.9387 \n",
            "[23/25][260/391] Loss_D: 0.6969 Loss_G: 1.9616 \n",
            "[23/25][261/391] Loss_D: 0.5264 Loss_G: 3.4085 \n",
            "[23/25][262/391] Loss_D: 0.5878 Loss_G: 2.1983 \n",
            "[23/25][263/391] Loss_D: 0.3816 Loss_G: 3.2752 \n",
            "[23/25][264/391] Loss_D: 0.6635 Loss_G: 2.5447 \n",
            "[23/25][265/391] Loss_D: 0.6074 Loss_G: 2.4686 \n",
            "[23/25][266/391] Loss_D: 0.3819 Loss_G: 2.9693 \n",
            "[23/25][267/391] Loss_D: 0.5767 Loss_G: 2.9509 \n",
            "[23/25][268/391] Loss_D: 0.7855 Loss_G: 1.2314 \n",
            "[23/25][269/391] Loss_D: 0.6873 Loss_G: 4.0615 \n",
            "[23/25][270/391] Loss_D: 0.5808 Loss_G: 2.5636 \n",
            "[23/25][271/391] Loss_D: 0.5100 Loss_G: 3.3912 \n",
            "[23/25][272/391] Loss_D: 0.4586 Loss_G: 2.5489 \n",
            "[23/25][273/391] Loss_D: 0.5372 Loss_G: 2.2316 \n",
            "[23/25][274/391] Loss_D: 0.5502 Loss_G: 2.9661 \n",
            "[23/25][275/391] Loss_D: 0.4587 Loss_G: 2.7540 \n",
            "[23/25][276/391] Loss_D: 0.5574 Loss_G: 1.3536 \n",
            "[23/25][277/391] Loss_D: 0.9390 Loss_G: 4.2310 \n",
            "[23/25][278/391] Loss_D: 1.1694 Loss_G: 0.5381 \n",
            "[23/25][279/391] Loss_D: 1.6021 Loss_G: 5.1263 \n",
            "[23/25][280/391] Loss_D: 1.2751 Loss_G: 1.0116 \n",
            "[23/25][281/391] Loss_D: 1.0817 Loss_G: 5.2063 \n",
            "[23/25][282/391] Loss_D: 1.1779 Loss_G: 1.6311 \n",
            "[23/25][283/391] Loss_D: 0.7672 Loss_G: 3.5688 \n",
            "[23/25][284/391] Loss_D: 0.4374 Loss_G: 3.0057 \n",
            "[23/25][285/391] Loss_D: 0.5935 Loss_G: 1.6177 \n",
            "[23/25][286/391] Loss_D: 0.5145 Loss_G: 4.1013 \n",
            "[23/25][287/391] Loss_D: 0.5851 Loss_G: 1.9294 \n",
            "[23/25][288/391] Loss_D: 0.8479 Loss_G: 4.9169 \n",
            "[23/25][289/391] Loss_D: 0.6371 Loss_G: 2.5074 \n",
            "[23/25][290/391] Loss_D: 0.7978 Loss_G: 0.8948 \n",
            "[23/25][291/391] Loss_D: 0.9415 Loss_G: 5.1255 \n",
            "[23/25][292/391] Loss_D: 0.9623 Loss_G: 1.6883 \n",
            "[23/25][293/391] Loss_D: 0.4039 Loss_G: 2.7044 \n",
            "[23/25][294/391] Loss_D: 0.5190 Loss_G: 2.7459 \n",
            "[23/25][295/391] Loss_D: 0.4664 Loss_G: 2.3507 \n",
            "[23/25][296/391] Loss_D: 0.5281 Loss_G: 3.3208 \n",
            "[23/25][297/391] Loss_D: 0.5734 Loss_G: 2.2367 \n",
            "[23/25][298/391] Loss_D: 0.4015 Loss_G: 2.8649 \n",
            "[23/25][299/391] Loss_D: 0.5994 Loss_G: 2.3292 \n",
            "[23/25][300/391] Loss_D: 0.5337 Loss_G: 2.2742 \n",
            "saving the output\n",
            "[23/25][301/391] Loss_D: 0.6245 Loss_G: 3.7083 \n",
            "[23/25][302/391] Loss_D: 0.5686 Loss_G: 2.3534 \n",
            "[23/25][303/391] Loss_D: 0.4205 Loss_G: 2.0649 \n",
            "[23/25][304/391] Loss_D: 0.5254 Loss_G: 3.1085 \n",
            "[23/25][305/391] Loss_D: 0.5159 Loss_G: 2.3484 \n",
            "[23/25][306/391] Loss_D: 0.3021 Loss_G: 2.8227 \n",
            "[23/25][307/391] Loss_D: 0.4103 Loss_G: 2.8952 \n",
            "[23/25][308/391] Loss_D: 0.4385 Loss_G: 1.9667 \n",
            "[23/25][309/391] Loss_D: 0.4819 Loss_G: 2.2538 \n",
            "[23/25][310/391] Loss_D: 0.5866 Loss_G: 3.6101 \n",
            "[23/25][311/391] Loss_D: 1.0662 Loss_G: 1.0208 \n",
            "[23/25][312/391] Loss_D: 1.0317 Loss_G: 3.8054 \n",
            "[23/25][313/391] Loss_D: 0.5205 Loss_G: 2.7005 \n",
            "[23/25][314/391] Loss_D: 0.5369 Loss_G: 1.2362 \n",
            "[23/25][315/391] Loss_D: 0.8934 Loss_G: 4.7213 \n",
            "[23/25][316/391] Loss_D: 0.6876 Loss_G: 2.2897 \n",
            "[23/25][317/391] Loss_D: 0.3303 Loss_G: 2.3495 \n",
            "[23/25][318/391] Loss_D: 0.4584 Loss_G: 3.8011 \n",
            "[23/25][319/391] Loss_D: 0.5150 Loss_G: 2.5120 \n",
            "[23/25][320/391] Loss_D: 0.5232 Loss_G: 2.0343 \n",
            "[23/25][321/391] Loss_D: 0.6582 Loss_G: 3.7554 \n",
            "[23/25][322/391] Loss_D: 0.5651 Loss_G: 2.0183 \n",
            "[23/25][323/391] Loss_D: 0.4430 Loss_G: 2.6280 \n",
            "[23/25][324/391] Loss_D: 0.4964 Loss_G: 2.8054 \n",
            "[23/25][325/391] Loss_D: 0.5350 Loss_G: 1.8113 \n",
            "[23/25][326/391] Loss_D: 0.5188 Loss_G: 3.1708 \n",
            "[23/25][327/391] Loss_D: 0.3736 Loss_G: 2.9994 \n",
            "[23/25][328/391] Loss_D: 0.4016 Loss_G: 1.9123 \n",
            "[23/25][329/391] Loss_D: 0.6019 Loss_G: 3.9543 \n",
            "[23/25][330/391] Loss_D: 0.5084 Loss_G: 1.9597 \n",
            "[23/25][331/391] Loss_D: 0.5255 Loss_G: 2.2366 \n",
            "[23/25][332/391] Loss_D: 0.4577 Loss_G: 3.4355 \n",
            "[23/25][333/391] Loss_D: 0.5773 Loss_G: 1.9519 \n",
            "[23/25][334/391] Loss_D: 0.4055 Loss_G: 3.2278 \n",
            "[23/25][335/391] Loss_D: 0.4303 Loss_G: 2.6083 \n",
            "[23/25][336/391] Loss_D: 0.4377 Loss_G: 2.5804 \n",
            "[23/25][337/391] Loss_D: 0.3507 Loss_G: 3.0527 \n",
            "[23/25][338/391] Loss_D: 0.3930 Loss_G: 2.4041 \n",
            "[23/25][339/391] Loss_D: 0.3690 Loss_G: 2.3465 \n",
            "[23/25][340/391] Loss_D: 0.4638 Loss_G: 3.1698 \n",
            "[23/25][341/391] Loss_D: 0.4140 Loss_G: 2.5489 \n",
            "[23/25][342/391] Loss_D: 0.2952 Loss_G: 2.8760 \n",
            "[23/25][343/391] Loss_D: 0.5504 Loss_G: 2.1852 \n",
            "[23/25][344/391] Loss_D: 0.3519 Loss_G: 3.0462 \n",
            "[23/25][345/391] Loss_D: 0.3997 Loss_G: 2.8205 \n",
            "[23/25][346/391] Loss_D: 0.3941 Loss_G: 2.3520 \n",
            "[23/25][347/391] Loss_D: 0.3098 Loss_G: 3.3811 \n",
            "[23/25][348/391] Loss_D: 0.5404 Loss_G: 1.6618 \n",
            "[23/25][349/391] Loss_D: 0.5241 Loss_G: 3.5039 \n",
            "[23/25][350/391] Loss_D: 0.3743 Loss_G: 2.7589 \n",
            "[23/25][351/391] Loss_D: 0.2983 Loss_G: 2.8481 \n",
            "[23/25][352/391] Loss_D: 0.2785 Loss_G: 3.3188 \n",
            "[23/25][353/391] Loss_D: 0.4954 Loss_G: 1.9330 \n",
            "[23/25][354/391] Loss_D: 0.5224 Loss_G: 2.7551 \n",
            "[23/25][355/391] Loss_D: 0.4833 Loss_G: 2.7302 \n",
            "[23/25][356/391] Loss_D: 0.3518 Loss_G: 2.7293 \n",
            "[23/25][357/391] Loss_D: 0.4907 Loss_G: 2.1583 \n",
            "[23/25][358/391] Loss_D: 0.5039 Loss_G: 2.5777 \n",
            "[23/25][359/391] Loss_D: 0.4049 Loss_G: 2.9088 \n",
            "[23/25][360/391] Loss_D: 0.2721 Loss_G: 2.9086 \n",
            "[23/25][361/391] Loss_D: 0.4645 Loss_G: 1.5633 \n",
            "[23/25][362/391] Loss_D: 0.5427 Loss_G: 3.4808 \n",
            "[23/25][363/391] Loss_D: 0.4659 Loss_G: 2.5657 \n",
            "[23/25][364/391] Loss_D: 0.3647 Loss_G: 2.4558 \n",
            "[23/25][365/391] Loss_D: 0.2773 Loss_G: 3.4799 \n",
            "[23/25][366/391] Loss_D: 0.7660 Loss_G: 1.0470 \n",
            "[23/25][367/391] Loss_D: 0.8947 Loss_G: 4.4247 \n",
            "[23/25][368/391] Loss_D: 0.9998 Loss_G: 0.7087 \n",
            "[23/25][369/391] Loss_D: 1.3331 Loss_G: 4.6282 \n",
            "[23/25][370/391] Loss_D: 0.9396 Loss_G: 0.3623 \n",
            "[23/25][371/391] Loss_D: 2.0652 Loss_G: 7.2337 \n",
            "[23/25][372/391] Loss_D: 2.8054 Loss_G: 1.7397 \n",
            "[23/25][373/391] Loss_D: 0.7916 Loss_G: 1.9838 \n",
            "[23/25][374/391] Loss_D: 0.9123 Loss_G: 2.4972 \n",
            "[23/25][375/391] Loss_D: 0.5621 Loss_G: 2.6594 \n",
            "[23/25][376/391] Loss_D: 0.6270 Loss_G: 1.8664 \n",
            "[23/25][377/391] Loss_D: 0.6449 Loss_G: 4.1246 \n",
            "[23/25][378/391] Loss_D: 0.9999 Loss_G: 0.3172 \n",
            "[23/25][379/391] Loss_D: 2.3865 Loss_G: 6.2002 \n",
            "[23/25][380/391] Loss_D: 2.1423 Loss_G: 0.4749 \n",
            "[23/25][381/391] Loss_D: 1.8903 Loss_G: 5.1304 \n",
            "[23/25][382/391] Loss_D: 0.6099 Loss_G: 2.9490 \n",
            "[23/25][383/391] Loss_D: 0.6766 Loss_G: 0.8827 \n",
            "[23/25][384/391] Loss_D: 1.2569 Loss_G: 5.7405 \n",
            "[23/25][385/391] Loss_D: 1.5713 Loss_G: 0.8791 \n",
            "[23/25][386/391] Loss_D: 1.0053 Loss_G: 3.4413 \n",
            "[23/25][387/391] Loss_D: 0.5870 Loss_G: 3.3515 \n",
            "[23/25][388/391] Loss_D: 0.8468 Loss_G: 0.9073 \n",
            "[23/25][389/391] Loss_D: 0.9947 Loss_G: 4.2066 \n",
            "[23/25][390/391] Loss_D: 1.0266 Loss_G: 1.5241 \n",
            "[24/25][0/391] Loss_D: 0.5828 Loss_G: 2.5007 \n",
            "saving the output\n",
            "[24/25][1/391] Loss_D: 0.7124 Loss_G: 3.8971 \n",
            "[24/25][2/391] Loss_D: 0.7069 Loss_G: 1.7148 \n",
            "[24/25][3/391] Loss_D: 0.7113 Loss_G: 2.8131 \n",
            "[24/25][4/391] Loss_D: 0.6816 Loss_G: 2.6696 \n",
            "[24/25][5/391] Loss_D: 0.4640 Loss_G: 2.4145 \n",
            "[24/25][6/391] Loss_D: 0.5920 Loss_G: 2.2743 \n",
            "[24/25][7/391] Loss_D: 0.9325 Loss_G: 3.5790 \n",
            "[24/25][8/391] Loss_D: 0.6765 Loss_G: 1.8346 \n",
            "[24/25][9/391] Loss_D: 0.5003 Loss_G: 2.8335 \n",
            "[24/25][10/391] Loss_D: 0.3507 Loss_G: 3.1275 \n",
            "[24/25][11/391] Loss_D: 0.3474 Loss_G: 2.8218 \n",
            "[24/25][12/391] Loss_D: 0.3891 Loss_G: 2.3805 \n",
            "[24/25][13/391] Loss_D: 0.3800 Loss_G: 2.9791 \n",
            "[24/25][14/391] Loss_D: 0.5280 Loss_G: 2.4952 \n",
            "[24/25][15/391] Loss_D: 0.5425 Loss_G: 1.9630 \n",
            "[24/25][16/391] Loss_D: 0.5421 Loss_G: 3.5115 \n",
            "[24/25][17/391] Loss_D: 0.5453 Loss_G: 1.8913 \n",
            "[24/25][18/391] Loss_D: 0.4935 Loss_G: 2.9129 \n",
            "[24/25][19/391] Loss_D: 0.4150 Loss_G: 3.0974 \n",
            "[24/25][20/391] Loss_D: 0.5486 Loss_G: 1.6732 \n",
            "[24/25][21/391] Loss_D: 0.5387 Loss_G: 3.5057 \n",
            "[24/25][22/391] Loss_D: 0.5719 Loss_G: 1.8863 \n",
            "[24/25][23/391] Loss_D: 0.3285 Loss_G: 3.1911 \n",
            "[24/25][24/391] Loss_D: 0.3378 Loss_G: 3.2445 \n",
            "[24/25][25/391] Loss_D: 0.4615 Loss_G: 2.1891 \n",
            "[24/25][26/391] Loss_D: 0.5026 Loss_G: 2.0291 \n",
            "[24/25][27/391] Loss_D: 0.4365 Loss_G: 2.7373 \n",
            "[24/25][28/391] Loss_D: 0.4585 Loss_G: 2.5897 \n",
            "[24/25][29/391] Loss_D: 0.4556 Loss_G: 2.7511 \n",
            "[24/25][30/391] Loss_D: 0.5854 Loss_G: 1.7697 \n",
            "[24/25][31/391] Loss_D: 0.4699 Loss_G: 3.3275 \n",
            "[24/25][32/391] Loss_D: 0.4032 Loss_G: 2.5121 \n",
            "[24/25][33/391] Loss_D: 0.4157 Loss_G: 2.1656 \n",
            "[24/25][34/391] Loss_D: 0.4472 Loss_G: 3.2575 \n",
            "[24/25][35/391] Loss_D: 0.3059 Loss_G: 3.0095 \n",
            "[24/25][36/391] Loss_D: 0.3672 Loss_G: 2.2523 \n",
            "[24/25][37/391] Loss_D: 0.4097 Loss_G: 2.9094 \n",
            "[24/25][38/391] Loss_D: 0.4265 Loss_G: 2.4082 \n",
            "[24/25][39/391] Loss_D: 0.4274 Loss_G: 2.4128 \n",
            "[24/25][40/391] Loss_D: 0.4336 Loss_G: 3.3704 \n",
            "[24/25][41/391] Loss_D: 0.7184 Loss_G: 1.3361 \n",
            "[24/25][42/391] Loss_D: 0.5148 Loss_G: 2.8645 \n",
            "[24/25][43/391] Loss_D: 0.3039 Loss_G: 3.6278 \n",
            "[24/25][44/391] Loss_D: 0.3510 Loss_G: 2.5837 \n",
            "[24/25][45/391] Loss_D: 0.2700 Loss_G: 2.7604 \n",
            "[24/25][46/391] Loss_D: 0.3537 Loss_G: 2.9831 \n",
            "[24/25][47/391] Loss_D: 0.3606 Loss_G: 2.4191 \n",
            "[24/25][48/391] Loss_D: 0.4615 Loss_G: 2.4782 \n",
            "[24/25][49/391] Loss_D: 0.2444 Loss_G: 3.3531 \n",
            "[24/25][50/391] Loss_D: 0.5850 Loss_G: 1.5704 \n",
            "[24/25][51/391] Loss_D: 0.4782 Loss_G: 3.0396 \n",
            "[24/25][52/391] Loss_D: 0.4037 Loss_G: 2.8176 \n",
            "[24/25][53/391] Loss_D: 0.4034 Loss_G: 2.7263 \n",
            "[24/25][54/391] Loss_D: 0.4074 Loss_G: 2.2182 \n",
            "[24/25][55/391] Loss_D: 0.5355 Loss_G: 3.2513 \n",
            "[24/25][56/391] Loss_D: 0.5082 Loss_G: 2.2712 \n",
            "[24/25][57/391] Loss_D: 0.4455 Loss_G: 2.5105 \n",
            "[24/25][58/391] Loss_D: 0.4590 Loss_G: 2.1144 \n",
            "[24/25][59/391] Loss_D: 0.4065 Loss_G: 2.6708 \n",
            "[24/25][60/391] Loss_D: 0.3553 Loss_G: 3.4692 \n",
            "[24/25][61/391] Loss_D: 0.4998 Loss_G: 1.9834 \n",
            "[24/25][62/391] Loss_D: 0.3848 Loss_G: 2.8279 \n",
            "[24/25][63/391] Loss_D: 0.3029 Loss_G: 2.9324 \n",
            "[24/25][64/391] Loss_D: 0.4485 Loss_G: 2.1025 \n",
            "[24/25][65/391] Loss_D: 0.4482 Loss_G: 2.3312 \n",
            "[24/25][66/391] Loss_D: 0.4971 Loss_G: 2.7271 \n",
            "[24/25][67/391] Loss_D: 0.3170 Loss_G: 2.7060 \n",
            "[24/25][68/391] Loss_D: 0.3538 Loss_G: 2.2560 \n",
            "[24/25][69/391] Loss_D: 0.3379 Loss_G: 3.0419 \n",
            "[24/25][70/391] Loss_D: 0.2643 Loss_G: 3.4331 \n",
            "[24/25][71/391] Loss_D: 0.4006 Loss_G: 1.8794 \n",
            "[24/25][72/391] Loss_D: 0.4397 Loss_G: 2.7915 \n",
            "[24/25][73/391] Loss_D: 0.2787 Loss_G: 3.4515 \n",
            "[24/25][74/391] Loss_D: 0.5825 Loss_G: 1.5677 \n",
            "[24/25][75/391] Loss_D: 0.5989 Loss_G: 2.5310 \n",
            "[24/25][76/391] Loss_D: 0.3906 Loss_G: 3.2762 \n",
            "[24/25][77/391] Loss_D: 0.5407 Loss_G: 1.6140 \n",
            "[24/25][78/391] Loss_D: 0.5111 Loss_G: 3.3107 \n",
            "[24/25][79/391] Loss_D: 0.4615 Loss_G: 2.1841 \n",
            "[24/25][80/391] Loss_D: 0.3977 Loss_G: 2.4313 \n",
            "[24/25][81/391] Loss_D: 0.3680 Loss_G: 3.1978 \n",
            "[24/25][82/391] Loss_D: 0.3583 Loss_G: 2.5898 \n",
            "[24/25][83/391] Loss_D: 0.3330 Loss_G: 2.2828 \n",
            "[24/25][84/391] Loss_D: 0.4216 Loss_G: 2.9300 \n",
            "[24/25][85/391] Loss_D: 0.3408 Loss_G: 2.6135 \n",
            "[24/25][86/391] Loss_D: 0.3408 Loss_G: 2.3270 \n",
            "[24/25][87/391] Loss_D: 0.3210 Loss_G: 3.0546 \n",
            "[24/25][88/391] Loss_D: 0.3828 Loss_G: 2.8285 \n",
            "[24/25][89/391] Loss_D: 0.3138 Loss_G: 2.3023 \n",
            "[24/25][90/391] Loss_D: 0.3407 Loss_G: 2.4484 \n",
            "[24/25][91/391] Loss_D: 0.3292 Loss_G: 3.4523 \n",
            "[24/25][92/391] Loss_D: 0.4571 Loss_G: 2.2721 \n",
            "[24/25][93/391] Loss_D: 0.3263 Loss_G: 2.8654 \n",
            "[24/25][94/391] Loss_D: 0.3588 Loss_G: 2.8329 \n",
            "[24/25][95/391] Loss_D: 0.2682 Loss_G: 2.6901 \n",
            "[24/25][96/391] Loss_D: 0.2845 Loss_G: 2.9634 \n",
            "[24/25][97/391] Loss_D: 0.3999 Loss_G: 2.4410 \n",
            "[24/25][98/391] Loss_D: 0.4164 Loss_G: 2.2505 \n",
            "[24/25][99/391] Loss_D: 0.4525 Loss_G: 2.5901 \n",
            "[24/25][100/391] Loss_D: 0.3587 Loss_G: 2.5461 \n",
            "saving the output\n",
            "[24/25][101/391] Loss_D: 0.3382 Loss_G: 2.8635 \n",
            "[24/25][102/391] Loss_D: 0.3964 Loss_G: 2.7541 \n",
            "[24/25][103/391] Loss_D: 0.3408 Loss_G: 2.4239 \n",
            "[24/25][104/391] Loss_D: 0.3862 Loss_G: 2.2481 \n",
            "[24/25][105/391] Loss_D: 0.3927 Loss_G: 3.1511 \n",
            "[24/25][106/391] Loss_D: 0.5385 Loss_G: 1.7001 \n",
            "[24/25][107/391] Loss_D: 0.4288 Loss_G: 3.8455 \n",
            "[24/25][108/391] Loss_D: 0.3407 Loss_G: 3.0294 \n",
            "[24/25][109/391] Loss_D: 0.3122 Loss_G: 2.4219 \n",
            "[24/25][110/391] Loss_D: 0.3244 Loss_G: 2.6823 \n",
            "[24/25][111/391] Loss_D: 0.4585 Loss_G: 2.2856 \n",
            "[24/25][112/391] Loss_D: 0.3748 Loss_G: 3.6633 \n",
            "[24/25][113/391] Loss_D: 0.2572 Loss_G: 3.0970 \n",
            "[24/25][114/391] Loss_D: 0.3601 Loss_G: 1.8740 \n",
            "[24/25][115/391] Loss_D: 0.3740 Loss_G: 3.2578 \n",
            "[24/25][116/391] Loss_D: 0.3383 Loss_G: 2.8028 \n",
            "[24/25][117/391] Loss_D: 0.2807 Loss_G: 3.1417 \n",
            "[24/25][118/391] Loss_D: 0.3584 Loss_G: 2.7035 \n",
            "[24/25][119/391] Loss_D: 0.6645 Loss_G: 0.8826 \n",
            "[24/25][120/391] Loss_D: 0.9807 Loss_G: 4.0391 \n",
            "[24/25][121/391] Loss_D: 0.7438 Loss_G: 0.3349 \n",
            "[24/25][122/391] Loss_D: 2.2248 Loss_G: 8.0539 \n",
            "[24/25][123/391] Loss_D: 3.3741 Loss_G: 0.6846 \n",
            "[24/25][124/391] Loss_D: 1.1097 Loss_G: 4.6437 \n",
            "[24/25][125/391] Loss_D: 1.2659 Loss_G: 0.4288 \n",
            "[24/25][126/391] Loss_D: 1.9267 Loss_G: 5.6067 \n",
            "[24/25][127/391] Loss_D: 1.8469 Loss_G: 1.2095 \n",
            "[24/25][128/391] Loss_D: 0.8911 Loss_G: 2.5861 \n",
            "[24/25][129/391] Loss_D: 0.6973 Loss_G: 2.8038 \n",
            "[24/25][130/391] Loss_D: 0.7369 Loss_G: 1.7646 \n",
            "[24/25][131/391] Loss_D: 0.6160 Loss_G: 2.8993 \n",
            "[24/25][132/391] Loss_D: 0.5964 Loss_G: 2.4008 \n",
            "[24/25][133/391] Loss_D: 0.6541 Loss_G: 1.8116 \n",
            "[24/25][134/391] Loss_D: 0.8305 Loss_G: 3.6837 \n",
            "[24/25][135/391] Loss_D: 0.9860 Loss_G: 0.7473 \n",
            "[24/25][136/391] Loss_D: 1.1352 Loss_G: 5.6918 \n",
            "[24/25][137/391] Loss_D: 1.5609 Loss_G: 0.4646 \n",
            "[24/25][138/391] Loss_D: 1.5141 Loss_G: 5.4760 \n",
            "[24/25][139/391] Loss_D: 1.0844 Loss_G: 0.8184 \n",
            "[24/25][140/391] Loss_D: 1.3439 Loss_G: 5.0886 \n",
            "[24/25][141/391] Loss_D: 1.3325 Loss_G: 0.6488 \n",
            "[24/25][142/391] Loss_D: 1.3842 Loss_G: 4.4828 \n",
            "[24/25][143/391] Loss_D: 0.6492 Loss_G: 2.4030 \n",
            "[24/25][144/391] Loss_D: 0.6208 Loss_G: 1.9761 \n",
            "[24/25][145/391] Loss_D: 0.7861 Loss_G: 3.8444 \n",
            "[24/25][146/391] Loss_D: 0.8794 Loss_G: 1.2488 \n",
            "[24/25][147/391] Loss_D: 0.9049 Loss_G: 3.1596 \n",
            "[24/25][148/391] Loss_D: 0.5881 Loss_G: 2.3681 \n",
            "[24/25][149/391] Loss_D: 0.5539 Loss_G: 2.7867 \n",
            "[24/25][150/391] Loss_D: 0.7312 Loss_G: 2.3533 \n",
            "[24/25][151/391] Loss_D: 0.5464 Loss_G: 2.3720 \n",
            "[24/25][152/391] Loss_D: 0.5223 Loss_G: 3.3145 \n",
            "[24/25][153/391] Loss_D: 0.6449 Loss_G: 1.4537 \n",
            "[24/25][154/391] Loss_D: 0.8502 Loss_G: 3.7047 \n",
            "[24/25][155/391] Loss_D: 0.5962 Loss_G: 1.9306 \n",
            "[24/25][156/391] Loss_D: 0.5441 Loss_G: 3.9171 \n",
            "[24/25][157/391] Loss_D: 0.7553 Loss_G: 1.2005 \n",
            "[24/25][158/391] Loss_D: 0.7081 Loss_G: 3.4253 \n",
            "[24/25][159/391] Loss_D: 0.6316 Loss_G: 1.7463 \n",
            "[24/25][160/391] Loss_D: 0.5888 Loss_G: 4.0450 \n",
            "[24/25][161/391] Loss_D: 0.3207 Loss_G: 2.9692 \n",
            "[24/25][162/391] Loss_D: 0.6213 Loss_G: 1.3405 \n",
            "[24/25][163/391] Loss_D: 0.6491 Loss_G: 3.8672 \n",
            "[24/25][164/391] Loss_D: 0.4766 Loss_G: 2.7322 \n",
            "[24/25][165/391] Loss_D: 0.3966 Loss_G: 2.1519 \n",
            "[24/25][166/391] Loss_D: 0.3980 Loss_G: 2.7432 \n",
            "[24/25][167/391] Loss_D: 0.4095 Loss_G: 2.8010 \n",
            "[24/25][168/391] Loss_D: 0.4280 Loss_G: 2.8510 \n",
            "[24/25][169/391] Loss_D: 0.3033 Loss_G: 2.9656 \n",
            "[24/25][170/391] Loss_D: 0.4496 Loss_G: 1.9856 \n",
            "[24/25][171/391] Loss_D: 0.4520 Loss_G: 2.2944 \n",
            "[24/25][172/391] Loss_D: 0.5948 Loss_G: 3.0098 \n",
            "[24/25][173/391] Loss_D: 0.4518 Loss_G: 2.7013 \n",
            "[24/25][174/391] Loss_D: 0.3831 Loss_G: 2.7352 \n",
            "[24/25][175/391] Loss_D: 0.5099 Loss_G: 1.8548 \n",
            "[24/25][176/391] Loss_D: 0.5424 Loss_G: 3.0466 \n",
            "[24/25][177/391] Loss_D: 0.4276 Loss_G: 2.5553 \n",
            "[24/25][178/391] Loss_D: 0.4661 Loss_G: 2.4928 \n",
            "[24/25][179/391] Loss_D: 0.3513 Loss_G: 2.9179 \n",
            "[24/25][180/391] Loss_D: 0.5053 Loss_G: 2.2324 \n",
            "[24/25][181/391] Loss_D: 0.4369 Loss_G: 2.7158 \n",
            "[24/25][182/391] Loss_D: 0.3398 Loss_G: 2.6887 \n",
            "[24/25][183/391] Loss_D: 0.3493 Loss_G: 3.2088 \n",
            "[24/25][184/391] Loss_D: 0.5760 Loss_G: 1.7676 \n",
            "[24/25][185/391] Loss_D: 0.4687 Loss_G: 3.2652 \n",
            "[24/25][186/391] Loss_D: 0.2980 Loss_G: 3.2598 \n",
            "[24/25][187/391] Loss_D: 0.4150 Loss_G: 2.0435 \n",
            "[24/25][188/391] Loss_D: 0.4832 Loss_G: 2.9108 \n",
            "[24/25][189/391] Loss_D: 0.3181 Loss_G: 3.2737 \n",
            "[24/25][190/391] Loss_D: 0.4286 Loss_G: 2.2487 \n",
            "[24/25][191/391] Loss_D: 0.4580 Loss_G: 2.4527 \n",
            "[24/25][192/391] Loss_D: 0.4027 Loss_G: 2.8834 \n",
            "[24/25][193/391] Loss_D: 0.3779 Loss_G: 2.7333 \n",
            "[24/25][194/391] Loss_D: 0.3843 Loss_G: 2.6594 \n",
            "[24/25][195/391] Loss_D: 0.3953 Loss_G: 2.4527 \n",
            "[24/25][196/391] Loss_D: 0.4460 Loss_G: 2.2695 \n",
            "[24/25][197/391] Loss_D: 0.2904 Loss_G: 2.9925 \n",
            "[24/25][198/391] Loss_D: 0.4297 Loss_G: 2.4720 \n",
            "[24/25][199/391] Loss_D: 0.4302 Loss_G: 2.8587 \n",
            "[24/25][200/391] Loss_D: 0.3601 Loss_G: 2.3587 \n",
            "saving the output\n",
            "[24/25][201/391] Loss_D: 0.5612 Loss_G: 3.1946 \n",
            "[24/25][202/391] Loss_D: 0.3125 Loss_G: 2.8243 \n",
            "[24/25][203/391] Loss_D: 0.3081 Loss_G: 2.7385 \n",
            "[24/25][204/391] Loss_D: 0.4307 Loss_G: 2.7403 \n",
            "[24/25][205/391] Loss_D: 0.5124 Loss_G: 1.7373 \n",
            "[24/25][206/391] Loss_D: 0.6943 Loss_G: 4.4213 \n",
            "[24/25][207/391] Loss_D: 0.4539 Loss_G: 2.5223 \n",
            "[24/25][208/391] Loss_D: 0.5804 Loss_G: 1.2356 \n",
            "[24/25][209/391] Loss_D: 0.6250 Loss_G: 4.0087 \n",
            "[24/25][210/391] Loss_D: 0.5968 Loss_G: 2.0910 \n",
            "[24/25][211/391] Loss_D: 0.6759 Loss_G: 3.1231 \n",
            "[24/25][212/391] Loss_D: 0.3979 Loss_G: 2.4419 \n",
            "[24/25][213/391] Loss_D: 0.4855 Loss_G: 2.7603 \n",
            "[24/25][214/391] Loss_D: 0.5063 Loss_G: 2.3677 \n",
            "[24/25][215/391] Loss_D: 0.4320 Loss_G: 3.4850 \n",
            "[24/25][216/391] Loss_D: 0.3921 Loss_G: 3.0243 \n",
            "[24/25][217/391] Loss_D: 0.4625 Loss_G: 1.8241 \n",
            "[24/25][218/391] Loss_D: 0.5753 Loss_G: 3.6038 \n",
            "[24/25][219/391] Loss_D: 0.5461 Loss_G: 1.8583 \n",
            "[24/25][220/391] Loss_D: 0.4703 Loss_G: 3.1672 \n",
            "[24/25][221/391] Loss_D: 0.3509 Loss_G: 3.3023 \n",
            "[24/25][222/391] Loss_D: 0.2514 Loss_G: 3.1310 \n",
            "[24/25][223/391] Loss_D: 0.3713 Loss_G: 2.1383 \n",
            "[24/25][224/391] Loss_D: 0.5586 Loss_G: 3.1376 \n",
            "[24/25][225/391] Loss_D: 0.3781 Loss_G: 3.1637 \n",
            "[24/25][226/391] Loss_D: 0.3803 Loss_G: 2.5181 \n",
            "[24/25][227/391] Loss_D: 0.4731 Loss_G: 1.7752 \n",
            "[24/25][228/391] Loss_D: 0.4629 Loss_G: 3.6303 \n",
            "[24/25][229/391] Loss_D: 0.3710 Loss_G: 2.9185 \n",
            "[24/25][230/391] Loss_D: 0.3310 Loss_G: 2.2565 \n",
            "[24/25][231/391] Loss_D: 0.3595 Loss_G: 3.0884 \n",
            "[24/25][232/391] Loss_D: 0.3936 Loss_G: 2.2792 \n",
            "[24/25][233/391] Loss_D: 0.3974 Loss_G: 2.6523 \n",
            "[24/25][234/391] Loss_D: 0.2519 Loss_G: 3.5519 \n",
            "[24/25][235/391] Loss_D: 0.3820 Loss_G: 2.3700 \n",
            "[24/25][236/391] Loss_D: 0.4715 Loss_G: 1.5928 \n",
            "[24/25][237/391] Loss_D: 0.6628 Loss_G: 4.0085 \n",
            "[24/25][238/391] Loss_D: 0.5197 Loss_G: 1.8728 \n",
            "[24/25][239/391] Loss_D: 0.4209 Loss_G: 3.0027 \n",
            "[24/25][240/391] Loss_D: 0.3849 Loss_G: 3.0308 \n",
            "[24/25][241/391] Loss_D: 0.4806 Loss_G: 1.9876 \n",
            "[24/25][242/391] Loss_D: 0.6845 Loss_G: 3.6927 \n",
            "[24/25][243/391] Loss_D: 0.4362 Loss_G: 2.4710 \n",
            "[24/25][244/391] Loss_D: 0.3054 Loss_G: 2.4652 \n",
            "[24/25][245/391] Loss_D: 0.3533 Loss_G: 3.2486 \n",
            "[24/25][246/391] Loss_D: 0.2712 Loss_G: 3.1653 \n",
            "[24/25][247/391] Loss_D: 0.5823 Loss_G: 1.1518 \n",
            "[24/25][248/391] Loss_D: 0.6788 Loss_G: 3.8163 \n",
            "[24/25][249/391] Loss_D: 0.5004 Loss_G: 1.6830 \n",
            "[24/25][250/391] Loss_D: 0.5979 Loss_G: 3.7849 \n",
            "[24/25][251/391] Loss_D: 0.4865 Loss_G: 2.0729 \n",
            "[24/25][252/391] Loss_D: 0.5927 Loss_G: 3.0540 \n",
            "[24/25][253/391] Loss_D: 0.4284 Loss_G: 2.2776 \n",
            "[24/25][254/391] Loss_D: 0.3638 Loss_G: 3.1339 \n",
            "[24/25][255/391] Loss_D: 0.3887 Loss_G: 2.6493 \n",
            "[24/25][256/391] Loss_D: 0.4585 Loss_G: 2.6502 \n",
            "[24/25][257/391] Loss_D: 0.3304 Loss_G: 2.5351 \n",
            "[24/25][258/391] Loss_D: 0.3450 Loss_G: 4.0280 \n",
            "[24/25][259/391] Loss_D: 0.8235 Loss_G: 1.0485 \n",
            "[24/25][260/391] Loss_D: 0.7270 Loss_G: 4.1991 \n",
            "[24/25][261/391] Loss_D: 0.6873 Loss_G: 0.8139 \n",
            "[24/25][262/391] Loss_D: 1.3873 Loss_G: 5.9690 \n",
            "[24/25][263/391] Loss_D: 1.0704 Loss_G: 1.1404 \n",
            "[24/25][264/391] Loss_D: 0.6771 Loss_G: 4.3943 \n",
            "[24/25][265/391] Loss_D: 0.5288 Loss_G: 2.5684 \n",
            "[24/25][266/391] Loss_D: 0.4277 Loss_G: 2.1060 \n",
            "[24/25][267/391] Loss_D: 0.5346 Loss_G: 4.8725 \n",
            "[24/25][268/391] Loss_D: 1.0509 Loss_G: 1.0720 \n",
            "[24/25][269/391] Loss_D: 0.8291 Loss_G: 4.3332 \n",
            "[24/25][270/391] Loss_D: 0.6443 Loss_G: 1.8564 \n",
            "[24/25][271/391] Loss_D: 0.4784 Loss_G: 3.0138 \n",
            "[24/25][272/391] Loss_D: 0.6192 Loss_G: 3.1624 \n",
            "[24/25][273/391] Loss_D: 0.6009 Loss_G: 1.9006 \n",
            "[24/25][274/391] Loss_D: 0.6142 Loss_G: 2.5134 \n",
            "[24/25][275/391] Loss_D: 0.2838 Loss_G: 3.6436 \n",
            "[24/25][276/391] Loss_D: 0.2828 Loss_G: 2.9515 \n",
            "[24/25][277/391] Loss_D: 0.6545 Loss_G: 1.1184 \n",
            "[24/25][278/391] Loss_D: 0.7491 Loss_G: 4.2161 \n",
            "[24/25][279/391] Loss_D: 0.3772 Loss_G: 3.2766 \n",
            "[24/25][280/391] Loss_D: 0.4689 Loss_G: 1.5139 \n",
            "[24/25][281/391] Loss_D: 0.6308 Loss_G: 4.0502 \n",
            "[24/25][282/391] Loss_D: 0.3679 Loss_G: 3.0539 \n",
            "[24/25][283/391] Loss_D: 0.3998 Loss_G: 2.0336 \n",
            "[24/25][284/391] Loss_D: 0.3575 Loss_G: 2.6898 \n",
            "[24/25][285/391] Loss_D: 0.3850 Loss_G: 3.0086 \n",
            "[24/25][286/391] Loss_D: 0.4067 Loss_G: 2.4952 \n",
            "[24/25][287/391] Loss_D: 0.2997 Loss_G: 3.1037 \n",
            "[24/25][288/391] Loss_D: 0.4652 Loss_G: 2.4125 \n",
            "[24/25][289/391] Loss_D: 0.3785 Loss_G: 2.6965 \n",
            "[24/25][290/391] Loss_D: 0.3773 Loss_G: 3.0789 \n",
            "[24/25][291/391] Loss_D: 0.3242 Loss_G: 2.7850 \n",
            "[24/25][292/391] Loss_D: 0.4453 Loss_G: 2.0689 \n",
            "[24/25][293/391] Loss_D: 0.4691 Loss_G: 2.9495 \n",
            "[24/25][294/391] Loss_D: 0.3535 Loss_G: 2.9154 \n",
            "[24/25][295/391] Loss_D: 0.3343 Loss_G: 3.0406 \n",
            "[24/25][296/391] Loss_D: 0.4055 Loss_G: 2.2162 \n",
            "[24/25][297/391] Loss_D: 0.3723 Loss_G: 3.2679 \n",
            "[24/25][298/391] Loss_D: 0.4345 Loss_G: 2.0549 \n",
            "[24/25][299/391] Loss_D: 0.3728 Loss_G: 3.2573 \n",
            "[24/25][300/391] Loss_D: 0.4335 Loss_G: 2.6202 \n",
            "saving the output\n",
            "[24/25][301/391] Loss_D: 0.3651 Loss_G: 2.0040 \n",
            "[24/25][302/391] Loss_D: 0.4363 Loss_G: 3.0675 \n",
            "[24/25][303/391] Loss_D: 0.4564 Loss_G: 2.3947 \n",
            "[24/25][304/391] Loss_D: 0.3843 Loss_G: 3.5881 \n",
            "[24/25][305/391] Loss_D: 0.5900 Loss_G: 1.1806 \n",
            "[24/25][306/391] Loss_D: 1.0123 Loss_G: 4.6095 \n",
            "[24/25][307/391] Loss_D: 0.5764 Loss_G: 1.6309 \n",
            "[24/25][308/391] Loss_D: 0.4828 Loss_G: 4.0491 \n",
            "[24/25][309/391] Loss_D: 0.5557 Loss_G: 1.5926 \n",
            "[24/25][310/391] Loss_D: 0.6906 Loss_G: 4.4688 \n",
            "[24/25][311/391] Loss_D: 0.6304 Loss_G: 1.0030 \n",
            "[24/25][312/391] Loss_D: 0.7408 Loss_G: 4.8328 \n",
            "[24/25][313/391] Loss_D: 1.0042 Loss_G: 0.5111 \n",
            "[24/25][314/391] Loss_D: 1.6404 Loss_G: 6.2246 \n",
            "[24/25][315/391] Loss_D: 1.3606 Loss_G: 0.3207 \n",
            "[24/25][316/391] Loss_D: 2.2612 Loss_G: 7.3371 \n",
            "[24/25][317/391] Loss_D: 2.3488 Loss_G: 0.1739 \n",
            "[24/25][318/391] Loss_D: 2.5905 Loss_G: 7.2003 \n",
            "[24/25][319/391] Loss_D: 4.9751 Loss_G: 0.2631 \n",
            "[24/25][320/391] Loss_D: 2.3933 Loss_G: 4.4113 \n",
            "[24/25][321/391] Loss_D: 1.6484 Loss_G: 1.4415 \n",
            "[24/25][322/391] Loss_D: 1.2316 Loss_G: 2.8948 \n",
            "[24/25][323/391] Loss_D: 0.6741 Loss_G: 3.8801 \n",
            "[24/25][324/391] Loss_D: 0.8997 Loss_G: 1.2821 \n",
            "[24/25][325/391] Loss_D: 1.0866 Loss_G: 4.6836 \n",
            "[24/25][326/391] Loss_D: 0.9971 Loss_G: 1.6667 \n",
            "[24/25][327/391] Loss_D: 1.0934 Loss_G: 2.9131 \n",
            "[24/25][328/391] Loss_D: 0.4621 Loss_G: 4.0078 \n",
            "[24/25][329/391] Loss_D: 0.8918 Loss_G: 1.1563 \n",
            "[24/25][330/391] Loss_D: 1.6826 Loss_G: 5.5139 \n",
            "[24/25][331/391] Loss_D: 0.5921 Loss_G: 4.0312 \n",
            "[24/25][332/391] Loss_D: 0.9628 Loss_G: 0.6532 \n",
            "[24/25][333/391] Loss_D: 1.7121 Loss_G: 4.5730 \n",
            "[24/25][334/391] Loss_D: 0.5481 Loss_G: 3.1403 \n",
            "[24/25][335/391] Loss_D: 0.5754 Loss_G: 1.3119 \n",
            "[24/25][336/391] Loss_D: 1.1275 Loss_G: 5.1988 \n",
            "[24/25][337/391] Loss_D: 1.3294 Loss_G: 1.7660 \n",
            "[24/25][338/391] Loss_D: 0.6275 Loss_G: 2.6678 \n",
            "[24/25][339/391] Loss_D: 0.6898 Loss_G: 2.6891 \n",
            "[24/25][340/391] Loss_D: 0.6012 Loss_G: 2.5222 \n",
            "[24/25][341/391] Loss_D: 0.7049 Loss_G: 3.2718 \n",
            "[24/25][342/391] Loss_D: 0.7911 Loss_G: 1.4063 \n",
            "[24/25][343/391] Loss_D: 0.7763 Loss_G: 3.9562 \n",
            "[24/25][344/391] Loss_D: 0.6545 Loss_G: 2.2459 \n",
            "[24/25][345/391] Loss_D: 0.5923 Loss_G: 2.8045 \n",
            "[24/25][346/391] Loss_D: 0.6776 Loss_G: 2.9804 \n",
            "[24/25][347/391] Loss_D: 0.4849 Loss_G: 3.4479 \n",
            "[24/25][348/391] Loss_D: 0.6248 Loss_G: 1.9949 \n",
            "[24/25][349/391] Loss_D: 0.5915 Loss_G: 2.5119 \n",
            "[24/25][350/391] Loss_D: 0.3706 Loss_G: 3.0859 \n",
            "[24/25][351/391] Loss_D: 0.5193 Loss_G: 3.1024 \n",
            "[24/25][352/391] Loss_D: 0.4372 Loss_G: 3.0147 \n",
            "[24/25][353/391] Loss_D: 0.4338 Loss_G: 2.5510 \n",
            "[24/25][354/391] Loss_D: 0.4650 Loss_G: 2.7649 \n",
            "[24/25][355/391] Loss_D: 0.4900 Loss_G: 2.4284 \n",
            "[24/25][356/391] Loss_D: 0.5222 Loss_G: 2.4885 \n",
            "[24/25][357/391] Loss_D: 0.3888 Loss_G: 2.9815 \n",
            "[24/25][358/391] Loss_D: 0.4233 Loss_G: 2.9879 \n",
            "[24/25][359/391] Loss_D: 0.4885 Loss_G: 2.4810 \n",
            "[24/25][360/391] Loss_D: 0.5459 Loss_G: 2.9843 \n",
            "[24/25][361/391] Loss_D: 0.4978 Loss_G: 2.2420 \n",
            "[24/25][362/391] Loss_D: 0.3895 Loss_G: 3.1656 \n",
            "[24/25][363/391] Loss_D: 0.4479 Loss_G: 2.4236 \n",
            "[24/25][364/391] Loss_D: 0.4828 Loss_G: 3.4871 \n",
            "[24/25][365/391] Loss_D: 0.6647 Loss_G: 1.6569 \n",
            "[24/25][366/391] Loss_D: 0.4788 Loss_G: 3.2396 \n",
            "[24/25][367/391] Loss_D: 0.4300 Loss_G: 2.7714 \n",
            "[24/25][368/391] Loss_D: 0.5321 Loss_G: 2.8456 \n",
            "[24/25][369/391] Loss_D: 0.5549 Loss_G: 1.9387 \n",
            "[24/25][370/391] Loss_D: 0.6892 Loss_G: 3.5477 \n",
            "[24/25][371/391] Loss_D: 0.6337 Loss_G: 2.0905 \n",
            "[24/25][372/391] Loss_D: 0.5464 Loss_G: 2.4363 \n",
            "[24/25][373/391] Loss_D: 0.4672 Loss_G: 2.8437 \n",
            "[24/25][374/391] Loss_D: 0.4936 Loss_G: 2.5302 \n",
            "[24/25][375/391] Loss_D: 0.3462 Loss_G: 3.2427 \n",
            "[24/25][376/391] Loss_D: 0.2659 Loss_G: 3.0915 \n",
            "[24/25][377/391] Loss_D: 0.3635 Loss_G: 2.5691 \n",
            "[24/25][378/391] Loss_D: 0.4375 Loss_G: 3.0403 \n",
            "[24/25][379/391] Loss_D: 0.2880 Loss_G: 3.0148 \n",
            "[24/25][380/391] Loss_D: 0.3442 Loss_G: 2.0954 \n",
            "[24/25][381/391] Loss_D: 0.6354 Loss_G: 3.3396 \n",
            "[24/25][382/391] Loss_D: 0.3224 Loss_G: 3.0994 \n",
            "[24/25][383/391] Loss_D: 0.4093 Loss_G: 1.9255 \n",
            "[24/25][384/391] Loss_D: 0.4839 Loss_G: 3.5820 \n",
            "[24/25][385/391] Loss_D: 0.7575 Loss_G: 1.6887 \n",
            "[24/25][386/391] Loss_D: 0.4793 Loss_G: 3.1123 \n",
            "[24/25][387/391] Loss_D: 0.3992 Loss_G: 2.8556 \n",
            "[24/25][388/391] Loss_D: 0.3501 Loss_G: 2.7229 \n",
            "[24/25][389/391] Loss_D: 0.4329 Loss_G: 2.1469 \n",
            "[24/25][390/391] Loss_D: 0.5006 Loss_G: 3.4736 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr8WmW4e7XLg",
        "outputId": "ab28806e-a59e-49b1-b448-e6ad912ee60f"
      },
      "source": [
        "!git clone https://github.com/sbarratt/inception-score-pytorch.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'inception-score-pytorch'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 43 (delta 0), reused 1 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qtoUg-H3ZsQ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('./inception-score-pytorch')\n",
        "from inception_score import inception_score"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "6be1cb7b22184708aeea0b2ab9bf4089",
            "08534094490f455e8016f800e6837ff9",
            "cccea77658ba4b0d85aa43f0780e7267",
            "63090f4d6db14eefa428801589ba1401",
            "0a2124f67552473e9fad3ffef2456124",
            "a78009aad69e4d1dbdeec3926a82795c",
            "9fd673ee68f04698bbe4b195d1a84f3f",
            "091aed46cde54a228dcd406a018a733d"
          ]
        },
        "id": "4JGUUsWu7qMD",
        "outputId": "4e39d81b-d371-4c8d-bf1c-9b6b79b76ede"
      },
      "source": [
        "\n",
        "print(inception_score(ld, cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6be1cb7b22184708aeea0b2ab9bf4089",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108949747.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "./inception-score-pytorch/inception_score.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x).data.cpu().numpy()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(9.672786661200819, 0.1499139460804442)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "6ede85a895f64e68847cdeee31644224",
            "3048ee5f602d403f94620b4233314bd9",
            "f835bced574d4dc59252a0136d53c7f9",
            "060b9239fd9a4a5882f40dd501c9d6d6",
            "c39dcec62aab46f3982252bc714ad0a4",
            "a95023631fa44ecdb89ef2644d9df68d",
            "c0b2b4f590fc470f9e490ea46bb4c6c3",
            "363b8ed2ffe54e3b9df80743115ce655"
          ]
        },
        "id": "uCQ-tQzJ-HIc",
        "outputId": "7c1d8fea-e1f3-4f04-fc01-10cdc9a1fa40"
      },
      "source": [
        "eval_images = []\n",
        "for i in range(600):\n",
        "    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    with torch.no_grad():\n",
        "        fake = netG(noise).detach().cpu()\n",
        "    eval_images += fake\n",
        "\n",
        "print(len(eval_images))\n",
        "print(eval_images[0].shape)\n",
        "print(inception_score(eval_images, cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48000\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ede85a895f64e68847cdeee31644224",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108949747.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "./inception-score-pytorch/inception_score.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x).data.cpu().numpy()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4.673854281750242, 0.02680361664162138)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiKI-3KgVD-y"
      },
      "source": [
        "Dual Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_2WSmPPnHWA"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxx0Xk7WVG_3",
        "outputId": "0389b0b5-2094-4033-89d9-9bd26e170289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "netD1 = Discriminator(ngpu).to(device)\n",
        "netD1.apply(weights_init)\n",
        "print(netD1)\n",
        "\n",
        "netD2 = Discriminator(ngpu).to(device)\n",
        "netD2.apply(weights_init)\n",
        "print(netD2)\n",
        "\n",
        "netG0 = Generator(ngpu).to(device)\n",
        "netG0.apply(weights_init)\n",
        "#load weights to test the model\n",
        "#netG.load_state_dict(torch.load('weights/netG_epoch_24.pth'))\n",
        "print(netG0)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odaSU6ZgX2qh"
      },
      "source": [
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD1 = optim.Adam(netD1.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerD2 = optim.Adam(netD2.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizerG0 = optim.Adam(netG0.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "fixed_noise = torch.randn(128, nz, 1, 1, device=device)\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "niter = 25\n",
        "g_loss = []\n",
        "d_loss = []"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpe1j8YGXUaD",
        "outputId": "9e96b031-0513-4621-e0f3-2d534b6932f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "for epoch in range(niter):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        # train with real\n",
        "        real_cpu = data[0].to(device)\n",
        "        batch_size = real_cpu.size(0)\n",
        "        rlabel = torch.full((batch_size,), real_label, dtype=torch.float32, device=device)\n",
        "        flabel = torch.full((batch_size,), fake_label, dtype=torch.float32, device=device)\n",
        "        \n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake = netG0(noise)\n",
        "\n",
        "        netD1.zero_grad()\n",
        "        errD1 = criterion(netD1(real_cpu), rlabel) + criterion(netD1(fake.detach()), flabel)\n",
        "        errD1.backward()\n",
        "        optimizerD1.step()\n",
        "\n",
        "        netD2.zero_grad()\n",
        "        errD2 = criterion(netD2(real_cpu), rlabel) + criterion(netD2(fake.detach()), flabel)\n",
        "        errD2.backward()\n",
        "        optimizerD2.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG0.zero_grad()\n",
        "\n",
        "        errG0 = criterion(netD1(fake), rlabel) + criterion(netD2(fake), rlabel)\n",
        "        errG0.backward()\n",
        "        optimizerG0.step()\n",
        "\n",
        "\n",
        "        print('[%d/%d][%d/%d] Loss_D1: %.4f Loss_D2: %.4f Loss_G: %.4f' % (epoch, niter, i, len(dataloader), errD1.item(),errD2.item(), errG0.item()))\n",
        "        \n",
        "        #save the output\n",
        "        if i % 100 == 0:\n",
        "            print('saving the output')\n",
        "            vutils.save_image(real_cpu,'./drive/MyDrive/t_DCGAN/output/2d_real_samples.png',normalize=True)\n",
        "            fake = netG0(fixed_noise)\n",
        "            vutils.save_image(fake.detach(),'./drive/MyDrive/t_DCGAN/output/2d_fake_samples_epoch_%03d.png' % (epoch),normalize=True)\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "[12/25][133/391] Loss_D1: 0.3677 Loss_D2: 0.6569 Loss_G: 4.5343\n",
            "[12/25][134/391] Loss_D1: 0.3502 Loss_D2: 0.6002 Loss_G: 5.2384\n",
            "[12/25][135/391] Loss_D1: 0.3567 Loss_D2: 0.4381 Loss_G: 5.7672\n",
            "[12/25][136/391] Loss_D1: 0.4205 Loss_D2: 0.5317 Loss_G: 4.4359\n",
            "[12/25][137/391] Loss_D1: 0.4773 Loss_D2: 0.5600 Loss_G: 6.6047\n",
            "[12/25][138/391] Loss_D1: 0.4481 Loss_D2: 0.5508 Loss_G: 3.5753\n",
            "[12/25][139/391] Loss_D1: 0.4925 Loss_D2: 0.4725 Loss_G: 6.3655\n",
            "[12/25][140/391] Loss_D1: 0.7016 Loss_D2: 0.5940 Loss_G: 3.7486\n",
            "[12/25][141/391] Loss_D1: 0.9029 Loss_D2: 0.4382 Loss_G: 6.8877\n",
            "[12/25][142/391] Loss_D1: 0.7239 Loss_D2: 0.5388 Loss_G: 4.2192\n",
            "[12/25][143/391] Loss_D1: 0.2769 Loss_D2: 0.4204 Loss_G: 6.3268\n",
            "[12/25][144/391] Loss_D1: 0.5037 Loss_D2: 0.6177 Loss_G: 4.4501\n",
            "[12/25][145/391] Loss_D1: 0.3865 Loss_D2: 0.4743 Loss_G: 6.2276\n",
            "[12/25][146/391] Loss_D1: 0.5777 Loss_D2: 0.4448 Loss_G: 4.6649\n",
            "[12/25][147/391] Loss_D1: 0.5215 Loss_D2: 0.4997 Loss_G: 5.3438\n",
            "[12/25][148/391] Loss_D1: 0.5611 Loss_D2: 0.5524 Loss_G: 4.8472\n",
            "[12/25][149/391] Loss_D1: 0.4023 Loss_D2: 0.5058 Loss_G: 6.7154\n",
            "[12/25][150/391] Loss_D1: 0.4113 Loss_D2: 0.3486 Loss_G: 4.8622\n",
            "[12/25][151/391] Loss_D1: 0.4082 Loss_D2: 0.5532 Loss_G: 5.2905\n",
            "[12/25][152/391] Loss_D1: 0.5644 Loss_D2: 0.7140 Loss_G: 6.0500\n",
            "[12/25][153/391] Loss_D1: 0.4720 Loss_D2: 0.9468 Loss_G: 4.1520\n",
            "[12/25][154/391] Loss_D1: 0.3051 Loss_D2: 0.8100 Loss_G: 6.6169\n",
            "[12/25][155/391] Loss_D1: 0.4210 Loss_D2: 0.3836 Loss_G: 6.0512\n",
            "[12/25][156/391] Loss_D1: 0.2089 Loss_D2: 0.2992 Loss_G: 5.8046\n",
            "[12/25][157/391] Loss_D1: 0.4596 Loss_D2: 0.4365 Loss_G: 4.3327\n",
            "[12/25][158/391] Loss_D1: 0.6299 Loss_D2: 0.4868 Loss_G: 6.0704\n",
            "[12/25][159/391] Loss_D1: 0.6389 Loss_D2: 0.5781 Loss_G: 4.8324\n",
            "[12/25][160/391] Loss_D1: 0.3937 Loss_D2: 0.4491 Loss_G: 6.5417\n",
            "[12/25][161/391] Loss_D1: 0.2411 Loss_D2: 0.5861 Loss_G: 5.4737\n",
            "[12/25][162/391] Loss_D1: 0.3859 Loss_D2: 0.5825 Loss_G: 5.5098\n",
            "[12/25][163/391] Loss_D1: 0.6381 Loss_D2: 0.5982 Loss_G: 6.0619\n",
            "[12/25][164/391] Loss_D1: 0.4247 Loss_D2: 0.3589 Loss_G: 5.1510\n",
            "[12/25][165/391] Loss_D1: 0.3129 Loss_D2: 0.5845 Loss_G: 6.7300\n",
            "[12/25][166/391] Loss_D1: 0.2906 Loss_D2: 0.5054 Loss_G: 5.7829\n",
            "[12/25][167/391] Loss_D1: 0.4804 Loss_D2: 0.3987 Loss_G: 4.6919\n",
            "[12/25][168/391] Loss_D1: 0.4699 Loss_D2: 0.3409 Loss_G: 5.6070\n",
            "[12/25][169/391] Loss_D1: 0.4430 Loss_D2: 0.2795 Loss_G: 7.7671\n",
            "[12/25][170/391] Loss_D1: 0.4666 Loss_D2: 0.5079 Loss_G: 4.9706\n",
            "[12/25][171/391] Loss_D1: 0.3575 Loss_D2: 0.5735 Loss_G: 4.5540\n",
            "[12/25][172/391] Loss_D1: 0.4916 Loss_D2: 0.4992 Loss_G: 6.4781\n",
            "[12/25][173/391] Loss_D1: 0.5761 Loss_D2: 0.5624 Loss_G: 5.7243\n",
            "[12/25][174/391] Loss_D1: 0.4039 Loss_D2: 0.6915 Loss_G: 4.8514\n",
            "[12/25][175/391] Loss_D1: 0.4000 Loss_D2: 0.7548 Loss_G: 6.8033\n",
            "[12/25][176/391] Loss_D1: 0.3635 Loss_D2: 0.6077 Loss_G: 4.5542\n",
            "[12/25][177/391] Loss_D1: 0.5954 Loss_D2: 0.8210 Loss_G: 6.0294\n",
            "[12/25][178/391] Loss_D1: 0.5905 Loss_D2: 0.8811 Loss_G: 5.8137\n",
            "[12/25][179/391] Loss_D1: 0.5920 Loss_D2: 0.4943 Loss_G: 4.6287\n",
            "[12/25][180/391] Loss_D1: 0.4576 Loss_D2: 0.3732 Loss_G: 7.4493\n",
            "[12/25][181/391] Loss_D1: 0.4802 Loss_D2: 0.5808 Loss_G: 3.5627\n",
            "[12/25][182/391] Loss_D1: 0.5951 Loss_D2: 0.6340 Loss_G: 7.4657\n",
            "[12/25][183/391] Loss_D1: 0.3813 Loss_D2: 0.3098 Loss_G: 6.3235\n",
            "[12/25][184/391] Loss_D1: 0.5527 Loss_D2: 0.5436 Loss_G: 4.8650\n",
            "[12/25][185/391] Loss_D1: 0.6651 Loss_D2: 0.5931 Loss_G: 4.0275\n",
            "[12/25][186/391] Loss_D1: 1.0227 Loss_D2: 0.5405 Loss_G: 6.7261\n",
            "[12/25][187/391] Loss_D1: 0.7017 Loss_D2: 0.5050 Loss_G: 3.7780\n",
            "[12/25][188/391] Loss_D1: 1.5342 Loss_D2: 0.3841 Loss_G: 11.4270\n",
            "[12/25][189/391] Loss_D1: 2.7751 Loss_D2: 0.5217 Loss_G: 2.0601\n",
            "[12/25][190/391] Loss_D1: 4.4794 Loss_D2: 0.5618 Loss_G: 12.5605\n",
            "[12/25][191/391] Loss_D1: 3.1863 Loss_D2: 0.5120 Loss_G: 3.0226\n",
            "[12/25][192/391] Loss_D1: 1.3660 Loss_D2: 0.5386 Loss_G: 9.1724\n",
            "[12/25][193/391] Loss_D1: 2.3821 Loss_D2: 0.4418 Loss_G: 2.8407\n",
            "[12/25][194/391] Loss_D1: 2.2892 Loss_D2: 0.5144 Loss_G: 7.5439\n",
            "[12/25][195/391] Loss_D1: 0.6278 Loss_D2: 0.6082 Loss_G: 5.0342\n",
            "[12/25][196/391] Loss_D1: 0.7038 Loss_D2: 0.4622 Loss_G: 3.7667\n",
            "[12/25][197/391] Loss_D1: 1.7881 Loss_D2: 0.5841 Loss_G: 9.9702\n",
            "[12/25][198/391] Loss_D1: 3.7753 Loss_D2: 0.6492 Loss_G: 4.4103\n",
            "[12/25][199/391] Loss_D1: 1.6010 Loss_D2: 0.5375 Loss_G: 5.9557\n",
            "[12/25][200/391] Loss_D1: 1.4761 Loss_D2: 0.5023 Loss_G: 2.9380\n",
            "saving the output\n",
            "[12/25][201/391] Loss_D1: 2.2498 Loss_D2: 0.3558 Loss_G: 9.3470\n",
            "[12/25][202/391] Loss_D1: 1.2651 Loss_D2: 0.5350 Loss_G: 4.8018\n",
            "[12/25][203/391] Loss_D1: 0.5211 Loss_D2: 0.4250 Loss_G: 5.2082\n",
            "[12/25][204/391] Loss_D1: 0.6809 Loss_D2: 0.4312 Loss_G: 7.1896\n",
            "[12/25][205/391] Loss_D1: 0.9176 Loss_D2: 0.4886 Loss_G: 4.5616\n",
            "[12/25][206/391] Loss_D1: 0.9515 Loss_D2: 0.5189 Loss_G: 6.2793\n",
            "[12/25][207/391] Loss_D1: 0.6638 Loss_D2: 0.4537 Loss_G: 5.4874\n",
            "[12/25][208/391] Loss_D1: 0.7526 Loss_D2: 0.6258 Loss_G: 5.1993\n",
            "[12/25][209/391] Loss_D1: 0.6674 Loss_D2: 0.3867 Loss_G: 6.0986\n",
            "[12/25][210/391] Loss_D1: 0.4047 Loss_D2: 0.4445 Loss_G: 4.7110\n",
            "[12/25][211/391] Loss_D1: 0.4564 Loss_D2: 0.5415 Loss_G: 6.3339\n",
            "[12/25][212/391] Loss_D1: 0.4678 Loss_D2: 0.5462 Loss_G: 4.8463\n",
            "[12/25][213/391] Loss_D1: 0.4026 Loss_D2: 0.4261 Loss_G: 5.6572\n",
            "[12/25][214/391] Loss_D1: 0.6147 Loss_D2: 0.7020 Loss_G: 6.1672\n",
            "[12/25][215/391] Loss_D1: 0.4494 Loss_D2: 0.5065 Loss_G: 4.7929\n",
            "[12/25][216/391] Loss_D1: 0.7468 Loss_D2: 0.6894 Loss_G: 4.5047\n",
            "[12/25][217/391] Loss_D1: 0.3699 Loss_D2: 0.5378 Loss_G: 7.2817\n",
            "[12/25][218/391] Loss_D1: 0.3558 Loss_D2: 0.7205 Loss_G: 4.3003\n",
            "[12/25][219/391] Loss_D1: 0.6918 Loss_D2: 1.0129 Loss_G: 5.8149\n",
            "[12/25][220/391] Loss_D1: 0.7931 Loss_D2: 0.7126 Loss_G: 5.2855\n",
            "[12/25][221/391] Loss_D1: 0.3555 Loss_D2: 1.0263 Loss_G: 8.9726\n",
            "[12/25][222/391] Loss_D1: 0.6806 Loss_D2: 0.8959 Loss_G: 1.8059\n",
            "[12/25][223/391] Loss_D1: 1.0245 Loss_D2: 1.2604 Loss_G: 9.4889\n",
            "[12/25][224/391] Loss_D1: 0.4608 Loss_D2: 1.1814 Loss_G: 3.8964\n",
            "[12/25][225/391] Loss_D1: 0.4894 Loss_D2: 0.8632 Loss_G: 6.3591\n",
            "[12/25][226/391] Loss_D1: 0.4218 Loss_D2: 0.6126 Loss_G: 6.3516\n",
            "[12/25][227/391] Loss_D1: 0.5635 Loss_D2: 0.5201 Loss_G: 5.1748\n",
            "[12/25][228/391] Loss_D1: 0.4281 Loss_D2: 0.4622 Loss_G: 6.1643\n",
            "[12/25][229/391] Loss_D1: 0.4494 Loss_D2: 0.7267 Loss_G: 3.8692\n",
            "[12/25][230/391] Loss_D1: 0.3577 Loss_D2: 1.0266 Loss_G: 7.0345\n",
            "[12/25][231/391] Loss_D1: 0.4177 Loss_D2: 0.6758 Loss_G: 4.1411\n",
            "[12/25][232/391] Loss_D1: 0.5612 Loss_D2: 0.4977 Loss_G: 5.6881\n",
            "[12/25][233/391] Loss_D1: 0.5258 Loss_D2: 0.4854 Loss_G: 5.3769\n",
            "[12/25][234/391] Loss_D1: 0.3428 Loss_D2: 0.4515 Loss_G: 5.4996\n",
            "[12/25][235/391] Loss_D1: 0.5142 Loss_D2: 0.5626 Loss_G: 4.6734\n",
            "[12/25][236/391] Loss_D1: 0.3194 Loss_D2: 0.4314 Loss_G: 6.8554\n",
            "[12/25][237/391] Loss_D1: 0.5061 Loss_D2: 0.4591 Loss_G: 4.3093\n",
            "[12/25][238/391] Loss_D1: 0.6054 Loss_D2: 0.8229 Loss_G: 4.7879\n",
            "[12/25][239/391] Loss_D1: 0.4761 Loss_D2: 0.5911 Loss_G: 5.8623\n",
            "[12/25][240/391] Loss_D1: 0.5409 Loss_D2: 0.6317 Loss_G: 4.5942\n",
            "[12/25][241/391] Loss_D1: 0.6439 Loss_D2: 0.8209 Loss_G: 5.8753\n",
            "[12/25][242/391] Loss_D1: 0.5767 Loss_D2: 0.9485 Loss_G: 3.4786\n",
            "[12/25][243/391] Loss_D1: 0.5292 Loss_D2: 1.6839 Loss_G: 8.8547\n",
            "[12/25][244/391] Loss_D1: 0.3740 Loss_D2: 1.4895 Loss_G: 3.5928\n",
            "[12/25][245/391] Loss_D1: 0.3550 Loss_D2: 1.2295 Loss_G: 8.9297\n",
            "[12/25][246/391] Loss_D1: 0.3429 Loss_D2: 1.5672 Loss_G: 4.2040\n",
            "[12/25][247/391] Loss_D1: 0.4556 Loss_D2: 1.3191 Loss_G: 6.8113\n",
            "[12/25][248/391] Loss_D1: 0.4297 Loss_D2: 0.6212 Loss_G: 5.3408\n",
            "[12/25][249/391] Loss_D1: 0.4709 Loss_D2: 0.4692 Loss_G: 4.1794\n",
            "[12/25][250/391] Loss_D1: 0.4813 Loss_D2: 1.1042 Loss_G: 8.0751\n",
            "[12/25][251/391] Loss_D1: 0.3617 Loss_D2: 1.7700 Loss_G: 3.4338\n",
            "[12/25][252/391] Loss_D1: 0.3841 Loss_D2: 1.8280 Loss_G: 9.9814\n",
            "[12/25][253/391] Loss_D1: 0.7899 Loss_D2: 1.1864 Loss_G: 2.9315\n",
            "[12/25][254/391] Loss_D1: 0.7385 Loss_D2: 0.6427 Loss_G: 6.5190\n",
            "[12/25][255/391] Loss_D1: 0.5673 Loss_D2: 0.5576 Loss_G: 4.8173\n",
            "[12/25][256/391] Loss_D1: 0.2528 Loss_D2: 0.5483 Loss_G: 6.1909\n",
            "[12/25][257/391] Loss_D1: 0.4982 Loss_D2: 1.0349 Loss_G: 3.7617\n",
            "[12/25][258/391] Loss_D1: 0.3510 Loss_D2: 1.2165 Loss_G: 7.6622\n",
            "[12/25][259/391] Loss_D1: 0.3969 Loss_D2: 0.9680 Loss_G: 4.3338\n",
            "[12/25][260/391] Loss_D1: 0.4083 Loss_D2: 0.7363 Loss_G: 4.9404\n",
            "[12/25][261/391] Loss_D1: 0.4385 Loss_D2: 0.8965 Loss_G: 6.5792\n",
            "[12/25][262/391] Loss_D1: 0.4443 Loss_D2: 0.4759 Loss_G: 5.1105\n",
            "[12/25][263/391] Loss_D1: 0.5432 Loss_D2: 0.6028 Loss_G: 4.9044\n",
            "[12/25][264/391] Loss_D1: 0.4978 Loss_D2: 0.8014 Loss_G: 7.0151\n",
            "[12/25][265/391] Loss_D1: 0.4063 Loss_D2: 1.0233 Loss_G: 3.4201\n",
            "[12/25][266/391] Loss_D1: 0.4972 Loss_D2: 0.9169 Loss_G: 6.1713\n",
            "[12/25][267/391] Loss_D1: 0.2659 Loss_D2: 0.6288 Loss_G: 6.2411\n",
            "[12/25][268/391] Loss_D1: 0.3675 Loss_D2: 0.4574 Loss_G: 4.7721\n",
            "[12/25][269/391] Loss_D1: 0.4374 Loss_D2: 0.6722 Loss_G: 4.2583\n",
            "[12/25][270/391] Loss_D1: 0.2788 Loss_D2: 0.8128 Loss_G: 7.7371\n",
            "[12/25][271/391] Loss_D1: 0.5273 Loss_D2: 0.8803 Loss_G: 3.7031\n",
            "[12/25][272/391] Loss_D1: 0.3399 Loss_D2: 0.7588 Loss_G: 5.4957\n",
            "[12/25][273/391] Loss_D1: 0.3135 Loss_D2: 0.6788 Loss_G: 5.6821\n",
            "[12/25][274/391] Loss_D1: 0.3486 Loss_D2: 0.4132 Loss_G: 4.9322\n",
            "[12/25][275/391] Loss_D1: 0.4861 Loss_D2: 0.4613 Loss_G: 5.8129\n",
            "[12/25][276/391] Loss_D1: 0.3376 Loss_D2: 0.4963 Loss_G: 5.1866\n",
            "[12/25][277/391] Loss_D1: 0.2710 Loss_D2: 0.6533 Loss_G: 4.9641\n",
            "[12/25][278/391] Loss_D1: 0.6199 Loss_D2: 0.5550 Loss_G: 5.7700\n",
            "[12/25][279/391] Loss_D1: 0.3175 Loss_D2: 0.5283 Loss_G: 4.8709\n",
            "[12/25][280/391] Loss_D1: 0.4791 Loss_D2: 0.5235 Loss_G: 5.9748\n",
            "[12/25][281/391] Loss_D1: 0.3945 Loss_D2: 0.6023 Loss_G: 4.9859\n",
            "[12/25][282/391] Loss_D1: 0.3350 Loss_D2: 0.7214 Loss_G: 3.5589\n",
            "[12/25][283/391] Loss_D1: 0.4025 Loss_D2: 0.6927 Loss_G: 6.0613\n",
            "[12/25][284/391] Loss_D1: 0.3696 Loss_D2: 0.5109 Loss_G: 5.3330\n",
            "[12/25][285/391] Loss_D1: 0.2073 Loss_D2: 0.5340 Loss_G: 5.2005\n",
            "[12/25][286/391] Loss_D1: 0.2657 Loss_D2: 0.5463 Loss_G: 6.5141\n",
            "[12/25][287/391] Loss_D1: 0.5738 Loss_D2: 0.6855 Loss_G: 3.6468\n",
            "[12/25][288/391] Loss_D1: 0.4854 Loss_D2: 0.4809 Loss_G: 5.2135\n",
            "[12/25][289/391] Loss_D1: 0.3780 Loss_D2: 0.6391 Loss_G: 5.3041\n",
            "[12/25][290/391] Loss_D1: 0.3599 Loss_D2: 0.6373 Loss_G: 4.2924\n",
            "[12/25][291/391] Loss_D1: 0.3641 Loss_D2: 0.7676 Loss_G: 6.0151\n",
            "[12/25][292/391] Loss_D1: 0.3351 Loss_D2: 0.5619 Loss_G: 5.3196\n",
            "[12/25][293/391] Loss_D1: 0.4076 Loss_D2: 0.5928 Loss_G: 4.4797\n",
            "[12/25][294/391] Loss_D1: 0.5117 Loss_D2: 1.2110 Loss_G: 6.4693\n",
            "[12/25][295/391] Loss_D1: 0.5444 Loss_D2: 1.1917 Loss_G: 5.6173\n",
            "[12/25][296/391] Loss_D1: 0.4647 Loss_D2: 0.6942 Loss_G: 5.0929\n",
            "[12/25][297/391] Loss_D1: 0.3223 Loss_D2: 0.4513 Loss_G: 5.7071\n",
            "[12/25][298/391] Loss_D1: 0.4731 Loss_D2: 0.4857 Loss_G: 5.8047\n",
            "[12/25][299/391] Loss_D1: 0.3282 Loss_D2: 0.6568 Loss_G: 6.0155\n",
            "[12/25][300/391] Loss_D1: 0.3495 Loss_D2: 0.4435 Loss_G: 4.8632\n",
            "saving the output\n",
            "[12/25][301/391] Loss_D1: 0.4994 Loss_D2: 0.4788 Loss_G: 5.2841\n",
            "[12/25][302/391] Loss_D1: 0.4473 Loss_D2: 0.5597 Loss_G: 5.6458\n",
            "[12/25][303/391] Loss_D1: 0.3348 Loss_D2: 0.5687 Loss_G: 3.9140\n",
            "[12/25][304/391] Loss_D1: 0.5158 Loss_D2: 0.6325 Loss_G: 7.3864\n",
            "[12/25][305/391] Loss_D1: 0.5701 Loss_D2: 0.8298 Loss_G: 2.5960\n",
            "[12/25][306/391] Loss_D1: 0.8988 Loss_D2: 0.6047 Loss_G: 8.8451\n",
            "[12/25][307/391] Loss_D1: 1.0709 Loss_D2: 0.5958 Loss_G: 3.7919\n",
            "[12/25][308/391] Loss_D1: 0.7300 Loss_D2: 0.5835 Loss_G: 6.2821\n",
            "[12/25][309/391] Loss_D1: 0.8892 Loss_D2: 0.6070 Loss_G: 3.4725\n",
            "[12/25][310/391] Loss_D1: 0.9155 Loss_D2: 0.5088 Loss_G: 7.1861\n",
            "[12/25][311/391] Loss_D1: 0.4134 Loss_D2: 0.4224 Loss_G: 6.0625\n",
            "[12/25][312/391] Loss_D1: 0.7031 Loss_D2: 0.8041 Loss_G: 2.5950\n",
            "[12/25][313/391] Loss_D1: 0.9444 Loss_D2: 0.5642 Loss_G: 7.8319\n",
            "[12/25][314/391] Loss_D1: 0.3159 Loss_D2: 0.4551 Loss_G: 6.4929\n",
            "[12/25][315/391] Loss_D1: 0.6045 Loss_D2: 0.4463 Loss_G: 3.2821\n",
            "[12/25][316/391] Loss_D1: 0.7819 Loss_D2: 0.4413 Loss_G: 7.6596\n",
            "[12/25][317/391] Loss_D1: 0.7951 Loss_D2: 0.5048 Loss_G: 4.2961\n",
            "[12/25][318/391] Loss_D1: 0.6819 Loss_D2: 0.4471 Loss_G: 5.8620\n",
            "[12/25][319/391] Loss_D1: 0.3801 Loss_D2: 0.5822 Loss_G: 6.2323\n",
            "[12/25][320/391] Loss_D1: 0.5147 Loss_D2: 0.6321 Loss_G: 3.4956\n",
            "[12/25][321/391] Loss_D1: 0.7081 Loss_D2: 0.4172 Loss_G: 6.3627\n",
            "[12/25][322/391] Loss_D1: 0.5290 Loss_D2: 0.6234 Loss_G: 5.3710\n",
            "[12/25][323/391] Loss_D1: 0.2574 Loss_D2: 0.4939 Loss_G: 5.1333\n",
            "[12/25][324/391] Loss_D1: 0.7754 Loss_D2: 0.4635 Loss_G: 5.8082\n",
            "[12/25][325/391] Loss_D1: 0.8883 Loss_D2: 0.6190 Loss_G: 4.5412\n",
            "[12/25][326/391] Loss_D1: 0.6347 Loss_D2: 0.5117 Loss_G: 5.4289\n",
            "[12/25][327/391] Loss_D1: 0.6400 Loss_D2: 0.4297 Loss_G: 4.7505\n",
            "[12/25][328/391] Loss_D1: 0.6391 Loss_D2: 0.4186 Loss_G: 6.3178\n",
            "[12/25][329/391] Loss_D1: 0.3860 Loss_D2: 0.5627 Loss_G: 5.2919\n",
            "[12/25][330/391] Loss_D1: 0.2575 Loss_D2: 0.4480 Loss_G: 6.0663\n",
            "[12/25][331/391] Loss_D1: 0.3798 Loss_D2: 0.6667 Loss_G: 3.6500\n",
            "[12/25][332/391] Loss_D1: 0.5278 Loss_D2: 0.7934 Loss_G: 7.4072\n",
            "[12/25][333/391] Loss_D1: 0.4325 Loss_D2: 0.6489 Loss_G: 3.5438\n",
            "[12/25][334/391] Loss_D1: 0.2993 Loss_D2: 0.6594 Loss_G: 6.8285\n",
            "[12/25][335/391] Loss_D1: 0.4522 Loss_D2: 0.8221 Loss_G: 5.8395\n",
            "[12/25][336/391] Loss_D1: 1.0617 Loss_D2: 0.5137 Loss_G: 3.9951\n",
            "[12/25][337/391] Loss_D1: 0.8053 Loss_D2: 0.8218 Loss_G: 7.1699\n",
            "[12/25][338/391] Loss_D1: 0.3868 Loss_D2: 0.4745 Loss_G: 5.1068\n",
            "[12/25][339/391] Loss_D1: 0.6279 Loss_D2: 0.5941 Loss_G: 3.4958\n",
            "[12/25][340/391] Loss_D1: 1.1145 Loss_D2: 0.4682 Loss_G: 7.2851\n",
            "[12/25][341/391] Loss_D1: 1.0644 Loss_D2: 0.4736 Loss_G: 3.0575\n",
            "[12/25][342/391] Loss_D1: 0.7970 Loss_D2: 0.4824 Loss_G: 6.5760\n",
            "[12/25][343/391] Loss_D1: 0.5730 Loss_D2: 0.5252 Loss_G: 4.2581\n",
            "[12/25][344/391] Loss_D1: 0.3346 Loss_D2: 0.7389 Loss_G: 7.0863\n",
            "[12/25][345/391] Loss_D1: 0.5361 Loss_D2: 1.0814 Loss_G: 3.6034\n",
            "[12/25][346/391] Loss_D1: 0.4151 Loss_D2: 1.0324 Loss_G: 7.3572\n",
            "[12/25][347/391] Loss_D1: 0.3427 Loss_D2: 0.5639 Loss_G: 5.0950\n",
            "[12/25][348/391] Loss_D1: 0.5267 Loss_D2: 0.3558 Loss_G: 5.9408\n",
            "[12/25][349/391] Loss_D1: 0.2958 Loss_D2: 0.5131 Loss_G: 6.6513\n",
            "[12/25][350/391] Loss_D1: 0.6120 Loss_D2: 0.5556 Loss_G: 3.2875\n",
            "[12/25][351/391] Loss_D1: 0.9335 Loss_D2: 0.9825 Loss_G: 8.9432\n",
            "[12/25][352/391] Loss_D1: 0.9253 Loss_D2: 1.5009 Loss_G: 1.8184\n",
            "[12/25][353/391] Loss_D1: 1.0810 Loss_D2: 0.9897 Loss_G: 8.5440\n",
            "[12/25][354/391] Loss_D1: 0.8768 Loss_D2: 0.6157 Loss_G: 3.4800\n",
            "[12/25][355/391] Loss_D1: 0.4752 Loss_D2: 0.5321 Loss_G: 5.6070\n",
            "[12/25][356/391] Loss_D1: 0.5658 Loss_D2: 0.5062 Loss_G: 6.0438\n",
            "[12/25][357/391] Loss_D1: 0.4365 Loss_D2: 0.4225 Loss_G: 5.5559\n",
            "[12/25][358/391] Loss_D1: 0.3358 Loss_D2: 0.5586 Loss_G: 5.3930\n",
            "[12/25][359/391] Loss_D1: 0.5179 Loss_D2: 0.6025 Loss_G: 4.5044\n",
            "[12/25][360/391] Loss_D1: 0.3301 Loss_D2: 0.6545 Loss_G: 7.0638\n",
            "[12/25][361/391] Loss_D1: 0.4288 Loss_D2: 0.7807 Loss_G: 4.7370\n",
            "[12/25][362/391] Loss_D1: 0.4854 Loss_D2: 0.6275 Loss_G: 4.5320\n",
            "[12/25][363/391] Loss_D1: 0.2739 Loss_D2: 1.0061 Loss_G: 7.5121\n",
            "[12/25][364/391] Loss_D1: 0.2697 Loss_D2: 0.6393 Loss_G: 5.7507\n",
            "[12/25][365/391] Loss_D1: 0.4584 Loss_D2: 0.3857 Loss_G: 5.0834\n",
            "[12/25][366/391] Loss_D1: 0.4843 Loss_D2: 0.3159 Loss_G: 5.6057\n",
            "[12/25][367/391] Loss_D1: 0.4980 Loss_D2: 0.4246 Loss_G: 6.4929\n",
            "[12/25][368/391] Loss_D1: 0.3319 Loss_D2: 0.3420 Loss_G: 6.0766\n",
            "[12/25][369/391] Loss_D1: 0.6836 Loss_D2: 0.5327 Loss_G: 3.9312\n",
            "[12/25][370/391] Loss_D1: 1.3314 Loss_D2: 0.4184 Loss_G: 8.6471\n",
            "[12/25][371/391] Loss_D1: 0.9489 Loss_D2: 0.6283 Loss_G: 2.6131\n",
            "[12/25][372/391] Loss_D1: 0.8527 Loss_D2: 0.4582 Loss_G: 7.8106\n",
            "[12/25][373/391] Loss_D1: 0.7568 Loss_D2: 0.5720 Loss_G: 4.0110\n",
            "[12/25][374/391] Loss_D1: 0.5528 Loss_D2: 0.6908 Loss_G: 8.0777\n",
            "[12/25][375/391] Loss_D1: 0.2190 Loss_D2: 0.6704 Loss_G: 6.4439\n",
            "[12/25][376/391] Loss_D1: 0.4530 Loss_D2: 0.5287 Loss_G: 4.7018\n",
            "[12/25][377/391] Loss_D1: 0.5803 Loss_D2: 0.4496 Loss_G: 5.6577\n",
            "[12/25][378/391] Loss_D1: 0.4553 Loss_D2: 0.5776 Loss_G: 5.7586\n",
            "[12/25][379/391] Loss_D1: 0.5381 Loss_D2: 0.5612 Loss_G: 5.2002\n",
            "[12/25][380/391] Loss_D1: 0.4916 Loss_D2: 0.5700 Loss_G: 4.8075\n",
            "[12/25][381/391] Loss_D1: 0.6504 Loss_D2: 0.4670 Loss_G: 6.9662\n",
            "[12/25][382/391] Loss_D1: 0.7638 Loss_D2: 0.5614 Loss_G: 3.7251\n",
            "[12/25][383/391] Loss_D1: 0.5005 Loss_D2: 0.4306 Loss_G: 5.4197\n",
            "[12/25][384/391] Loss_D1: 0.2306 Loss_D2: 0.5002 Loss_G: 6.2869\n",
            "[12/25][385/391] Loss_D1: 0.4505 Loss_D2: 0.5739 Loss_G: 4.1802\n",
            "[12/25][386/391] Loss_D1: 0.6170 Loss_D2: 0.4973 Loss_G: 7.1879\n",
            "[12/25][387/391] Loss_D1: 0.5596 Loss_D2: 0.5326 Loss_G: 4.3615\n",
            "[12/25][388/391] Loss_D1: 0.4332 Loss_D2: 0.5427 Loss_G: 6.9195\n",
            "[12/25][389/391] Loss_D1: 0.4284 Loss_D2: 0.7253 Loss_G: 4.1067\n",
            "[12/25][390/391] Loss_D1: 0.4869 Loss_D2: 0.5109 Loss_G: 5.9340\n",
            "[13/25][0/391] Loss_D1: 0.3711 Loss_D2: 0.5847 Loss_G: 5.9894\n",
            "saving the output\n",
            "[13/25][1/391] Loss_D1: 0.3099 Loss_D2: 0.4480 Loss_G: 5.9945\n",
            "[13/25][2/391] Loss_D1: 0.5269 Loss_D2: 0.4032 Loss_G: 5.9681\n",
            "[13/25][3/391] Loss_D1: 0.2675 Loss_D2: 0.4792 Loss_G: 5.0610\n",
            "[13/25][4/391] Loss_D1: 0.7244 Loss_D2: 0.4319 Loss_G: 4.5676\n",
            "[13/25][5/391] Loss_D1: 0.6921 Loss_D2: 0.3616 Loss_G: 7.4384\n",
            "[13/25][6/391] Loss_D1: 0.5833 Loss_D2: 0.4090 Loss_G: 4.1888\n",
            "[13/25][7/391] Loss_D1: 0.4030 Loss_D2: 0.4243 Loss_G: 5.5513\n",
            "[13/25][8/391] Loss_D1: 0.5721 Loss_D2: 0.4994 Loss_G: 6.4848\n",
            "[13/25][9/391] Loss_D1: 0.5543 Loss_D2: 0.3788 Loss_G: 4.9190\n",
            "[13/25][10/391] Loss_D1: 0.4579 Loss_D2: 0.3186 Loss_G: 5.4130\n",
            "[13/25][11/391] Loss_D1: 0.3730 Loss_D2: 0.3920 Loss_G: 6.3824\n",
            "[13/25][12/391] Loss_D1: 0.7237 Loss_D2: 0.4612 Loss_G: 4.5342\n",
            "[13/25][13/391] Loss_D1: 0.5966 Loss_D2: 0.4020 Loss_G: 6.0701\n",
            "[13/25][14/391] Loss_D1: 0.4295 Loss_D2: 0.6649 Loss_G: 3.5131\n",
            "[13/25][15/391] Loss_D1: 0.3673 Loss_D2: 1.2842 Loss_G: 7.9380\n",
            "[13/25][16/391] Loss_D1: 0.4785 Loss_D2: 0.7786 Loss_G: 3.6704\n",
            "[13/25][17/391] Loss_D1: 0.4845 Loss_D2: 1.0174 Loss_G: 6.9556\n",
            "[13/25][18/391] Loss_D1: 0.4890 Loss_D2: 1.4839 Loss_G: 4.6572\n",
            "[13/25][19/391] Loss_D1: 0.7268 Loss_D2: 1.0624 Loss_G: 5.4360\n",
            "[13/25][20/391] Loss_D1: 0.6683 Loss_D2: 0.7879 Loss_G: 6.2591\n",
            "[13/25][21/391] Loss_D1: 0.3723 Loss_D2: 0.4651 Loss_G: 5.8367\n",
            "[13/25][22/391] Loss_D1: 0.3564 Loss_D2: 0.5061 Loss_G: 5.0753\n",
            "[13/25][23/391] Loss_D1: 0.5435 Loss_D2: 0.6267 Loss_G: 5.2029\n",
            "[13/25][24/391] Loss_D1: 0.4321 Loss_D2: 0.5913 Loss_G: 6.6499\n",
            "[13/25][25/391] Loss_D1: 0.6929 Loss_D2: 0.5138 Loss_G: 3.7380\n",
            "[13/25][26/391] Loss_D1: 1.3553 Loss_D2: 0.5946 Loss_G: 8.4405\n",
            "[13/25][27/391] Loss_D1: 1.4682 Loss_D2: 0.6071 Loss_G: 2.7736\n",
            "[13/25][28/391] Loss_D1: 0.8743 Loss_D2: 0.6588 Loss_G: 8.8857\n",
            "[13/25][29/391] Loss_D1: 1.4648 Loss_D2: 0.3904 Loss_G: 4.4853\n",
            "[13/25][30/391] Loss_D1: 1.2574 Loss_D2: 0.5862 Loss_G: 6.3123\n",
            "[13/25][31/391] Loss_D1: 0.4710 Loss_D2: 0.7837 Loss_G: 5.2368\n",
            "[13/25][32/391] Loss_D1: 0.7537 Loss_D2: 0.5785 Loss_G: 8.1775\n",
            "[13/25][33/391] Loss_D1: 1.0361 Loss_D2: 0.6028 Loss_G: 3.2612\n",
            "[13/25][34/391] Loss_D1: 0.7995 Loss_D2: 0.4651 Loss_G: 5.7187\n",
            "[13/25][35/391] Loss_D1: 0.5610 Loss_D2: 0.5482 Loss_G: 6.1641\n",
            "[13/25][36/391] Loss_D1: 0.4834 Loss_D2: 0.5652 Loss_G: 5.9298\n",
            "[13/25][37/391] Loss_D1: 0.6374 Loss_D2: 0.5696 Loss_G: 3.9671\n",
            "[13/25][38/391] Loss_D1: 0.6841 Loss_D2: 0.6636 Loss_G: 5.4474\n",
            "[13/25][39/391] Loss_D1: 0.5230 Loss_D2: 0.5002 Loss_G: 6.7451\n",
            "[13/25][40/391] Loss_D1: 0.3781 Loss_D2: 0.4335 Loss_G: 5.0638\n",
            "[13/25][41/391] Loss_D1: 0.5517 Loss_D2: 0.4913 Loss_G: 5.2154\n",
            "[13/25][42/391] Loss_D1: 0.4449 Loss_D2: 0.4750 Loss_G: 5.2903\n",
            "[13/25][43/391] Loss_D1: 0.3278 Loss_D2: 0.5106 Loss_G: 5.5690\n",
            "[13/25][44/391] Loss_D1: 0.3631 Loss_D2: 0.4339 Loss_G: 6.2414\n",
            "[13/25][45/391] Loss_D1: 0.6829 Loss_D2: 0.4885 Loss_G: 3.1485\n",
            "[13/25][46/391] Loss_D1: 0.6800 Loss_D2: 0.6134 Loss_G: 6.8146\n",
            "[13/25][47/391] Loss_D1: 0.4193 Loss_D2: 0.6381 Loss_G: 5.2956\n",
            "[13/25][48/391] Loss_D1: 0.3997 Loss_D2: 0.5237 Loss_G: 5.9333\n",
            "[13/25][49/391] Loss_D1: 0.4594 Loss_D2: 0.3993 Loss_G: 4.8272\n",
            "[13/25][50/391] Loss_D1: 0.4123 Loss_D2: 0.5321 Loss_G: 4.7601\n",
            "[13/25][51/391] Loss_D1: 0.4250 Loss_D2: 0.7482 Loss_G: 6.8696\n",
            "[13/25][52/391] Loss_D1: 0.3981 Loss_D2: 0.6284 Loss_G: 4.5970\n",
            "[13/25][53/391] Loss_D1: 0.5283 Loss_D2: 0.5269 Loss_G: 4.9241\n",
            "[13/25][54/391] Loss_D1: 0.2940 Loss_D2: 0.5713 Loss_G: 6.6817\n",
            "[13/25][55/391] Loss_D1: 0.4264 Loss_D2: 0.6669 Loss_G: 4.4169\n",
            "[13/25][56/391] Loss_D1: 0.3777 Loss_D2: 0.6757 Loss_G: 6.5201\n",
            "[13/25][57/391] Loss_D1: 0.3034 Loss_D2: 0.4173 Loss_G: 4.9621\n",
            "[13/25][58/391] Loss_D1: 0.4943 Loss_D2: 0.5122 Loss_G: 4.8060\n",
            "[13/25][59/391] Loss_D1: 0.3507 Loss_D2: 1.0793 Loss_G: 8.1528\n",
            "[13/25][60/391] Loss_D1: 0.2977 Loss_D2: 1.4364 Loss_G: 3.4966\n",
            "[13/25][61/391] Loss_D1: 0.3682 Loss_D2: 1.2856 Loss_G: 8.5794\n",
            "[13/25][62/391] Loss_D1: 0.7238 Loss_D2: 2.1171 Loss_G: 4.1294\n",
            "[13/25][63/391] Loss_D1: 0.5607 Loss_D2: 2.1777 Loss_G: 10.9948\n",
            "[13/25][64/391] Loss_D1: 0.5199 Loss_D2: 4.0132 Loss_G: 1.4131\n",
            "[13/25][65/391] Loss_D1: 1.0042 Loss_D2: 2.8529 Loss_G: 9.3382\n",
            "[13/25][66/391] Loss_D1: 0.9961 Loss_D2: 0.8453 Loss_G: 4.1051\n",
            "[13/25][67/391] Loss_D1: 0.5639 Loss_D2: 0.7165 Loss_G: 5.5943\n",
            "[13/25][68/391] Loss_D1: 0.6758 Loss_D2: 0.7738 Loss_G: 4.1002\n",
            "[13/25][69/391] Loss_D1: 0.8154 Loss_D2: 0.7993 Loss_G: 6.2255\n",
            "[13/25][70/391] Loss_D1: 0.2871 Loss_D2: 0.7189 Loss_G: 6.1957\n",
            "[13/25][71/391] Loss_D1: 0.5810 Loss_D2: 0.8982 Loss_G: 2.7847\n",
            "[13/25][72/391] Loss_D1: 0.6234 Loss_D2: 0.7779 Loss_G: 8.1695\n",
            "[13/25][73/391] Loss_D1: 0.5402 Loss_D2: 0.9360 Loss_G: 3.2223\n",
            "[13/25][74/391] Loss_D1: 0.4289 Loss_D2: 1.0274 Loss_G: 6.8115\n",
            "[13/25][75/391] Loss_D1: 0.7286 Loss_D2: 1.1936 Loss_G: 5.0993\n",
            "[13/25][76/391] Loss_D1: 0.6052 Loss_D2: 0.9064 Loss_G: 6.2418\n",
            "[13/25][77/391] Loss_D1: 0.3544 Loss_D2: 0.7341 Loss_G: 4.1285\n",
            "[13/25][78/391] Loss_D1: 0.4346 Loss_D2: 0.7236 Loss_G: 5.3310\n",
            "[13/25][79/391] Loss_D1: 0.3606 Loss_D2: 0.6022 Loss_G: 5.6709\n",
            "[13/25][80/391] Loss_D1: 0.4024 Loss_D2: 0.5341 Loss_G: 5.1447\n",
            "[13/25][81/391] Loss_D1: 0.6838 Loss_D2: 0.7056 Loss_G: 5.1778\n",
            "[13/25][82/391] Loss_D1: 0.6320 Loss_D2: 0.9072 Loss_G: 5.6519\n",
            "[13/25][83/391] Loss_D1: 0.5060 Loss_D2: 0.7451 Loss_G: 4.8200\n",
            "[13/25][84/391] Loss_D1: 0.4047 Loss_D2: 0.5309 Loss_G: 5.4849\n",
            "[13/25][85/391] Loss_D1: 0.3943 Loss_D2: 0.7610 Loss_G: 4.7604\n",
            "[13/25][86/391] Loss_D1: 0.4331 Loss_D2: 0.5565 Loss_G: 5.4502\n",
            "[13/25][87/391] Loss_D1: 0.3158 Loss_D2: 0.5044 Loss_G: 5.1161\n",
            "[13/25][88/391] Loss_D1: 0.3623 Loss_D2: 0.4472 Loss_G: 5.3126\n",
            "[13/25][89/391] Loss_D1: 0.5184 Loss_D2: 0.4809 Loss_G: 3.9558\n",
            "[13/25][90/391] Loss_D1: 0.5570 Loss_D2: 0.4496 Loss_G: 6.1900\n",
            "[13/25][91/391] Loss_D1: 0.3930 Loss_D2: 0.4648 Loss_G: 4.8993\n",
            "[13/25][92/391] Loss_D1: 0.3363 Loss_D2: 0.4940 Loss_G: 5.5619\n",
            "[13/25][93/391] Loss_D1: 0.3578 Loss_D2: 0.4732 Loss_G: 5.7327\n",
            "[13/25][94/391] Loss_D1: 0.3819 Loss_D2: 0.4811 Loss_G: 4.9791\n",
            "[13/25][95/391] Loss_D1: 0.5349 Loss_D2: 0.5240 Loss_G: 4.7259\n",
            "[13/25][96/391] Loss_D1: 0.4443 Loss_D2: 0.5481 Loss_G: 5.9867\n",
            "[13/25][97/391] Loss_D1: 0.3211 Loss_D2: 0.3739 Loss_G: 6.1177\n",
            "[13/25][98/391] Loss_D1: 0.3143 Loss_D2: 0.4602 Loss_G: 4.7737\n",
            "[13/25][99/391] Loss_D1: 0.3910 Loss_D2: 0.4415 Loss_G: 4.5143\n",
            "[13/25][100/391] Loss_D1: 0.3714 Loss_D2: 1.0755 Loss_G: 7.5263\n",
            "saving the output\n",
            "[13/25][101/391] Loss_D1: 0.3509 Loss_D2: 1.2399 Loss_G: 3.3008\n",
            "[13/25][102/391] Loss_D1: 0.4474 Loss_D2: 1.5775 Loss_G: 7.9477\n",
            "[13/25][103/391] Loss_D1: 0.6034 Loss_D2: 0.9557 Loss_G: 5.8888\n",
            "[13/25][104/391] Loss_D1: 0.6413 Loss_D2: 0.5572 Loss_G: 4.6049\n",
            "[13/25][105/391] Loss_D1: 0.8131 Loss_D2: 0.5387 Loss_G: 7.1157\n",
            "[13/25][106/391] Loss_D1: 1.2157 Loss_D2: 0.4687 Loss_G: 3.1629\n",
            "[13/25][107/391] Loss_D1: 1.1391 Loss_D2: 0.7007 Loss_G: 9.8601\n",
            "[13/25][108/391] Loss_D1: 1.0891 Loss_D2: 1.2555 Loss_G: 1.1172\n",
            "[13/25][109/391] Loss_D1: 2.0985 Loss_D2: 1.1001 Loss_G: 10.1045\n",
            "[13/25][110/391] Loss_D1: 2.0863 Loss_D2: 0.6207 Loss_G: 3.5570\n",
            "[13/25][111/391] Loss_D1: 1.3429 Loss_D2: 0.5842 Loss_G: 6.9511\n",
            "[13/25][112/391] Loss_D1: 1.2125 Loss_D2: 0.6894 Loss_G: 2.9249\n",
            "[13/25][113/391] Loss_D1: 1.1368 Loss_D2: 0.6151 Loss_G: 6.8329\n",
            "[13/25][114/391] Loss_D1: 0.4821 Loss_D2: 0.7339 Loss_G: 5.2855\n",
            "[13/25][115/391] Loss_D1: 1.7923 Loss_D2: 0.6334 Loss_G: 2.8330\n",
            "[13/25][116/391] Loss_D1: 2.4314 Loss_D2: 0.5829 Loss_G: 7.4870\n",
            "[13/25][117/391] Loss_D1: 1.7784 Loss_D2: 0.4707 Loss_G: 3.7818\n",
            "[13/25][118/391] Loss_D1: 0.8012 Loss_D2: 0.4664 Loss_G: 6.8016\n",
            "[13/25][119/391] Loss_D1: 0.4859 Loss_D2: 0.8200 Loss_G: 4.5549\n",
            "[13/25][120/391] Loss_D1: 0.6940 Loss_D2: 0.9923 Loss_G: 6.0675\n",
            "[13/25][121/391] Loss_D1: 0.8967 Loss_D2: 0.7332 Loss_G: 4.6232\n",
            "[13/25][122/391] Loss_D1: 0.9189 Loss_D2: 0.5308 Loss_G: 3.8695\n",
            "[13/25][123/391] Loss_D1: 1.1957 Loss_D2: 0.5759 Loss_G: 8.2610\n",
            "[13/25][124/391] Loss_D1: 1.2355 Loss_D2: 0.4159 Loss_G: 5.3902\n",
            "[13/25][125/391] Loss_D1: 0.5304 Loss_D2: 0.6651 Loss_G: 3.6957\n",
            "[13/25][126/391] Loss_D1: 0.4352 Loss_D2: 0.7204 Loss_G: 6.9430\n",
            "[13/25][127/391] Loss_D1: 0.6343 Loss_D2: 0.8583 Loss_G: 3.5288\n",
            "[13/25][128/391] Loss_D1: 0.5596 Loss_D2: 0.6367 Loss_G: 5.4911\n",
            "[13/25][129/391] Loss_D1: 0.7290 Loss_D2: 0.4690 Loss_G: 4.5423\n",
            "[13/25][130/391] Loss_D1: 0.4563 Loss_D2: 0.4917 Loss_G: 3.9248\n",
            "[13/25][131/391] Loss_D1: 0.5454 Loss_D2: 0.4534 Loss_G: 6.6583\n",
            "[13/25][132/391] Loss_D1: 0.5057 Loss_D2: 0.5505 Loss_G: 4.5488\n",
            "[13/25][133/391] Loss_D1: 0.5917 Loss_D2: 0.5507 Loss_G: 4.9430\n",
            "[13/25][134/391] Loss_D1: 0.4379 Loss_D2: 0.3887 Loss_G: 6.2671\n",
            "[13/25][135/391] Loss_D1: 0.5910 Loss_D2: 0.5362 Loss_G: 3.8388\n",
            "[13/25][136/391] Loss_D1: 0.5641 Loss_D2: 0.5256 Loss_G: 6.4652\n",
            "[13/25][137/391] Loss_D1: 0.4121 Loss_D2: 0.6064 Loss_G: 4.9611\n",
            "[13/25][138/391] Loss_D1: 0.2931 Loss_D2: 0.4993 Loss_G: 4.9471\n",
            "[13/25][139/391] Loss_D1: 0.5452 Loss_D2: 0.5659 Loss_G: 4.0074\n",
            "[13/25][140/391] Loss_D1: 0.6648 Loss_D2: 0.4627 Loss_G: 6.4756\n",
            "[13/25][141/391] Loss_D1: 0.4510 Loss_D2: 0.7529 Loss_G: 4.1624\n",
            "[13/25][142/391] Loss_D1: 0.2457 Loss_D2: 0.9471 Loss_G: 6.7001\n",
            "[13/25][143/391] Loss_D1: 0.5034 Loss_D2: 0.7568 Loss_G: 3.4612\n",
            "[13/25][144/391] Loss_D1: 0.4813 Loss_D2: 0.5152 Loss_G: 5.9229\n",
            "[13/25][145/391] Loss_D1: 0.2154 Loss_D2: 0.4191 Loss_G: 6.3001\n",
            "[13/25][146/391] Loss_D1: 0.4087 Loss_D2: 0.4229 Loss_G: 4.8164\n",
            "[13/25][147/391] Loss_D1: 0.4091 Loss_D2: 0.6128 Loss_G: 5.4829\n",
            "[13/25][148/391] Loss_D1: 0.3079 Loss_D2: 0.4573 Loss_G: 5.6729\n",
            "[13/25][149/391] Loss_D1: 0.3280 Loss_D2: 0.5581 Loss_G: 5.3422\n",
            "[13/25][150/391] Loss_D1: 0.5940 Loss_D2: 0.4424 Loss_G: 3.6362\n",
            "[13/25][151/391] Loss_D1: 0.6099 Loss_D2: 0.6051 Loss_G: 5.5670\n",
            "[13/25][152/391] Loss_D1: 0.3953 Loss_D2: 0.5776 Loss_G: 6.3846\n",
            "[13/25][153/391] Loss_D1: 0.2454 Loss_D2: 0.5159 Loss_G: 5.1404\n",
            "[13/25][154/391] Loss_D1: 0.5314 Loss_D2: 0.5454 Loss_G: 4.8556\n",
            "[13/25][155/391] Loss_D1: 0.5809 Loss_D2: 0.4346 Loss_G: 4.5738\n",
            "[13/25][156/391] Loss_D1: 0.5149 Loss_D2: 0.7431 Loss_G: 4.5121\n",
            "[13/25][157/391] Loss_D1: 0.2863 Loss_D2: 0.8228 Loss_G: 7.1339\n",
            "[13/25][158/391] Loss_D1: 0.6375 Loss_D2: 0.7355 Loss_G: 3.1677\n",
            "[13/25][159/391] Loss_D1: 0.4478 Loss_D2: 0.5797 Loss_G: 5.7086\n",
            "[13/25][160/391] Loss_D1: 0.3270 Loss_D2: 0.5177 Loss_G: 6.3287\n",
            "[13/25][161/391] Loss_D1: 0.4291 Loss_D2: 0.3734 Loss_G: 5.0733\n",
            "[13/25][162/391] Loss_D1: 0.3155 Loss_D2: 0.4185 Loss_G: 4.3330\n",
            "[13/25][163/391] Loss_D1: 0.4137 Loss_D2: 0.4067 Loss_G: 5.5443\n",
            "[13/25][164/391] Loss_D1: 0.3129 Loss_D2: 0.5115 Loss_G: 6.0932\n",
            "[13/25][165/391] Loss_D1: 0.3535 Loss_D2: 0.3952 Loss_G: 5.4580\n",
            "[13/25][166/391] Loss_D1: 0.2728 Loss_D2: 0.6207 Loss_G: 4.1869\n",
            "[13/25][167/391] Loss_D1: 0.3636 Loss_D2: 0.4365 Loss_G: 5.3402\n",
            "[13/25][168/391] Loss_D1: 0.3406 Loss_D2: 0.4781 Loss_G: 6.6446\n",
            "[13/25][169/391] Loss_D1: 0.4248 Loss_D2: 0.8742 Loss_G: 3.6505\n",
            "[13/25][170/391] Loss_D1: 0.3946 Loss_D2: 1.1810 Loss_G: 7.2953\n",
            "[13/25][171/391] Loss_D1: 0.2770 Loss_D2: 1.4197 Loss_G: 3.9944\n",
            "[13/25][172/391] Loss_D1: 0.5007 Loss_D2: 0.7289 Loss_G: 5.3630\n",
            "[13/25][173/391] Loss_D1: 0.4623 Loss_D2: 0.4007 Loss_G: 6.2587\n",
            "[13/25][174/391] Loss_D1: 0.2298 Loss_D2: 0.6508 Loss_G: 6.8227\n",
            "[13/25][175/391] Loss_D1: 0.3674 Loss_D2: 0.7655 Loss_G: 3.2840\n",
            "[13/25][176/391] Loss_D1: 0.3108 Loss_D2: 0.8726 Loss_G: 6.1425\n",
            "[13/25][177/391] Loss_D1: 0.4597 Loss_D2: 0.6559 Loss_G: 4.2560\n",
            "[13/25][178/391] Loss_D1: 0.3583 Loss_D2: 0.5807 Loss_G: 5.4538\n",
            "[13/25][179/391] Loss_D1: 0.4050 Loss_D2: 0.4518 Loss_G: 6.0767\n",
            "[13/25][180/391] Loss_D1: 0.5833 Loss_D2: 0.5024 Loss_G: 3.8004\n",
            "[13/25][181/391] Loss_D1: 0.5791 Loss_D2: 0.4001 Loss_G: 5.6391\n",
            "[13/25][182/391] Loss_D1: 0.3106 Loss_D2: 0.5073 Loss_G: 6.1317\n",
            "[13/25][183/391] Loss_D1: 0.7853 Loss_D2: 0.6395 Loss_G: 2.4122\n",
            "[13/25][184/391] Loss_D1: 0.7313 Loss_D2: 0.7518 Loss_G: 7.2194\n",
            "[13/25][185/391] Loss_D1: 0.4541 Loss_D2: 0.4364 Loss_G: 4.7768\n",
            "[13/25][186/391] Loss_D1: 0.4812 Loss_D2: 0.5213 Loss_G: 3.4841\n",
            "[13/25][187/391] Loss_D1: 0.4905 Loss_D2: 0.6982 Loss_G: 6.2010\n",
            "[13/25][188/391] Loss_D1: 0.3558 Loss_D2: 0.5995 Loss_G: 4.9626\n",
            "[13/25][189/391] Loss_D1: 0.4761 Loss_D2: 0.5029 Loss_G: 4.4377\n",
            "[13/25][190/391] Loss_D1: 0.4435 Loss_D2: 0.4425 Loss_G: 6.0836\n",
            "[13/25][191/391] Loss_D1: 0.4437 Loss_D2: 0.5967 Loss_G: 3.8305\n",
            "[13/25][192/391] Loss_D1: 0.3846 Loss_D2: 0.7445 Loss_G: 5.5853\n",
            "[13/25][193/391] Loss_D1: 0.3697 Loss_D2: 0.6098 Loss_G: 5.4720\n",
            "[13/25][194/391] Loss_D1: 0.6475 Loss_D2: 0.4578 Loss_G: 3.8575\n",
            "[13/25][195/391] Loss_D1: 0.4601 Loss_D2: 0.5542 Loss_G: 6.6279\n",
            "[13/25][196/391] Loss_D1: 0.2840 Loss_D2: 0.6545 Loss_G: 5.1324\n",
            "[13/25][197/391] Loss_D1: 0.4376 Loss_D2: 0.6104 Loss_G: 4.1015\n",
            "[13/25][198/391] Loss_D1: 0.5871 Loss_D2: 0.5320 Loss_G: 6.8612\n",
            "[13/25][199/391] Loss_D1: 0.3552 Loss_D2: 0.5673 Loss_G: 5.2444\n",
            "[13/25][200/391] Loss_D1: 0.6272 Loss_D2: 0.6498 Loss_G: 4.0691\n",
            "saving the output\n",
            "[13/25][201/391] Loss_D1: 0.8303 Loss_D2: 0.5348 Loss_G: 6.6687\n",
            "[13/25][202/391] Loss_D1: 0.4749 Loss_D2: 0.4219 Loss_G: 5.0328\n",
            "[13/25][203/391] Loss_D1: 0.3800 Loss_D2: 0.4479 Loss_G: 4.8675\n",
            "[13/25][204/391] Loss_D1: 0.5179 Loss_D2: 0.5537 Loss_G: 5.7744\n",
            "[13/25][205/391] Loss_D1: 0.6385 Loss_D2: 0.4467 Loss_G: 4.6879\n",
            "[13/25][206/391] Loss_D1: 0.4598 Loss_D2: 0.4214 Loss_G: 5.5731\n",
            "[13/25][207/391] Loss_D1: 0.3641 Loss_D2: 0.5068 Loss_G: 5.4995\n",
            "[13/25][208/391] Loss_D1: 0.3144 Loss_D2: 0.4448 Loss_G: 5.7009\n",
            "[13/25][209/391] Loss_D1: 0.4649 Loss_D2: 0.4941 Loss_G: 3.5877\n",
            "[13/25][210/391] Loss_D1: 0.6096 Loss_D2: 0.7773 Loss_G: 6.4474\n",
            "[13/25][211/391] Loss_D1: 0.5125 Loss_D2: 0.6128 Loss_G: 4.1047\n",
            "[13/25][212/391] Loss_D1: 0.5214 Loss_D2: 0.5100 Loss_G: 6.4570\n",
            "[13/25][213/391] Loss_D1: 0.7526 Loss_D2: 0.4864 Loss_G: 4.3902\n",
            "[13/25][214/391] Loss_D1: 0.5480 Loss_D2: 0.4020 Loss_G: 6.1490\n",
            "[13/25][215/391] Loss_D1: 0.4121 Loss_D2: 0.4895 Loss_G: 4.5780\n",
            "[13/25][216/391] Loss_D1: 0.2205 Loss_D2: 0.6599 Loss_G: 5.9555\n",
            "[13/25][217/391] Loss_D1: 0.4410 Loss_D2: 0.3070 Loss_G: 6.0737\n",
            "[13/25][218/391] Loss_D1: 0.3177 Loss_D2: 0.3206 Loss_G: 5.4998\n",
            "[13/25][219/391] Loss_D1: 0.4597 Loss_D2: 0.4758 Loss_G: 4.0370\n",
            "[13/25][220/391] Loss_D1: 0.5332 Loss_D2: 0.3517 Loss_G: 7.0146\n",
            "[13/25][221/391] Loss_D1: 0.4010 Loss_D2: 0.5238 Loss_G: 5.1662\n",
            "[13/25][222/391] Loss_D1: 0.3865 Loss_D2: 0.5819 Loss_G: 4.6024\n",
            "[13/25][223/391] Loss_D1: 0.5925 Loss_D2: 0.3503 Loss_G: 4.7611\n",
            "[13/25][224/391] Loss_D1: 0.5785 Loss_D2: 0.5381 Loss_G: 6.9229\n",
            "[13/25][225/391] Loss_D1: 1.4667 Loss_D2: 0.5362 Loss_G: 2.8630\n",
            "[13/25][226/391] Loss_D1: 1.0956 Loss_D2: 0.6511 Loss_G: 8.1835\n",
            "[13/25][227/391] Loss_D1: 1.0606 Loss_D2: 0.4180 Loss_G: 3.0704\n",
            "[13/25][228/391] Loss_D1: 2.8161 Loss_D2: 0.4637 Loss_G: 9.1018\n",
            "[13/25][229/391] Loss_D1: 1.2737 Loss_D2: 0.5386 Loss_G: 4.2773\n",
            "[13/25][230/391] Loss_D1: 1.1773 Loss_D2: 0.5733 Loss_G: 7.8464\n",
            "[13/25][231/391] Loss_D1: 1.4375 Loss_D2: 0.7416 Loss_G: 4.1300\n",
            "[13/25][232/391] Loss_D1: 1.5068 Loss_D2: 0.7519 Loss_G: 8.0592\n",
            "[13/25][233/391] Loss_D1: 1.6554 Loss_D2: 0.7769 Loss_G: 5.4816\n",
            "[13/25][234/391] Loss_D1: 1.3485 Loss_D2: 1.1394 Loss_G: 4.7407\n",
            "[13/25][235/391] Loss_D1: 0.7859 Loss_D2: 1.6254 Loss_G: 6.8254\n",
            "[13/25][236/391] Loss_D1: 0.6175 Loss_D2: 0.9220 Loss_G: 1.9823\n",
            "[13/25][237/391] Loss_D1: 1.1910 Loss_D2: 2.3698 Loss_G: 13.0942\n",
            "[13/25][238/391] Loss_D1: 2.5772 Loss_D2: 2.5742 Loss_G: 1.4592\n",
            "[13/25][239/391] Loss_D1: 1.8703 Loss_D2: 1.9162 Loss_G: 11.2060\n",
            "[13/25][240/391] Loss_D1: 1.3852 Loss_D2: 2.1179 Loss_G: 1.6586\n",
            "[13/25][241/391] Loss_D1: 1.0400 Loss_D2: 1.5498 Loss_G: 8.2120\n",
            "[13/25][242/391] Loss_D1: 0.6408 Loss_D2: 0.8570 Loss_G: 4.6991\n",
            "[13/25][243/391] Loss_D1: 1.0086 Loss_D2: 0.8841 Loss_G: 4.9863\n",
            "[13/25][244/391] Loss_D1: 0.9559 Loss_D2: 0.8165 Loss_G: 4.4539\n",
            "[13/25][245/391] Loss_D1: 1.3049 Loss_D2: 1.1602 Loss_G: 6.1837\n",
            "[13/25][246/391] Loss_D1: 1.6263 Loss_D2: 1.4407 Loss_G: 5.7064\n",
            "[13/25][247/391] Loss_D1: 1.2420 Loss_D2: 1.4912 Loss_G: 5.0123\n",
            "[13/25][248/391] Loss_D1: 0.9843 Loss_D2: 1.0615 Loss_G: 4.4177\n",
            "[13/25][249/391] Loss_D1: 0.6700 Loss_D2: 0.7325 Loss_G: 7.3767\n",
            "[13/25][250/391] Loss_D1: 0.4191 Loss_D2: 0.7870 Loss_G: 4.5524\n",
            "[13/25][251/391] Loss_D1: 0.4768 Loss_D2: 0.8691 Loss_G: 6.1874\n",
            "[13/25][252/391] Loss_D1: 0.4427 Loss_D2: 0.8194 Loss_G: 4.6503\n",
            "[13/25][253/391] Loss_D1: 0.9417 Loss_D2: 1.0551 Loss_G: 9.2461\n",
            "[13/25][254/391] Loss_D1: 1.9928 Loss_D2: 1.0810 Loss_G: 1.5918\n",
            "[13/25][255/391] Loss_D1: 1.2953 Loss_D2: 1.3507 Loss_G: 8.9023\n",
            "[13/25][256/391] Loss_D1: 0.5622 Loss_D2: 1.1012 Loss_G: 5.4486\n",
            "[13/25][257/391] Loss_D1: 0.8053 Loss_D2: 0.7631 Loss_G: 2.5425\n",
            "[13/25][258/391] Loss_D1: 1.8697 Loss_D2: 0.6701 Loss_G: 9.4077\n",
            "[13/25][259/391] Loss_D1: 1.3630 Loss_D2: 0.8032 Loss_G: 2.8405\n",
            "[13/25][260/391] Loss_D1: 1.1779 Loss_D2: 0.5501 Loss_G: 7.0452\n",
            "[13/25][261/391] Loss_D1: 1.0610 Loss_D2: 0.4763 Loss_G: 5.3160\n",
            "[13/25][262/391] Loss_D1: 0.5147 Loss_D2: 0.6743 Loss_G: 4.8214\n",
            "[13/25][263/391] Loss_D1: 0.7446 Loss_D2: 0.4883 Loss_G: 5.5169\n",
            "[13/25][264/391] Loss_D1: 0.5207 Loss_D2: 0.6276 Loss_G: 5.7326\n",
            "[13/25][265/391] Loss_D1: 0.5464 Loss_D2: 0.6283 Loss_G: 5.6340\n",
            "[13/25][266/391] Loss_D1: 0.5614 Loss_D2: 0.7466 Loss_G: 4.3190\n",
            "[13/25][267/391] Loss_D1: 0.5252 Loss_D2: 0.7226 Loss_G: 5.9053\n",
            "[13/25][268/391] Loss_D1: 0.4847 Loss_D2: 0.6909 Loss_G: 5.4716\n",
            "[13/25][269/391] Loss_D1: 0.6342 Loss_D2: 0.6576 Loss_G: 4.7342\n",
            "[13/25][270/391] Loss_D1: 0.6828 Loss_D2: 0.7529 Loss_G: 3.9523\n",
            "[13/25][271/391] Loss_D1: 0.5377 Loss_D2: 0.5856 Loss_G: 7.0499\n",
            "[13/25][272/391] Loss_D1: 0.4857 Loss_D2: 0.4936 Loss_G: 5.3623\n",
            "[13/25][273/391] Loss_D1: 0.4138 Loss_D2: 0.3683 Loss_G: 5.2399\n",
            "[13/25][274/391] Loss_D1: 0.4661 Loss_D2: 0.5127 Loss_G: 6.4619\n",
            "[13/25][275/391] Loss_D1: 0.2906 Loss_D2: 0.4400 Loss_G: 5.3484\n",
            "[13/25][276/391] Loss_D1: 0.4459 Loss_D2: 0.4792 Loss_G: 5.6097\n",
            "[13/25][277/391] Loss_D1: 0.3109 Loss_D2: 0.5626 Loss_G: 4.1782\n",
            "[13/25][278/391] Loss_D1: 0.5059 Loss_D2: 0.7878 Loss_G: 5.9252\n",
            "[13/25][279/391] Loss_D1: 0.4261 Loss_D2: 0.3350 Loss_G: 6.7420\n",
            "[13/25][280/391] Loss_D1: 0.3847 Loss_D2: 0.4826 Loss_G: 4.5465\n",
            "[13/25][281/391] Loss_D1: 0.5110 Loss_D2: 0.7058 Loss_G: 5.8328\n",
            "[13/25][282/391] Loss_D1: 0.8128 Loss_D2: 0.6994 Loss_G: 6.6677\n",
            "[13/25][283/391] Loss_D1: 0.9645 Loss_D2: 0.6847 Loss_G: 3.9245\n",
            "[13/25][284/391] Loss_D1: 0.3786 Loss_D2: 0.5503 Loss_G: 7.3085\n",
            "[13/25][285/391] Loss_D1: 0.6125 Loss_D2: 0.6909 Loss_G: 5.8538\n",
            "[13/25][286/391] Loss_D1: 0.5800 Loss_D2: 0.4489 Loss_G: 5.4519\n",
            "[13/25][287/391] Loss_D1: 0.3687 Loss_D2: 0.4584 Loss_G: 6.1320\n",
            "[13/25][288/391] Loss_D1: 0.3871 Loss_D2: 0.6232 Loss_G: 5.4155\n",
            "[13/25][289/391] Loss_D1: 0.5668 Loss_D2: 0.7698 Loss_G: 6.1210\n",
            "[13/25][290/391] Loss_D1: 0.6416 Loss_D2: 1.0221 Loss_G: 4.6931\n",
            "[13/25][291/391] Loss_D1: 0.5709 Loss_D2: 1.4152 Loss_G: 7.2639\n",
            "[13/25][292/391] Loss_D1: 0.6615 Loss_D2: 1.0650 Loss_G: 3.8282\n",
            "[13/25][293/391] Loss_D1: 0.6020 Loss_D2: 0.6488 Loss_G: 5.8979\n",
            "[13/25][294/391] Loss_D1: 0.4221 Loss_D2: 0.3352 Loss_G: 6.5189\n",
            "[13/25][295/391] Loss_D1: 0.3775 Loss_D2: 0.4072 Loss_G: 4.9287\n",
            "[13/25][296/391] Loss_D1: 0.5370 Loss_D2: 0.5152 Loss_G: 5.9300\n",
            "[13/25][297/391] Loss_D1: 0.5528 Loss_D2: 0.4356 Loss_G: 5.0118\n",
            "[13/25][298/391] Loss_D1: 0.3679 Loss_D2: 0.4742 Loss_G: 5.6527\n",
            "[13/25][299/391] Loss_D1: 0.3909 Loss_D2: 0.5655 Loss_G: 5.2788\n",
            "[13/25][300/391] Loss_D1: 0.3485 Loss_D2: 0.6602 Loss_G: 5.2202\n",
            "saving the output\n",
            "[13/25][301/391] Loss_D1: 0.4969 Loss_D2: 0.7856 Loss_G: 5.4339\n",
            "[13/25][302/391] Loss_D1: 0.4400 Loss_D2: 0.6811 Loss_G: 5.8698\n",
            "[13/25][303/391] Loss_D1: 0.7194 Loss_D2: 0.6944 Loss_G: 3.5025\n",
            "[13/25][304/391] Loss_D1: 0.6673 Loss_D2: 0.4563 Loss_G: 6.5109\n",
            "[13/25][305/391] Loss_D1: 0.4006 Loss_D2: 0.5016 Loss_G: 5.8249\n",
            "[13/25][306/391] Loss_D1: 0.3923 Loss_D2: 0.5644 Loss_G: 5.5420\n",
            "[13/25][307/391] Loss_D1: 0.4579 Loss_D2: 0.4600 Loss_G: 5.4431\n",
            "[13/25][308/391] Loss_D1: 0.2632 Loss_D2: 0.5369 Loss_G: 6.2801\n",
            "[13/25][309/391] Loss_D1: 0.3035 Loss_D2: 0.3713 Loss_G: 5.5182\n",
            "[13/25][310/391] Loss_D1: 0.4054 Loss_D2: 0.4901 Loss_G: 5.2426\n",
            "[13/25][311/391] Loss_D1: 0.4528 Loss_D2: 0.7023 Loss_G: 5.3818\n",
            "[13/25][312/391] Loss_D1: 0.7019 Loss_D2: 0.6508 Loss_G: 4.5525\n",
            "[13/25][313/391] Loss_D1: 0.3951 Loss_D2: 0.3612 Loss_G: 6.4723\n",
            "[13/25][314/391] Loss_D1: 0.4255 Loss_D2: 0.6124 Loss_G: 4.0866\n",
            "[13/25][315/391] Loss_D1: 0.3549 Loss_D2: 0.5596 Loss_G: 5.0388\n",
            "[13/25][316/391] Loss_D1: 0.4488 Loss_D2: 0.4184 Loss_G: 5.4096\n",
            "[13/25][317/391] Loss_D1: 0.4070 Loss_D2: 0.6071 Loss_G: 6.6430\n",
            "[13/25][318/391] Loss_D1: 0.6700 Loss_D2: 0.3963 Loss_G: 4.8786\n",
            "[13/25][319/391] Loss_D1: 0.3252 Loss_D2: 0.4876 Loss_G: 3.3387\n",
            "[13/25][320/391] Loss_D1: 0.7140 Loss_D2: 0.7813 Loss_G: 8.1035\n",
            "[13/25][321/391] Loss_D1: 0.3342 Loss_D2: 0.7570 Loss_G: 5.1213\n",
            "[13/25][322/391] Loss_D1: 0.2640 Loss_D2: 0.6208 Loss_G: 6.5114\n",
            "[13/25][323/391] Loss_D1: 0.4171 Loss_D2: 0.6196 Loss_G: 3.8510\n",
            "[13/25][324/391] Loss_D1: 0.6851 Loss_D2: 0.6619 Loss_G: 6.4605\n",
            "[13/25][325/391] Loss_D1: 0.4549 Loss_D2: 1.0183 Loss_G: 3.0309\n",
            "[13/25][326/391] Loss_D1: 0.4565 Loss_D2: 1.1413 Loss_G: 7.3887\n",
            "[13/25][327/391] Loss_D1: 0.4284 Loss_D2: 0.4509 Loss_G: 5.8551\n",
            "[13/25][328/391] Loss_D1: 0.4136 Loss_D2: 0.4927 Loss_G: 3.7049\n",
            "[13/25][329/391] Loss_D1: 0.5723 Loss_D2: 0.8314 Loss_G: 6.3483\n",
            "[13/25][330/391] Loss_D1: 0.7453 Loss_D2: 1.0215 Loss_G: 4.9582\n",
            "[13/25][331/391] Loss_D1: 0.7366 Loss_D2: 0.8635 Loss_G: 5.1741\n",
            "[13/25][332/391] Loss_D1: 0.4722 Loss_D2: 0.4886 Loss_G: 5.7808\n",
            "[13/25][333/391] Loss_D1: 0.4362 Loss_D2: 0.6013 Loss_G: 4.7512\n",
            "[13/25][334/391] Loss_D1: 0.4931 Loss_D2: 0.8939 Loss_G: 6.3925\n",
            "[13/25][335/391] Loss_D1: 0.5075 Loss_D2: 0.7213 Loss_G: 5.3986\n",
            "[13/25][336/391] Loss_D1: 0.6346 Loss_D2: 0.5254 Loss_G: 3.6886\n",
            "[13/25][337/391] Loss_D1: 0.8607 Loss_D2: 0.4275 Loss_G: 7.8662\n",
            "[13/25][338/391] Loss_D1: 0.8262 Loss_D2: 0.7121 Loss_G: 3.4622\n",
            "[13/25][339/391] Loss_D1: 0.7108 Loss_D2: 0.7371 Loss_G: 4.6442\n",
            "[13/25][340/391] Loss_D1: 0.5994 Loss_D2: 0.6279 Loss_G: 7.7331\n",
            "[13/25][341/391] Loss_D1: 0.7544 Loss_D2: 0.5492 Loss_G: 4.2032\n",
            "[13/25][342/391] Loss_D1: 0.5722 Loss_D2: 0.5320 Loss_G: 5.3258\n",
            "[13/25][343/391] Loss_D1: 0.3446 Loss_D2: 0.6195 Loss_G: 6.7139\n",
            "[13/25][344/391] Loss_D1: 0.3064 Loss_D2: 0.5124 Loss_G: 5.3469\n",
            "[13/25][345/391] Loss_D1: 0.4132 Loss_D2: 0.5582 Loss_G: 4.7649\n",
            "[13/25][346/391] Loss_D1: 0.4204 Loss_D2: 0.5768 Loss_G: 6.5307\n",
            "[13/25][347/391] Loss_D1: 0.5895 Loss_D2: 0.8667 Loss_G: 2.7831\n",
            "[13/25][348/391] Loss_D1: 0.6127 Loss_D2: 1.5489 Loss_G: 7.0338\n",
            "[13/25][349/391] Loss_D1: 0.4663 Loss_D2: 1.2718 Loss_G: 3.8415\n",
            "[13/25][350/391] Loss_D1: 0.4045 Loss_D2: 1.1035 Loss_G: 7.3126\n",
            "[13/25][351/391] Loss_D1: 0.2693 Loss_D2: 0.8079 Loss_G: 5.5877\n",
            "[13/25][352/391] Loss_D1: 0.4331 Loss_D2: 0.6383 Loss_G: 4.3221\n",
            "[13/25][353/391] Loss_D1: 0.6060 Loss_D2: 0.6093 Loss_G: 6.4897\n",
            "[13/25][354/391] Loss_D1: 0.5277 Loss_D2: 0.6457 Loss_G: 4.5526\n",
            "[13/25][355/391] Loss_D1: 0.3360 Loss_D2: 0.6033 Loss_G: 6.0710\n",
            "[13/25][356/391] Loss_D1: 0.2147 Loss_D2: 0.5944 Loss_G: 5.9775\n",
            "[13/25][357/391] Loss_D1: 0.4487 Loss_D2: 0.8915 Loss_G: 4.3596\n",
            "[13/25][358/391] Loss_D1: 0.5807 Loss_D2: 0.7013 Loss_G: 5.9609\n",
            "[13/25][359/391] Loss_D1: 0.4019 Loss_D2: 0.7970 Loss_G: 4.4405\n",
            "[13/25][360/391] Loss_D1: 0.3474 Loss_D2: 1.0129 Loss_G: 6.3168\n",
            "[13/25][361/391] Loss_D1: 0.5747 Loss_D2: 1.1686 Loss_G: 5.1948\n",
            "[13/25][362/391] Loss_D1: 0.8297 Loss_D2: 0.7839 Loss_G: 4.0906\n",
            "[13/25][363/391] Loss_D1: 0.6828 Loss_D2: 0.8336 Loss_G: 4.8074\n",
            "[13/25][364/391] Loss_D1: 0.3405 Loss_D2: 0.7537 Loss_G: 7.1761\n",
            "[13/25][365/391] Loss_D1: 0.4704 Loss_D2: 0.7110 Loss_G: 3.6186\n",
            "[13/25][366/391] Loss_D1: 0.3820 Loss_D2: 0.7675 Loss_G: 5.4142\n",
            "[13/25][367/391] Loss_D1: 0.5468 Loss_D2: 0.6134 Loss_G: 5.9940\n",
            "[13/25][368/391] Loss_D1: 0.5202 Loss_D2: 0.4888 Loss_G: 4.6467\n",
            "[13/25][369/391] Loss_D1: 0.4902 Loss_D2: 0.6618 Loss_G: 5.2075\n",
            "[13/25][370/391] Loss_D1: 0.3608 Loss_D2: 0.5659 Loss_G: 5.8144\n",
            "[13/25][371/391] Loss_D1: 0.3992 Loss_D2: 0.6767 Loss_G: 4.3731\n",
            "[13/25][372/391] Loss_D1: 0.3157 Loss_D2: 0.6530 Loss_G: 5.8892\n",
            "[13/25][373/391] Loss_D1: 0.4424 Loss_D2: 0.4798 Loss_G: 4.6947\n",
            "[13/25][374/391] Loss_D1: 0.5208 Loss_D2: 0.4807 Loss_G: 5.7272\n",
            "[13/25][375/391] Loss_D1: 0.8703 Loss_D2: 0.5806 Loss_G: 3.6396\n",
            "[13/25][376/391] Loss_D1: 0.7374 Loss_D2: 0.4975 Loss_G: 6.6769\n",
            "[13/25][377/391] Loss_D1: 0.4289 Loss_D2: 0.4731 Loss_G: 5.1486\n",
            "[13/25][378/391] Loss_D1: 0.3745 Loss_D2: 0.5233 Loss_G: 3.8367\n",
            "[13/25][379/391] Loss_D1: 0.4677 Loss_D2: 0.6029 Loss_G: 8.0461\n",
            "[13/25][380/391] Loss_D1: 0.4309 Loss_D2: 0.6162 Loss_G: 3.7847\n",
            "[13/25][381/391] Loss_D1: 0.3459 Loss_D2: 0.7108 Loss_G: 6.9148\n",
            "[13/25][382/391] Loss_D1: 0.3146 Loss_D2: 0.5205 Loss_G: 5.4574\n",
            "[13/25][383/391] Loss_D1: 0.3445 Loss_D2: 0.4456 Loss_G: 4.1254\n",
            "[13/25][384/391] Loss_D1: 0.7315 Loss_D2: 0.6981 Loss_G: 5.3595\n",
            "[13/25][385/391] Loss_D1: 0.6469 Loss_D2: 0.6619 Loss_G: 4.6392\n",
            "[13/25][386/391] Loss_D1: 0.3654 Loss_D2: 0.6589 Loss_G: 5.5432\n",
            "[13/25][387/391] Loss_D1: 0.2102 Loss_D2: 0.4207 Loss_G: 6.3722\n",
            "[13/25][388/391] Loss_D1: 0.5994 Loss_D2: 0.3116 Loss_G: 4.9464\n",
            "[13/25][389/391] Loss_D1: 0.4555 Loss_D2: 0.4509 Loss_G: 4.9285\n",
            "[13/25][390/391] Loss_D1: 0.4213 Loss_D2: 0.5460 Loss_G: 7.8832\n",
            "[14/25][0/391] Loss_D1: 0.7605 Loss_D2: 0.4233 Loss_G: 4.0693\n",
            "saving the output\n",
            "[14/25][1/391] Loss_D1: 0.5124 Loss_D2: 0.5220 Loss_G: 4.9231\n",
            "[14/25][2/391] Loss_D1: 0.2909 Loss_D2: 1.0294 Loss_G: 8.2197\n",
            "[14/25][3/391] Loss_D1: 0.4569 Loss_D2: 0.8532 Loss_G: 3.7163\n",
            "[14/25][4/391] Loss_D1: 0.5704 Loss_D2: 0.3655 Loss_G: 6.7405\n",
            "[14/25][5/391] Loss_D1: 0.5705 Loss_D2: 0.5027 Loss_G: 6.2082\n",
            "[14/25][6/391] Loss_D1: 0.4915 Loss_D2: 0.7410 Loss_G: 4.4049\n",
            "[14/25][7/391] Loss_D1: 0.3751 Loss_D2: 0.5956 Loss_G: 6.6870\n",
            "[14/25][8/391] Loss_D1: 0.5429 Loss_D2: 0.3931 Loss_G: 5.1066\n",
            "[14/25][9/391] Loss_D1: 0.3382 Loss_D2: 0.4014 Loss_G: 5.3861\n",
            "[14/25][10/391] Loss_D1: 0.3120 Loss_D2: 0.6435 Loss_G: 6.2083\n",
            "[14/25][11/391] Loss_D1: 0.6127 Loss_D2: 0.3811 Loss_G: 4.0702\n",
            "[14/25][12/391] Loss_D1: 0.6692 Loss_D2: 0.3179 Loss_G: 7.3933\n",
            "[14/25][13/391] Loss_D1: 0.4840 Loss_D2: 0.5331 Loss_G: 6.1597\n",
            "[14/25][14/391] Loss_D1: 0.2945 Loss_D2: 0.5034 Loss_G: 5.3214\n",
            "[14/25][15/391] Loss_D1: 0.3503 Loss_D2: 0.4181 Loss_G: 5.1979\n",
            "[14/25][16/391] Loss_D1: 0.5248 Loss_D2: 0.6816 Loss_G: 4.4074\n",
            "[14/25][17/391] Loss_D1: 0.5381 Loss_D2: 0.5701 Loss_G: 7.0416\n",
            "[14/25][18/391] Loss_D1: 0.7694 Loss_D2: 0.9767 Loss_G: 2.4122\n",
            "[14/25][19/391] Loss_D1: 0.6216 Loss_D2: 0.8372 Loss_G: 7.9270\n",
            "[14/25][20/391] Loss_D1: 0.3252 Loss_D2: 0.7248 Loss_G: 5.2227\n",
            "[14/25][21/391] Loss_D1: 0.4150 Loss_D2: 0.5257 Loss_G: 4.9729\n",
            "[14/25][22/391] Loss_D1: 0.5144 Loss_D2: 0.5069 Loss_G: 6.4252\n",
            "[14/25][23/391] Loss_D1: 0.2886 Loss_D2: 0.5109 Loss_G: 5.1237\n",
            "[14/25][24/391] Loss_D1: 0.4154 Loss_D2: 0.6877 Loss_G: 4.9481\n",
            "[14/25][25/391] Loss_D1: 0.3965 Loss_D2: 0.5158 Loss_G: 5.2734\n",
            "[14/25][26/391] Loss_D1: 0.3355 Loss_D2: 0.6513 Loss_G: 6.8507\n",
            "[14/25][27/391] Loss_D1: 0.3361 Loss_D2: 0.4879 Loss_G: 5.0746\n",
            "[14/25][28/391] Loss_D1: 0.4240 Loss_D2: 0.4590 Loss_G: 5.1182\n",
            "[14/25][29/391] Loss_D1: 0.5347 Loss_D2: 0.6484 Loss_G: 6.2898\n",
            "[14/25][30/391] Loss_D1: 0.4252 Loss_D2: 1.2267 Loss_G: 3.6931\n",
            "[14/25][31/391] Loss_D1: 0.6777 Loss_D2: 1.0707 Loss_G: 5.8329\n",
            "[14/25][32/391] Loss_D1: 0.4410 Loss_D2: 0.4170 Loss_G: 6.0992\n",
            "[14/25][33/391] Loss_D1: 0.4324 Loss_D2: 0.4324 Loss_G: 4.6826\n",
            "[14/25][34/391] Loss_D1: 0.4446 Loss_D2: 0.6343 Loss_G: 9.3376\n",
            "[14/25][35/391] Loss_D1: 0.5442 Loss_D2: 0.9159 Loss_G: 4.0634\n",
            "[14/25][36/391] Loss_D1: 0.3808 Loss_D2: 0.5960 Loss_G: 5.1743\n",
            "[14/25][37/391] Loss_D1: 0.5037 Loss_D2: 0.5207 Loss_G: 6.5969\n",
            "[14/25][38/391] Loss_D1: 0.2345 Loss_D2: 0.5972 Loss_G: 7.2467\n",
            "[14/25][39/391] Loss_D1: 0.7593 Loss_D2: 0.7483 Loss_G: 2.2659\n",
            "[14/25][40/391] Loss_D1: 1.0982 Loss_D2: 0.7222 Loss_G: 8.3403\n",
            "[14/25][41/391] Loss_D1: 1.2339 Loss_D2: 0.5242 Loss_G: 3.5127\n",
            "[14/25][42/391] Loss_D1: 1.7799 Loss_D2: 0.5509 Loss_G: 8.7062\n",
            "[14/25][43/391] Loss_D1: 1.3243 Loss_D2: 0.5713 Loss_G: 5.3784\n",
            "[14/25][44/391] Loss_D1: 0.4921 Loss_D2: 0.4026 Loss_G: 4.8692\n",
            "[14/25][45/391] Loss_D1: 0.9129 Loss_D2: 0.3774 Loss_G: 9.0051\n",
            "[14/25][46/391] Loss_D1: 1.1595 Loss_D2: 0.8629 Loss_G: 4.1372\n",
            "[14/25][47/391] Loss_D1: 1.3672 Loss_D2: 0.6912 Loss_G: 6.2442\n",
            "[14/25][48/391] Loss_D1: 1.1764 Loss_D2: 0.5236 Loss_G: 3.4767\n",
            "[14/25][49/391] Loss_D1: 1.5743 Loss_D2: 0.5420 Loss_G: 9.3365\n",
            "[14/25][50/391] Loss_D1: 1.9792 Loss_D2: 0.6985 Loss_G: 4.7133\n",
            "[14/25][51/391] Loss_D1: 0.6309 Loss_D2: 0.3024 Loss_G: 6.0460\n",
            "[14/25][52/391] Loss_D1: 0.5691 Loss_D2: 0.3888 Loss_G: 7.5606\n",
            "[14/25][53/391] Loss_D1: 0.7938 Loss_D2: 0.4878 Loss_G: 3.4564\n",
            "[14/25][54/391] Loss_D1: 0.7319 Loss_D2: 0.6153 Loss_G: 7.3811\n",
            "[14/25][55/391] Loss_D1: 0.3009 Loss_D2: 0.4104 Loss_G: 6.0851\n",
            "[14/25][56/391] Loss_D1: 0.5118 Loss_D2: 0.4845 Loss_G: 5.5167\n",
            "[14/25][57/391] Loss_D1: 1.0530 Loss_D2: 0.4403 Loss_G: 3.5142\n",
            "[14/25][58/391] Loss_D1: 1.9222 Loss_D2: 0.4519 Loss_G: 8.8639\n",
            "[14/25][59/391] Loss_D1: 2.0450 Loss_D2: 0.5408 Loss_G: 3.0736\n",
            "[14/25][60/391] Loss_D1: 1.3929 Loss_D2: 0.3808 Loss_G: 7.0925\n",
            "[14/25][61/391] Loss_D1: 0.5826 Loss_D2: 0.5158 Loss_G: 5.6640\n",
            "[14/25][62/391] Loss_D1: 0.6581 Loss_D2: 0.6405 Loss_G: 4.1636\n",
            "[14/25][63/391] Loss_D1: 0.6950 Loss_D2: 0.5178 Loss_G: 6.0035\n",
            "[14/25][64/391] Loss_D1: 0.4578 Loss_D2: 0.6126 Loss_G: 5.2814\n",
            "[14/25][65/391] Loss_D1: 0.4971 Loss_D2: 0.6039 Loss_G: 4.1968\n",
            "[14/25][66/391] Loss_D1: 0.6805 Loss_D2: 0.3886 Loss_G: 6.0073\n",
            "[14/25][67/391] Loss_D1: 0.3989 Loss_D2: 0.5093 Loss_G: 6.0633\n",
            "[14/25][68/391] Loss_D1: 0.6265 Loss_D2: 0.5936 Loss_G: 3.9662\n",
            "[14/25][69/391] Loss_D1: 0.4685 Loss_D2: 0.7263 Loss_G: 7.0959\n",
            "[14/25][70/391] Loss_D1: 0.7577 Loss_D2: 0.5338 Loss_G: 3.6815\n",
            "[14/25][71/391] Loss_D1: 0.6801 Loss_D2: 0.4546 Loss_G: 6.4510\n",
            "[14/25][72/391] Loss_D1: 0.3352 Loss_D2: 0.4306 Loss_G: 6.3678\n",
            "[14/25][73/391] Loss_D1: 0.5432 Loss_D2: 0.5151 Loss_G: 3.9528\n",
            "[14/25][74/391] Loss_D1: 0.4070 Loss_D2: 0.4899 Loss_G: 5.7423\n",
            "[14/25][75/391] Loss_D1: 0.3200 Loss_D2: 0.3405 Loss_G: 5.9452\n",
            "[14/25][76/391] Loss_D1: 0.2515 Loss_D2: 0.4019 Loss_G: 5.8502\n",
            "[14/25][77/391] Loss_D1: 0.2843 Loss_D2: 0.3754 Loss_G: 5.8876\n",
            "[14/25][78/391] Loss_D1: 0.3052 Loss_D2: 0.5708 Loss_G: 4.1775\n",
            "[14/25][79/391] Loss_D1: 0.3172 Loss_D2: 0.6264 Loss_G: 6.7722\n",
            "[14/25][80/391] Loss_D1: 0.6395 Loss_D2: 0.6853 Loss_G: 3.5000\n",
            "[14/25][81/391] Loss_D1: 0.4487 Loss_D2: 0.5139 Loss_G: 4.3053\n",
            "[14/25][82/391] Loss_D1: 0.5556 Loss_D2: 0.3972 Loss_G: 7.0680\n",
            "[14/25][83/391] Loss_D1: 0.4951 Loss_D2: 0.6351 Loss_G: 3.9315\n",
            "[14/25][84/391] Loss_D1: 0.4099 Loss_D2: 0.8782 Loss_G: 6.4792\n",
            "[14/25][85/391] Loss_D1: 0.6101 Loss_D2: 0.7574 Loss_G: 5.8039\n",
            "[14/25][86/391] Loss_D1: 0.3144 Loss_D2: 0.3960 Loss_G: 5.4968\n",
            "[14/25][87/391] Loss_D1: 0.5488 Loss_D2: 0.4953 Loss_G: 4.4788\n",
            "[14/25][88/391] Loss_D1: 0.4963 Loss_D2: 0.5363 Loss_G: 5.9025\n",
            "[14/25][89/391] Loss_D1: 0.4762 Loss_D2: 0.5159 Loss_G: 4.3452\n",
            "[14/25][90/391] Loss_D1: 0.6664 Loss_D2: 0.5772 Loss_G: 4.2059\n",
            "[14/25][91/391] Loss_D1: 0.7021 Loss_D2: 0.3627 Loss_G: 6.8207\n",
            "[14/25][92/391] Loss_D1: 0.3783 Loss_D2: 0.5725 Loss_G: 5.4365\n",
            "[14/25][93/391] Loss_D1: 0.4020 Loss_D2: 0.4537 Loss_G: 4.2637\n",
            "[14/25][94/391] Loss_D1: 0.5632 Loss_D2: 0.6248 Loss_G: 5.9427\n",
            "[14/25][95/391] Loss_D1: 0.4639 Loss_D2: 0.4834 Loss_G: 4.5912\n",
            "[14/25][96/391] Loss_D1: 0.3580 Loss_D2: 0.3792 Loss_G: 4.5414\n",
            "[14/25][97/391] Loss_D1: 0.4396 Loss_D2: 0.7168 Loss_G: 5.9170\n",
            "[14/25][98/391] Loss_D1: 0.4216 Loss_D2: 0.8154 Loss_G: 3.5691\n",
            "[14/25][99/391] Loss_D1: 0.3846 Loss_D2: 0.8140 Loss_G: 6.2743\n",
            "[14/25][100/391] Loss_D1: 0.4970 Loss_D2: 0.4542 Loss_G: 3.0192\n",
            "saving the output\n",
            "[14/25][101/391] Loss_D1: 0.4828 Loss_D2: 0.9210 Loss_G: 9.2261\n",
            "[14/25][102/391] Loss_D1: 0.5362 Loss_D2: 2.3342 Loss_G: 3.3267\n",
            "[14/25][103/391] Loss_D1: 0.8163 Loss_D2: 2.4768 Loss_G: 7.1914\n",
            "[14/25][104/391] Loss_D1: 0.8704 Loss_D2: 2.2053 Loss_G: 3.8538\n",
            "[14/25][105/391] Loss_D1: 0.5229 Loss_D2: 3.6871 Loss_G: 8.8375\n",
            "[14/25][106/391] Loss_D1: 0.5154 Loss_D2: 2.5520 Loss_G: 3.6421\n",
            "[14/25][107/391] Loss_D1: 0.5712 Loss_D2: 0.7117 Loss_G: 5.2107\n",
            "[14/25][108/391] Loss_D1: 0.4381 Loss_D2: 0.4472 Loss_G: 6.6558\n",
            "[14/25][109/391] Loss_D1: 0.4789 Loss_D2: 0.5961 Loss_G: 3.6482\n",
            "[14/25][110/391] Loss_D1: 0.6866 Loss_D2: 0.6135 Loss_G: 7.0757\n",
            "[14/25][111/391] Loss_D1: 0.6238 Loss_D2: 0.4825 Loss_G: 5.0043\n",
            "[14/25][112/391] Loss_D1: 0.6066 Loss_D2: 0.9737 Loss_G: 4.8775\n",
            "[14/25][113/391] Loss_D1: 0.8576 Loss_D2: 1.0891 Loss_G: 4.9948\n",
            "[14/25][114/391] Loss_D1: 0.8829 Loss_D2: 0.8441 Loss_G: 5.7911\n",
            "[14/25][115/391] Loss_D1: 0.6708 Loss_D2: 0.7744 Loss_G: 3.7212\n",
            "[14/25][116/391] Loss_D1: 0.4552 Loss_D2: 0.6920 Loss_G: 5.7285\n",
            "[14/25][117/391] Loss_D1: 0.4062 Loss_D2: 0.7909 Loss_G: 4.4482\n",
            "[14/25][118/391] Loss_D1: 0.2344 Loss_D2: 0.8292 Loss_G: 6.1744\n",
            "[14/25][119/391] Loss_D1: 0.4791 Loss_D2: 0.7322 Loss_G: 3.6136\n",
            "[14/25][120/391] Loss_D1: 0.5713 Loss_D2: 0.5175 Loss_G: 5.9094\n",
            "[14/25][121/391] Loss_D1: 0.5222 Loss_D2: 0.6638 Loss_G: 4.1165\n",
            "[14/25][122/391] Loss_D1: 0.5183 Loss_D2: 0.4883 Loss_G: 6.4796\n",
            "[14/25][123/391] Loss_D1: 0.3571 Loss_D2: 0.7337 Loss_G: 4.3792\n",
            "[14/25][124/391] Loss_D1: 0.5613 Loss_D2: 0.6466 Loss_G: 4.9951\n",
            "[14/25][125/391] Loss_D1: 0.3027 Loss_D2: 0.6363 Loss_G: 5.7933\n",
            "[14/25][126/391] Loss_D1: 0.4693 Loss_D2: 0.4205 Loss_G: 4.4623\n",
            "[14/25][127/391] Loss_D1: 0.5433 Loss_D2: 0.4785 Loss_G: 5.1183\n",
            "[14/25][128/391] Loss_D1: 0.2575 Loss_D2: 0.6649 Loss_G: 6.8583\n",
            "[14/25][129/391] Loss_D1: 0.5778 Loss_D2: 0.6511 Loss_G: 3.0968\n",
            "[14/25][130/391] Loss_D1: 1.0059 Loss_D2: 0.6324 Loss_G: 6.7966\n",
            "[14/25][131/391] Loss_D1: 0.4935 Loss_D2: 0.3714 Loss_G: 6.0166\n",
            "[14/25][132/391] Loss_D1: 0.4289 Loss_D2: 0.4621 Loss_G: 3.6196\n",
            "[14/25][133/391] Loss_D1: 0.8427 Loss_D2: 0.5025 Loss_G: 7.2692\n",
            "[14/25][134/391] Loss_D1: 0.5890 Loss_D2: 0.4868 Loss_G: 4.7921\n",
            "[14/25][135/391] Loss_D1: 0.4026 Loss_D2: 0.3249 Loss_G: 4.4177\n",
            "[14/25][136/391] Loss_D1: 0.5662 Loss_D2: 0.4440 Loss_G: 6.6426\n",
            "[14/25][137/391] Loss_D1: 0.9414 Loss_D2: 0.6852 Loss_G: 5.2840\n",
            "[14/25][138/391] Loss_D1: 0.7333 Loss_D2: 0.6548 Loss_G: 5.2703\n",
            "[14/25][139/391] Loss_D1: 0.4987 Loss_D2: 0.9406 Loss_G: 6.8191\n",
            "[14/25][140/391] Loss_D1: 0.4381 Loss_D2: 1.1316 Loss_G: 2.4404\n",
            "[14/25][141/391] Loss_D1: 0.7678 Loss_D2: 1.1478 Loss_G: 8.5086\n",
            "[14/25][142/391] Loss_D1: 0.8686 Loss_D2: 0.9016 Loss_G: 2.9607\n",
            "[14/25][143/391] Loss_D1: 0.6266 Loss_D2: 0.6694 Loss_G: 5.8289\n",
            "[14/25][144/391] Loss_D1: 0.3558 Loss_D2: 0.6148 Loss_G: 5.9538\n",
            "[14/25][145/391] Loss_D1: 0.4384 Loss_D2: 0.5259 Loss_G: 5.1374\n",
            "[14/25][146/391] Loss_D1: 0.4199 Loss_D2: 0.4472 Loss_G: 5.6987\n",
            "[14/25][147/391] Loss_D1: 0.5466 Loss_D2: 0.4604 Loss_G: 3.5620\n",
            "[14/25][148/391] Loss_D1: 0.7000 Loss_D2: 0.6228 Loss_G: 7.0045\n",
            "[14/25][149/391] Loss_D1: 0.4364 Loss_D2: 0.7107 Loss_G: 4.5647\n",
            "[14/25][150/391] Loss_D1: 0.5248 Loss_D2: 0.7982 Loss_G: 5.3572\n",
            "[14/25][151/391] Loss_D1: 0.6338 Loss_D2: 0.6549 Loss_G: 6.0028\n",
            "[14/25][152/391] Loss_D1: 0.4076 Loss_D2: 0.5607 Loss_G: 4.7549\n",
            "[14/25][153/391] Loss_D1: 0.7439 Loss_D2: 0.5573 Loss_G: 3.9268\n",
            "[14/25][154/391] Loss_D1: 0.8892 Loss_D2: 0.7559 Loss_G: 5.5826\n",
            "[14/25][155/391] Loss_D1: 0.3305 Loss_D2: 0.7570 Loss_G: 7.4871\n",
            "[14/25][156/391] Loss_D1: 0.6189 Loss_D2: 0.6310 Loss_G: 3.5950\n",
            "[14/25][157/391] Loss_D1: 0.4772 Loss_D2: 0.5579 Loss_G: 4.9763\n",
            "[14/25][158/391] Loss_D1: 0.3705 Loss_D2: 0.4577 Loss_G: 5.2749\n",
            "[14/25][159/391] Loss_D1: 0.2884 Loss_D2: 0.3024 Loss_G: 6.1191\n",
            "[14/25][160/391] Loss_D1: 0.3274 Loss_D2: 0.6609 Loss_G: 4.4621\n",
            "[14/25][161/391] Loss_D1: 0.4093 Loss_D2: 0.5211 Loss_G: 4.5543\n",
            "[14/25][162/391] Loss_D1: 0.4849 Loss_D2: 0.5168 Loss_G: 5.5027\n",
            "[14/25][163/391] Loss_D1: 0.5306 Loss_D2: 0.4947 Loss_G: 4.4053\n",
            "[14/25][164/391] Loss_D1: 0.5999 Loss_D2: 0.5161 Loss_G: 5.6761\n",
            "[14/25][165/391] Loss_D1: 0.5189 Loss_D2: 0.4716 Loss_G: 4.6197\n",
            "[14/25][166/391] Loss_D1: 0.5141 Loss_D2: 0.3881 Loss_G: 4.5818\n",
            "[14/25][167/391] Loss_D1: 0.5312 Loss_D2: 0.6898 Loss_G: 6.9684\n",
            "[14/25][168/391] Loss_D1: 0.5498 Loss_D2: 0.7250 Loss_G: 3.5928\n",
            "[14/25][169/391] Loss_D1: 0.5968 Loss_D2: 0.7105 Loss_G: 5.5120\n",
            "[14/25][170/391] Loss_D1: 0.4252 Loss_D2: 0.4480 Loss_G: 5.3266\n",
            "[14/25][171/391] Loss_D1: 0.5285 Loss_D2: 0.5516 Loss_G: 3.3841\n",
            "[14/25][172/391] Loss_D1: 0.7266 Loss_D2: 0.9301 Loss_G: 7.3459\n",
            "[14/25][173/391] Loss_D1: 0.6539 Loss_D2: 0.4947 Loss_G: 4.9225\n",
            "[14/25][174/391] Loss_D1: 0.4390 Loss_D2: 0.3607 Loss_G: 4.8197\n",
            "[14/25][175/391] Loss_D1: 0.4047 Loss_D2: 0.4245 Loss_G: 6.1114\n",
            "[14/25][176/391] Loss_D1: 0.2711 Loss_D2: 0.4703 Loss_G: 5.3921\n",
            "[14/25][177/391] Loss_D1: 0.5424 Loss_D2: 0.5243 Loss_G: 3.1874\n",
            "[14/25][178/391] Loss_D1: 0.4689 Loss_D2: 0.7301 Loss_G: 6.3798\n",
            "[14/25][179/391] Loss_D1: 0.3635 Loss_D2: 0.6458 Loss_G: 5.2319\n",
            "[14/25][180/391] Loss_D1: 0.7153 Loss_D2: 0.3898 Loss_G: 3.6739\n",
            "[14/25][181/391] Loss_D1: 0.7823 Loss_D2: 0.3981 Loss_G: 7.4859\n",
            "[14/25][182/391] Loss_D1: 0.6681 Loss_D2: 0.4283 Loss_G: 4.8387\n",
            "[14/25][183/391] Loss_D1: 0.3740 Loss_D2: 0.3244 Loss_G: 5.2766\n",
            "[14/25][184/391] Loss_D1: 0.2961 Loss_D2: 0.5001 Loss_G: 5.7121\n",
            "[14/25][185/391] Loss_D1: 0.5218 Loss_D2: 0.4945 Loss_G: 4.8393\n",
            "[14/25][186/391] Loss_D1: 0.4138 Loss_D2: 0.5869 Loss_G: 5.0625\n",
            "[14/25][187/391] Loss_D1: 0.3435 Loss_D2: 0.4436 Loss_G: 5.5838\n",
            "[14/25][188/391] Loss_D1: 0.4613 Loss_D2: 0.4430 Loss_G: 4.3426\n",
            "[14/25][189/391] Loss_D1: 0.5960 Loss_D2: 0.5335 Loss_G: 4.7635\n",
            "[14/25][190/391] Loss_D1: 0.6071 Loss_D2: 0.5056 Loss_G: 6.5948\n",
            "[14/25][191/391] Loss_D1: 0.9782 Loss_D2: 0.4295 Loss_G: 3.2152\n",
            "[14/25][192/391] Loss_D1: 0.7818 Loss_D2: 0.5140 Loss_G: 7.3370\n",
            "[14/25][193/391] Loss_D1: 0.3813 Loss_D2: 0.4353 Loss_G: 5.0947\n",
            "[14/25][194/391] Loss_D1: 0.3484 Loss_D2: 0.5445 Loss_G: 3.7559\n",
            "[14/25][195/391] Loss_D1: 0.6582 Loss_D2: 0.5043 Loss_G: 6.3731\n",
            "[14/25][196/391] Loss_D1: 0.3183 Loss_D2: 0.4420 Loss_G: 5.1352\n",
            "[14/25][197/391] Loss_D1: 0.2262 Loss_D2: 0.4527 Loss_G: 5.5624\n",
            "[14/25][198/391] Loss_D1: 0.3191 Loss_D2: 0.3195 Loss_G: 5.3802\n",
            "[14/25][199/391] Loss_D1: 0.2800 Loss_D2: 0.4315 Loss_G: 6.5620\n",
            "[14/25][200/391] Loss_D1: 0.5392 Loss_D2: 0.5100 Loss_G: 3.7596\n",
            "saving the output\n",
            "[14/25][201/391] Loss_D1: 0.4654 Loss_D2: 0.5353 Loss_G: 5.3965\n",
            "[14/25][202/391] Loss_D1: 0.4830 Loss_D2: 0.5716 Loss_G: 5.5460\n",
            "[14/25][203/391] Loss_D1: 0.4933 Loss_D2: 0.5946 Loss_G: 4.5817\n",
            "[14/25][204/391] Loss_D1: 0.5066 Loss_D2: 0.6647 Loss_G: 5.0467\n",
            "[14/25][205/391] Loss_D1: 0.3673 Loss_D2: 0.6981 Loss_G: 4.8896\n",
            "[14/25][206/391] Loss_D1: 0.2600 Loss_D2: 0.4389 Loss_G: 6.9062\n",
            "[14/25][207/391] Loss_D1: 0.5585 Loss_D2: 0.5319 Loss_G: 4.2719\n",
            "[14/25][208/391] Loss_D1: 0.4346 Loss_D2: 0.4834 Loss_G: 4.6490\n",
            "[14/25][209/391] Loss_D1: 0.2697 Loss_D2: 0.4820 Loss_G: 6.4486\n",
            "[14/25][210/391] Loss_D1: 0.3864 Loss_D2: 0.7910 Loss_G: 3.1396\n",
            "[14/25][211/391] Loss_D1: 0.5462 Loss_D2: 0.7897 Loss_G: 7.1530\n",
            "[14/25][212/391] Loss_D1: 0.4273 Loss_D2: 0.5019 Loss_G: 4.4814\n",
            "[14/25][213/391] Loss_D1: 0.2895 Loss_D2: 0.6520 Loss_G: 5.5959\n",
            "[14/25][214/391] Loss_D1: 0.4012 Loss_D2: 0.6394 Loss_G: 4.2455\n",
            "[14/25][215/391] Loss_D1: 0.4671 Loss_D2: 0.5957 Loss_G: 6.4126\n",
            "[14/25][216/391] Loss_D1: 0.5828 Loss_D2: 0.5485 Loss_G: 3.9031\n",
            "[14/25][217/391] Loss_D1: 0.3077 Loss_D2: 0.5441 Loss_G: 5.8502\n",
            "[14/25][218/391] Loss_D1: 0.6901 Loss_D2: 0.6726 Loss_G: 4.4541\n",
            "[14/25][219/391] Loss_D1: 0.6776 Loss_D2: 0.4699 Loss_G: 6.1290\n",
            "[14/25][220/391] Loss_D1: 0.4695 Loss_D2: 0.3966 Loss_G: 4.8014\n",
            "[14/25][221/391] Loss_D1: 0.4747 Loss_D2: 0.4702 Loss_G: 3.4457\n",
            "[14/25][222/391] Loss_D1: 1.3309 Loss_D2: 0.4545 Loss_G: 7.7046\n",
            "[14/25][223/391] Loss_D1: 1.1494 Loss_D2: 0.5707 Loss_G: 3.1933\n",
            "[14/25][224/391] Loss_D1: 1.8094 Loss_D2: 0.6170 Loss_G: 10.4113\n",
            "[14/25][225/391] Loss_D1: 3.2234 Loss_D2: 0.4192 Loss_G: 2.6988\n",
            "[14/25][226/391] Loss_D1: 2.3559 Loss_D2: 0.4315 Loss_G: 12.3075\n",
            "[14/25][227/391] Loss_D1: 5.3099 Loss_D2: 0.4409 Loss_G: 2.4845\n",
            "[14/25][228/391] Loss_D1: 2.7416 Loss_D2: 0.7906 Loss_G: 7.9379\n",
            "[14/25][229/391] Loss_D1: 2.0099 Loss_D2: 0.7968 Loss_G: 1.7773\n",
            "[14/25][230/391] Loss_D1: 1.6184 Loss_D2: 1.0050 Loss_G: 7.3678\n",
            "[14/25][231/391] Loss_D1: 0.8701 Loss_D2: 0.6935 Loss_G: 3.2082\n",
            "[14/25][232/391] Loss_D1: 0.9228 Loss_D2: 0.9170 Loss_G: 9.1602\n",
            "[14/25][233/391] Loss_D1: 1.1997 Loss_D2: 1.0336 Loss_G: 3.0157\n",
            "[14/25][234/391] Loss_D1: 0.9063 Loss_D2: 0.9346 Loss_G: 7.3277\n",
            "[14/25][235/391] Loss_D1: 0.8462 Loss_D2: 0.8192 Loss_G: 2.8069\n",
            "[14/25][236/391] Loss_D1: 0.8332 Loss_D2: 0.9073 Loss_G: 6.3735\n",
            "[14/25][237/391] Loss_D1: 0.5833 Loss_D2: 0.8414 Loss_G: 4.7445\n",
            "[14/25][238/391] Loss_D1: 0.4488 Loss_D2: 0.6629 Loss_G: 5.1415\n",
            "[14/25][239/391] Loss_D1: 0.5240 Loss_D2: 0.6245 Loss_G: 5.0308\n",
            "[14/25][240/391] Loss_D1: 0.7725 Loss_D2: 0.6955 Loss_G: 5.6344\n",
            "[14/25][241/391] Loss_D1: 0.7754 Loss_D2: 1.2474 Loss_G: 4.3563\n",
            "[14/25][242/391] Loss_D1: 0.6417 Loss_D2: 1.2135 Loss_G: 5.7596\n",
            "[14/25][243/391] Loss_D1: 0.5807 Loss_D2: 0.4934 Loss_G: 6.4946\n",
            "[14/25][244/391] Loss_D1: 0.6626 Loss_D2: 0.4476 Loss_G: 4.2283\n",
            "[14/25][245/391] Loss_D1: 0.5318 Loss_D2: 0.5497 Loss_G: 4.1775\n",
            "[14/25][246/391] Loss_D1: 0.6418 Loss_D2: 0.6756 Loss_G: 5.7220\n",
            "[14/25][247/391] Loss_D1: 0.8609 Loss_D2: 0.6521 Loss_G: 5.4282\n",
            "[14/25][248/391] Loss_D1: 0.4908 Loss_D2: 0.4481 Loss_G: 5.5534\n",
            "[14/25][249/391] Loss_D1: 0.5007 Loss_D2: 0.6445 Loss_G: 4.1266\n",
            "[14/25][250/391] Loss_D1: 0.5540 Loss_D2: 0.6711 Loss_G: 5.5395\n",
            "[14/25][251/391] Loss_D1: 0.6606 Loss_D2: 0.5719 Loss_G: 5.0411\n",
            "[14/25][252/391] Loss_D1: 0.4600 Loss_D2: 0.5080 Loss_G: 6.0417\n",
            "[14/25][253/391] Loss_D1: 0.5883 Loss_D2: 0.4410 Loss_G: 4.4545\n",
            "[14/25][254/391] Loss_D1: 0.5674 Loss_D2: 0.6736 Loss_G: 6.9500\n",
            "[14/25][255/391] Loss_D1: 0.4284 Loss_D2: 0.4945 Loss_G: 4.5065\n",
            "[14/25][256/391] Loss_D1: 0.5422 Loss_D2: 0.5684 Loss_G: 3.9798\n",
            "[14/25][257/391] Loss_D1: 0.8424 Loss_D2: 0.4067 Loss_G: 6.4219\n",
            "[14/25][258/391] Loss_D1: 0.3271 Loss_D2: 0.4575 Loss_G: 6.4817\n",
            "[14/25][259/391] Loss_D1: 0.8264 Loss_D2: 0.6325 Loss_G: 2.4491\n",
            "[14/25][260/391] Loss_D1: 1.0074 Loss_D2: 0.6349 Loss_G: 7.9298\n",
            "[14/25][261/391] Loss_D1: 0.3492 Loss_D2: 0.4856 Loss_G: 5.3756\n",
            "[14/25][262/391] Loss_D1: 0.6662 Loss_D2: 0.3582 Loss_G: 4.5008\n",
            "[14/25][263/391] Loss_D1: 0.7890 Loss_D2: 0.7412 Loss_G: 5.6081\n",
            "[14/25][264/391] Loss_D1: 0.9509 Loss_D2: 0.8490 Loss_G: 5.4110\n",
            "[14/25][265/391] Loss_D1: 0.7677 Loss_D2: 0.4358 Loss_G: 5.1939\n",
            "[14/25][266/391] Loss_D1: 0.4637 Loss_D2: 0.6720 Loss_G: 6.0859\n",
            "[14/25][267/391] Loss_D1: 0.4126 Loss_D2: 0.9561 Loss_G: 2.7849\n",
            "[14/25][268/391] Loss_D1: 0.5989 Loss_D2: 1.2833 Loss_G: 8.8939\n",
            "[14/25][269/391] Loss_D1: 0.4824 Loss_D2: 1.1851 Loss_G: 2.6909\n",
            "[14/25][270/391] Loss_D1: 0.6064 Loss_D2: 1.9817 Loss_G: 6.9731\n",
            "[14/25][271/391] Loss_D1: 0.7740 Loss_D2: 1.8770 Loss_G: 4.5511\n",
            "[14/25][272/391] Loss_D1: 0.3722 Loss_D2: 1.3334 Loss_G: 7.9804\n",
            "[14/25][273/391] Loss_D1: 0.8062 Loss_D2: 0.8104 Loss_G: 3.2078\n",
            "[14/25][274/391] Loss_D1: 1.1338 Loss_D2: 0.7060 Loss_G: 6.1381\n",
            "[14/25][275/391] Loss_D1: 0.9770 Loss_D2: 0.8848 Loss_G: 5.7122\n",
            "[14/25][276/391] Loss_D1: 0.8821 Loss_D2: 0.8384 Loss_G: 6.2144\n",
            "[14/25][277/391] Loss_D1: 0.7920 Loss_D2: 0.7514 Loss_G: 3.9811\n",
            "[14/25][278/391] Loss_D1: 0.5164 Loss_D2: 0.6812 Loss_G: 5.6846\n",
            "[14/25][279/391] Loss_D1: 0.4719 Loss_D2: 1.1262 Loss_G: 4.0866\n",
            "[14/25][280/391] Loss_D1: 0.5037 Loss_D2: 1.2960 Loss_G: 7.5227\n",
            "[14/25][281/391] Loss_D1: 0.5581 Loss_D2: 1.3121 Loss_G: 3.1990\n",
            "[14/25][282/391] Loss_D1: 0.6899 Loss_D2: 0.6281 Loss_G: 5.3436\n",
            "[14/25][283/391] Loss_D1: 0.3399 Loss_D2: 0.6810 Loss_G: 6.6077\n",
            "[14/25][284/391] Loss_D1: 0.7435 Loss_D2: 0.6050 Loss_G: 3.5797\n",
            "[14/25][285/391] Loss_D1: 0.6081 Loss_D2: 0.6377 Loss_G: 5.1815\n",
            "[14/25][286/391] Loss_D1: 0.4303 Loss_D2: 1.0199 Loss_G: 5.7173\n",
            "[14/25][287/391] Loss_D1: 0.3967 Loss_D2: 0.8272 Loss_G: 4.6582\n",
            "[14/25][288/391] Loss_D1: 0.3784 Loss_D2: 0.9215 Loss_G: 3.9071\n",
            "[14/25][289/391] Loss_D1: 0.4679 Loss_D2: 0.9577 Loss_G: 8.4359\n",
            "[14/25][290/391] Loss_D1: 0.6520 Loss_D2: 0.7204 Loss_G: 3.6645\n",
            "[14/25][291/391] Loss_D1: 0.3989 Loss_D2: 0.7142 Loss_G: 6.2043\n",
            "[14/25][292/391] Loss_D1: 0.4221 Loss_D2: 0.6854 Loss_G: 5.9231\n",
            "[14/25][293/391] Loss_D1: 0.6267 Loss_D2: 0.5467 Loss_G: 4.5131\n",
            "[14/25][294/391] Loss_D1: 0.7193 Loss_D2: 0.7130 Loss_G: 6.6725\n",
            "[14/25][295/391] Loss_D1: 0.3644 Loss_D2: 0.7555 Loss_G: 4.6365\n",
            "[14/25][296/391] Loss_D1: 0.5375 Loss_D2: 0.5890 Loss_G: 4.8149\n",
            "[14/25][297/391] Loss_D1: 0.3376 Loss_D2: 0.6928 Loss_G: 5.7870\n",
            "[14/25][298/391] Loss_D1: 0.3775 Loss_D2: 0.6252 Loss_G: 4.0027\n",
            "[14/25][299/391] Loss_D1: 0.3617 Loss_D2: 0.7698 Loss_G: 6.1851\n",
            "[14/25][300/391] Loss_D1: 0.4844 Loss_D2: 0.5629 Loss_G: 4.2203\n",
            "saving the output\n",
            "[14/25][301/391] Loss_D1: 0.5418 Loss_D2: 0.5274 Loss_G: 5.1773\n",
            "[14/25][302/391] Loss_D1: 0.5009 Loss_D2: 0.5628 Loss_G: 4.3296\n",
            "[14/25][303/391] Loss_D1: 0.4704 Loss_D2: 0.6104 Loss_G: 6.0864\n",
            "[14/25][304/391] Loss_D1: 0.4279 Loss_D2: 0.5899 Loss_G: 4.9523\n",
            "[14/25][305/391] Loss_D1: 0.4479 Loss_D2: 0.5603 Loss_G: 5.1518\n",
            "[14/25][306/391] Loss_D1: 0.3343 Loss_D2: 0.5685 Loss_G: 4.7258\n",
            "[14/25][307/391] Loss_D1: 0.3383 Loss_D2: 0.4145 Loss_G: 5.7078\n",
            "[14/25][308/391] Loss_D1: 0.4537 Loss_D2: 0.8339 Loss_G: 3.8184\n",
            "[14/25][309/391] Loss_D1: 0.3258 Loss_D2: 0.4509 Loss_G: 5.0952\n",
            "[14/25][310/391] Loss_D1: 0.2679 Loss_D2: 0.4575 Loss_G: 5.4616\n",
            "[14/25][311/391] Loss_D1: 0.3318 Loss_D2: 0.5949 Loss_G: 6.8000\n",
            "[14/25][312/391] Loss_D1: 0.3763 Loss_D2: 0.5711 Loss_G: 4.4872\n",
            "[14/25][313/391] Loss_D1: 0.3412 Loss_D2: 0.6029 Loss_G: 3.6218\n",
            "[14/25][314/391] Loss_D1: 0.5417 Loss_D2: 0.7099 Loss_G: 6.8666\n",
            "[14/25][315/391] Loss_D1: 0.3727 Loss_D2: 0.8862 Loss_G: 3.6489\n",
            "[14/25][316/391] Loss_D1: 0.2479 Loss_D2: 0.6164 Loss_G: 5.5802\n",
            "[14/25][317/391] Loss_D1: 0.3665 Loss_D2: 0.4588 Loss_G: 5.4853\n",
            "[14/25][318/391] Loss_D1: 0.3910 Loss_D2: 0.6471 Loss_G: 4.5088\n",
            "[14/25][319/391] Loss_D1: 0.3612 Loss_D2: 0.4915 Loss_G: 5.0707\n",
            "[14/25][320/391] Loss_D1: 0.4456 Loss_D2: 0.4969 Loss_G: 5.0688\n",
            "[14/25][321/391] Loss_D1: 0.3252 Loss_D2: 0.5898 Loss_G: 4.4708\n",
            "[14/25][322/391] Loss_D1: 0.4439 Loss_D2: 0.6909 Loss_G: 5.4651\n",
            "[14/25][323/391] Loss_D1: 0.4260 Loss_D2: 0.6232 Loss_G: 5.0994\n",
            "[14/25][324/391] Loss_D1: 0.3097 Loss_D2: 0.4714 Loss_G: 4.7372\n",
            "[14/25][325/391] Loss_D1: 0.6055 Loss_D2: 0.6297 Loss_G: 4.8138\n",
            "[14/25][326/391] Loss_D1: 0.4145 Loss_D2: 0.5009 Loss_G: 5.3186\n",
            "[14/25][327/391] Loss_D1: 0.4500 Loss_D2: 0.5016 Loss_G: 4.4333\n",
            "[14/25][328/391] Loss_D1: 0.2941 Loss_D2: 0.4908 Loss_G: 5.7469\n",
            "[14/25][329/391] Loss_D1: 0.3908 Loss_D2: 0.4197 Loss_G: 5.0775\n",
            "[14/25][330/391] Loss_D1: 0.5547 Loss_D2: 0.4522 Loss_G: 5.2199\n",
            "[14/25][331/391] Loss_D1: 0.3384 Loss_D2: 0.4386 Loss_G: 5.4629\n",
            "[14/25][332/391] Loss_D1: 0.3336 Loss_D2: 0.6428 Loss_G: 3.5923\n",
            "[14/25][333/391] Loss_D1: 0.4310 Loss_D2: 0.6327 Loss_G: 7.0848\n",
            "[14/25][334/391] Loss_D1: 0.5335 Loss_D2: 0.6560 Loss_G: 3.8320\n",
            "[14/25][335/391] Loss_D1: 0.3578 Loss_D2: 0.4105 Loss_G: 5.7212\n",
            "[14/25][336/391] Loss_D1: 0.3645 Loss_D2: 0.5930 Loss_G: 5.3809\n",
            "[14/25][337/391] Loss_D1: 0.2190 Loss_D2: 0.4642 Loss_G: 5.1236\n",
            "[14/25][338/391] Loss_D1: 0.4374 Loss_D2: 0.5165 Loss_G: 3.6879\n",
            "[14/25][339/391] Loss_D1: 0.3874 Loss_D2: 0.4757 Loss_G: 6.7087\n",
            "[14/25][340/391] Loss_D1: 0.3782 Loss_D2: 0.8542 Loss_G: 3.9753\n",
            "[14/25][341/391] Loss_D1: 0.5155 Loss_D2: 0.7208 Loss_G: 5.5176\n",
            "[14/25][342/391] Loss_D1: 0.3492 Loss_D2: 0.5409 Loss_G: 4.7333\n",
            "[14/25][343/391] Loss_D1: 0.5114 Loss_D2: 0.3767 Loss_G: 6.0104\n",
            "[14/25][344/391] Loss_D1: 0.5605 Loss_D2: 0.5646 Loss_G: 3.3897\n",
            "[14/25][345/391] Loss_D1: 0.6022 Loss_D2: 0.6478 Loss_G: 4.9696\n",
            "[14/25][346/391] Loss_D1: 0.3703 Loss_D2: 0.3999 Loss_G: 5.8351\n",
            "[14/25][347/391] Loss_D1: 0.5432 Loss_D2: 0.4378 Loss_G: 3.7450\n",
            "[14/25][348/391] Loss_D1: 0.6678 Loss_D2: 0.5740 Loss_G: 5.8374\n",
            "[14/25][349/391] Loss_D1: 0.3032 Loss_D2: 0.4928 Loss_G: 4.8975\n",
            "[14/25][350/391] Loss_D1: 0.5364 Loss_D2: 0.5892 Loss_G: 4.3871\n",
            "[14/25][351/391] Loss_D1: 0.3968 Loss_D2: 0.4390 Loss_G: 5.7193\n",
            "[14/25][352/391] Loss_D1: 0.4161 Loss_D2: 0.3894 Loss_G: 4.4489\n",
            "[14/25][353/391] Loss_D1: 0.4384 Loss_D2: 0.5421 Loss_G: 6.2134\n",
            "[14/25][354/391] Loss_D1: 0.3009 Loss_D2: 0.3758 Loss_G: 6.1955\n",
            "[14/25][355/391] Loss_D1: 0.4545 Loss_D2: 0.5794 Loss_G: 2.9788\n",
            "[14/25][356/391] Loss_D1: 0.5044 Loss_D2: 0.9023 Loss_G: 6.9057\n",
            "[14/25][357/391] Loss_D1: 0.4161 Loss_D2: 0.7711 Loss_G: 3.6302\n",
            "[14/25][358/391] Loss_D1: 0.4273 Loss_D2: 0.8128 Loss_G: 5.9581\n",
            "[14/25][359/391] Loss_D1: 0.3340 Loss_D2: 0.8680 Loss_G: 4.8280\n",
            "[14/25][360/391] Loss_D1: 0.4388 Loss_D2: 0.6052 Loss_G: 5.1310\n",
            "[14/25][361/391] Loss_D1: 0.4132 Loss_D2: 0.4967 Loss_G: 5.9135\n",
            "[14/25][362/391] Loss_D1: 0.3274 Loss_D2: 0.5626 Loss_G: 4.7275\n",
            "[14/25][363/391] Loss_D1: 0.4017 Loss_D2: 0.4572 Loss_G: 4.9926\n",
            "[14/25][364/391] Loss_D1: 0.3447 Loss_D2: 0.3715 Loss_G: 5.5304\n",
            "[14/25][365/391] Loss_D1: 0.4037 Loss_D2: 0.3339 Loss_G: 5.8451\n",
            "[14/25][366/391] Loss_D1: 0.5572 Loss_D2: 0.7370 Loss_G: 2.5797\n",
            "[14/25][367/391] Loss_D1: 0.5056 Loss_D2: 1.0588 Loss_G: 7.9056\n",
            "[14/25][368/391] Loss_D1: 0.4626 Loss_D2: 1.3086 Loss_G: 2.8368\n",
            "[14/25][369/391] Loss_D1: 0.6067 Loss_D2: 1.4227 Loss_G: 6.8120\n",
            "[14/25][370/391] Loss_D1: 0.3199 Loss_D2: 0.8166 Loss_G: 5.7283\n",
            "[14/25][371/391] Loss_D1: 0.3734 Loss_D2: 0.7257 Loss_G: 5.1378\n",
            "[14/25][372/391] Loss_D1: 0.4106 Loss_D2: 0.6395 Loss_G: 4.0550\n",
            "[14/25][373/391] Loss_D1: 0.4384 Loss_D2: 0.5786 Loss_G: 6.0724\n",
            "[14/25][374/391] Loss_D1: 0.2966 Loss_D2: 0.5060 Loss_G: 5.9677\n",
            "[14/25][375/391] Loss_D1: 0.4187 Loss_D2: 0.5277 Loss_G: 4.2936\n",
            "[14/25][376/391] Loss_D1: 0.3533 Loss_D2: 0.5948 Loss_G: 6.0828\n",
            "[14/25][377/391] Loss_D1: 0.2237 Loss_D2: 0.4521 Loss_G: 5.8501\n",
            "[14/25][378/391] Loss_D1: 0.5075 Loss_D2: 0.3818 Loss_G: 4.2943\n",
            "[14/25][379/391] Loss_D1: 0.4906 Loss_D2: 0.4179 Loss_G: 6.0930\n",
            "[14/25][380/391] Loss_D1: 0.2856 Loss_D2: 0.4018 Loss_G: 5.6051\n",
            "[14/25][381/391] Loss_D1: 0.3537 Loss_D2: 0.3942 Loss_G: 4.2530\n",
            "[14/25][382/391] Loss_D1: 0.4049 Loss_D2: 0.2796 Loss_G: 6.9720\n",
            "[14/25][383/391] Loss_D1: 0.7100 Loss_D2: 0.7583 Loss_G: 2.5280\n",
            "[14/25][384/391] Loss_D1: 1.0515 Loss_D2: 0.8855 Loss_G: 7.9357\n",
            "[14/25][385/391] Loss_D1: 0.5727 Loss_D2: 0.4333 Loss_G: 5.8108\n",
            "[14/25][386/391] Loss_D1: 0.2532 Loss_D2: 0.6642 Loss_G: 3.9414\n",
            "[14/25][387/391] Loss_D1: 0.4981 Loss_D2: 0.6537 Loss_G: 8.4112\n",
            "[14/25][388/391] Loss_D1: 1.0355 Loss_D2: 0.5469 Loss_G: 2.7578\n",
            "[14/25][389/391] Loss_D1: 1.5263 Loss_D2: 0.5734 Loss_G: 8.3658\n",
            "[14/25][390/391] Loss_D1: 1.8164 Loss_D2: 0.7147 Loss_G: 2.4421\n",
            "[15/25][0/391] Loss_D1: 2.5639 Loss_D2: 0.4802 Loss_G: 9.5844\n",
            "saving the output\n",
            "[15/25][1/391] Loss_D1: 1.3752 Loss_D2: 0.3783 Loss_G: 3.5272\n",
            "[15/25][2/391] Loss_D1: 1.2115 Loss_D2: 0.6230 Loss_G: 7.1578\n",
            "[15/25][3/391] Loss_D1: 2.4961 Loss_D2: 0.5624 Loss_G: 3.6505\n",
            "[15/25][4/391] Loss_D1: 3.5797 Loss_D2: 0.6001 Loss_G: 6.3090\n",
            "[15/25][5/391] Loss_D1: 1.0569 Loss_D2: 0.4302 Loss_G: 6.0463\n",
            "[15/25][6/391] Loss_D1: 0.9215 Loss_D2: 0.5238 Loss_G: 3.8004\n",
            "[15/25][7/391] Loss_D1: 1.1867 Loss_D2: 0.7287 Loss_G: 6.9951\n",
            "[15/25][8/391] Loss_D1: 1.5067 Loss_D2: 0.5456 Loss_G: 4.0746\n",
            "[15/25][9/391] Loss_D1: 0.8937 Loss_D2: 0.6190 Loss_G: 6.8417\n",
            "[15/25][10/391] Loss_D1: 0.5235 Loss_D2: 0.6480 Loss_G: 3.9335\n",
            "[15/25][11/391] Loss_D1: 0.5626 Loss_D2: 1.0312 Loss_G: 8.2608\n",
            "[15/25][12/391] Loss_D1: 0.3828 Loss_D2: 1.3011 Loss_G: 3.4183\n",
            "[15/25][13/391] Loss_D1: 0.6200 Loss_D2: 1.2377 Loss_G: 7.4713\n",
            "[15/25][14/391] Loss_D1: 0.7157 Loss_D2: 1.1340 Loss_G: 4.2358\n",
            "[15/25][15/391] Loss_D1: 0.7772 Loss_D2: 1.3559 Loss_G: 6.7359\n",
            "[15/25][16/391] Loss_D1: 0.7995 Loss_D2: 0.9716 Loss_G: 5.2616\n",
            "[15/25][17/391] Loss_D1: 0.4338 Loss_D2: 0.6450 Loss_G: 6.3909\n",
            "[15/25][18/391] Loss_D1: 1.2914 Loss_D2: 0.7686 Loss_G: 3.1941\n",
            "[15/25][19/391] Loss_D1: 1.6124 Loss_D2: 0.6623 Loss_G: 7.1032\n",
            "[15/25][20/391] Loss_D1: 0.4704 Loss_D2: 0.4753 Loss_G: 6.1337\n",
            "[15/25][21/391] Loss_D1: 1.0439 Loss_D2: 0.5791 Loss_G: 4.2847\n",
            "[15/25][22/391] Loss_D1: 2.1980 Loss_D2: 0.8790 Loss_G: 7.0338\n",
            "[15/25][23/391] Loss_D1: 1.8104 Loss_D2: 0.6672 Loss_G: 3.4992\n",
            "[15/25][24/391] Loss_D1: 1.1563 Loss_D2: 0.5679 Loss_G: 6.5790\n",
            "[15/25][25/391] Loss_D1: 0.7060 Loss_D2: 0.8248 Loss_G: 4.8964\n",
            "[15/25][26/391] Loss_D1: 1.2358 Loss_D2: 0.9129 Loss_G: 4.6810\n",
            "[15/25][27/391] Loss_D1: 1.3202 Loss_D2: 0.6201 Loss_G: 8.5458\n",
            "[15/25][28/391] Loss_D1: 1.1238 Loss_D2: 0.5296 Loss_G: 3.0497\n",
            "[15/25][29/391] Loss_D1: 0.9284 Loss_D2: 0.7193 Loss_G: 8.1843\n",
            "[15/25][30/391] Loss_D1: 0.6548 Loss_D2: 0.6838 Loss_G: 4.4547\n",
            "[15/25][31/391] Loss_D1: 0.6325 Loss_D2: 0.5607 Loss_G: 4.4613\n",
            "[15/25][32/391] Loss_D1: 0.5755 Loss_D2: 0.7495 Loss_G: 6.8625\n",
            "[15/25][33/391] Loss_D1: 0.5059 Loss_D2: 0.5609 Loss_G: 6.2505\n",
            "[15/25][34/391] Loss_D1: 0.6428 Loss_D2: 0.3835 Loss_G: 4.2631\n",
            "[15/25][35/391] Loss_D1: 0.5212 Loss_D2: 0.4706 Loss_G: 5.7288\n",
            "[15/25][36/391] Loss_D1: 0.6605 Loss_D2: 0.6298 Loss_G: 3.7219\n",
            "[15/25][37/391] Loss_D1: 0.7148 Loss_D2: 0.4642 Loss_G: 7.1109\n",
            "[15/25][38/391] Loss_D1: 1.0644 Loss_D2: 0.5374 Loss_G: 4.1438\n",
            "[15/25][39/391] Loss_D1: 1.0201 Loss_D2: 0.5413 Loss_G: 5.5962\n",
            "[15/25][40/391] Loss_D1: 0.5596 Loss_D2: 0.5058 Loss_G: 6.6299\n",
            "[15/25][41/391] Loss_D1: 0.7406 Loss_D2: 0.9744 Loss_G: 2.3856\n",
            "[15/25][42/391] Loss_D1: 0.6942 Loss_D2: 1.0054 Loss_G: 7.1671\n",
            "[15/25][43/391] Loss_D1: 0.4616 Loss_D2: 0.5880 Loss_G: 5.5710\n",
            "[15/25][44/391] Loss_D1: 0.3017 Loss_D2: 0.5230 Loss_G: 6.1079\n",
            "[15/25][45/391] Loss_D1: 0.5994 Loss_D2: 0.6054 Loss_G: 3.2041\n",
            "[15/25][46/391] Loss_D1: 0.7455 Loss_D2: 0.7427 Loss_G: 8.1288\n",
            "[15/25][47/391] Loss_D1: 0.5552 Loss_D2: 0.7192 Loss_G: 3.7704\n",
            "[15/25][48/391] Loss_D1: 0.5400 Loss_D2: 0.5937 Loss_G: 4.7932\n",
            "[15/25][49/391] Loss_D1: 0.4596 Loss_D2: 0.5673 Loss_G: 5.1808\n",
            "[15/25][50/391] Loss_D1: 0.5550 Loss_D2: 0.4534 Loss_G: 5.9204\n",
            "[15/25][51/391] Loss_D1: 0.4348 Loss_D2: 0.4795 Loss_G: 5.5856\n",
            "[15/25][52/391] Loss_D1: 0.3832 Loss_D2: 0.4591 Loss_G: 4.6370\n",
            "[15/25][53/391] Loss_D1: 0.5146 Loss_D2: 0.3877 Loss_G: 6.5414\n",
            "[15/25][54/391] Loss_D1: 0.2459 Loss_D2: 0.6288 Loss_G: 5.5379\n",
            "[15/25][55/391] Loss_D1: 0.3739 Loss_D2: 0.5020 Loss_G: 4.4668\n",
            "[15/25][56/391] Loss_D1: 0.6280 Loss_D2: 0.5945 Loss_G: 5.6030\n",
            "[15/25][57/391] Loss_D1: 0.4788 Loss_D2: 0.4035 Loss_G: 5.5601\n",
            "[15/25][58/391] Loss_D1: 0.2793 Loss_D2: 0.4997 Loss_G: 5.7815\n",
            "[15/25][59/391] Loss_D1: 0.4495 Loss_D2: 0.5516 Loss_G: 5.6779\n",
            "[15/25][60/391] Loss_D1: 0.3709 Loss_D2: 0.7230 Loss_G: 3.2546\n",
            "[15/25][61/391] Loss_D1: 0.4265 Loss_D2: 0.9126 Loss_G: 7.3358\n",
            "[15/25][62/391] Loss_D1: 0.3320 Loss_D2: 0.8849 Loss_G: 4.6984\n",
            "[15/25][63/391] Loss_D1: 0.3416 Loss_D2: 0.4591 Loss_G: 4.4581\n",
            "[15/25][64/391] Loss_D1: 0.5769 Loss_D2: 0.7251 Loss_G: 5.6781\n",
            "[15/25][65/391] Loss_D1: 0.4443 Loss_D2: 0.7358 Loss_G: 4.8029\n",
            "[15/25][66/391] Loss_D1: 0.2624 Loss_D2: 0.5677 Loss_G: 6.7044\n",
            "[15/25][67/391] Loss_D1: 0.4284 Loss_D2: 0.3824 Loss_G: 5.2011\n",
            "[15/25][68/391] Loss_D1: 0.5685 Loss_D2: 0.7221 Loss_G: 2.6173\n",
            "[15/25][69/391] Loss_D1: 0.6553 Loss_D2: 0.8094 Loss_G: 7.4538\n",
            "[15/25][70/391] Loss_D1: 0.2949 Loss_D2: 0.3484 Loss_G: 6.6167\n",
            "[15/25][71/391] Loss_D1: 0.5663 Loss_D2: 0.4639 Loss_G: 2.7617\n",
            "[15/25][72/391] Loss_D1: 0.8642 Loss_D2: 0.8965 Loss_G: 7.7123\n",
            "[15/25][73/391] Loss_D1: 0.5153 Loss_D2: 0.4638 Loss_G: 4.1170\n",
            "[15/25][74/391] Loss_D1: 0.4932 Loss_D2: 0.6401 Loss_G: 4.9807\n",
            "[15/25][75/391] Loss_D1: 0.4720 Loss_D2: 0.5129 Loss_G: 5.5850\n",
            "[15/25][76/391] Loss_D1: 0.4789 Loss_D2: 0.5659 Loss_G: 4.0830\n",
            "[15/25][77/391] Loss_D1: 0.4703 Loss_D2: 0.5246 Loss_G: 5.7148\n",
            "[15/25][78/391] Loss_D1: 0.3332 Loss_D2: 0.7848 Loss_G: 4.4322\n",
            "[15/25][79/391] Loss_D1: 0.4863 Loss_D2: 0.5754 Loss_G: 5.7645\n",
            "[15/25][80/391] Loss_D1: 0.4459 Loss_D2: 0.5512 Loss_G: 4.9974\n",
            "[15/25][81/391] Loss_D1: 0.4784 Loss_D2: 0.5796 Loss_G: 4.7592\n",
            "[15/25][82/391] Loss_D1: 0.4460 Loss_D2: 0.4214 Loss_G: 6.5338\n",
            "[15/25][83/391] Loss_D1: 0.4665 Loss_D2: 0.6551 Loss_G: 3.9305\n",
            "[15/25][84/391] Loss_D1: 0.3877 Loss_D2: 0.5181 Loss_G: 5.3010\n",
            "[15/25][85/391] Loss_D1: 0.4839 Loss_D2: 0.4403 Loss_G: 7.0913\n",
            "[15/25][86/391] Loss_D1: 0.7195 Loss_D2: 0.3809 Loss_G: 4.7156\n",
            "[15/25][87/391] Loss_D1: 0.4946 Loss_D2: 0.3732 Loss_G: 5.2960\n",
            "[15/25][88/391] Loss_D1: 0.4423 Loss_D2: 0.5509 Loss_G: 4.6662\n",
            "[15/25][89/391] Loss_D1: 0.4285 Loss_D2: 0.7217 Loss_G: 6.0873\n",
            "[15/25][90/391] Loss_D1: 0.7923 Loss_D2: 0.7257 Loss_G: 3.2384\n",
            "[15/25][91/391] Loss_D1: 0.5283 Loss_D2: 0.5739 Loss_G: 5.9706\n",
            "[15/25][92/391] Loss_D1: 0.3414 Loss_D2: 0.6480 Loss_G: 4.7424\n",
            "[15/25][93/391] Loss_D1: 0.5697 Loss_D2: 0.6949 Loss_G: 5.4067\n",
            "[15/25][94/391] Loss_D1: 0.5280 Loss_D2: 0.5192 Loss_G: 4.6439\n",
            "[15/25][95/391] Loss_D1: 0.4033 Loss_D2: 0.4397 Loss_G: 4.9880\n",
            "[15/25][96/391] Loss_D1: 0.2435 Loss_D2: 0.6009 Loss_G: 6.7207\n",
            "[15/25][97/391] Loss_D1: 0.4155 Loss_D2: 0.6122 Loss_G: 4.3390\n",
            "[15/25][98/391] Loss_D1: 0.5248 Loss_D2: 0.4997 Loss_G: 4.2267\n",
            "[15/25][99/391] Loss_D1: 0.4953 Loss_D2: 0.6400 Loss_G: 6.0424\n",
            "[15/25][100/391] Loss_D1: 0.3595 Loss_D2: 0.4234 Loss_G: 5.8852\n",
            "saving the output\n",
            "[15/25][101/391] Loss_D1: 0.3151 Loss_D2: 0.4873 Loss_G: 5.0338\n",
            "[15/25][102/391] Loss_D1: 0.5309 Loss_D2: 0.6022 Loss_G: 4.2207\n",
            "[15/25][103/391] Loss_D1: 0.3918 Loss_D2: 0.4103 Loss_G: 4.9604\n",
            "[15/25][104/391] Loss_D1: 0.3898 Loss_D2: 0.4184 Loss_G: 6.2624\n",
            "[15/25][105/391] Loss_D1: 0.4928 Loss_D2: 0.4442 Loss_G: 5.0536\n",
            "[15/25][106/391] Loss_D1: 0.4000 Loss_D2: 0.5310 Loss_G: 3.7844\n",
            "[15/25][107/391] Loss_D1: 0.4856 Loss_D2: 0.4845 Loss_G: 5.7800\n",
            "[15/25][108/391] Loss_D1: 0.3164 Loss_D2: 0.5492 Loss_G: 5.0282\n",
            "[15/25][109/391] Loss_D1: 0.3903 Loss_D2: 0.5706 Loss_G: 6.0351\n",
            "[15/25][110/391] Loss_D1: 0.3572 Loss_D2: 0.5989 Loss_G: 4.6835\n",
            "[15/25][111/391] Loss_D1: 0.3220 Loss_D2: 0.6014 Loss_G: 5.5755\n",
            "[15/25][112/391] Loss_D1: 0.4086 Loss_D2: 0.6637 Loss_G: 3.9698\n",
            "[15/25][113/391] Loss_D1: 0.4212 Loss_D2: 0.6322 Loss_G: 5.3157\n",
            "[15/25][114/391] Loss_D1: 0.4797 Loss_D2: 0.5654 Loss_G: 5.3828\n",
            "[15/25][115/391] Loss_D1: 0.3070 Loss_D2: 0.4493 Loss_G: 5.9692\n",
            "[15/25][116/391] Loss_D1: 0.4865 Loss_D2: 0.3944 Loss_G: 4.1139\n",
            "[15/25][117/391] Loss_D1: 0.3521 Loss_D2: 0.5568 Loss_G: 6.4044\n",
            "[15/25][118/391] Loss_D1: 0.3797 Loss_D2: 0.5303 Loss_G: 4.5795\n",
            "[15/25][119/391] Loss_D1: 0.3753 Loss_D2: 0.4069 Loss_G: 5.7660\n",
            "[15/25][120/391] Loss_D1: 0.3936 Loss_D2: 0.4175 Loss_G: 5.5721\n",
            "[15/25][121/391] Loss_D1: 0.4331 Loss_D2: 0.8271 Loss_G: 4.1211\n",
            "[15/25][122/391] Loss_D1: 0.3370 Loss_D2: 0.6881 Loss_G: 6.0327\n",
            "[15/25][123/391] Loss_D1: 0.2434 Loss_D2: 0.2737 Loss_G: 6.4567\n",
            "[15/25][124/391] Loss_D1: 0.3425 Loss_D2: 0.3417 Loss_G: 4.9790\n",
            "[15/25][125/391] Loss_D1: 0.3835 Loss_D2: 0.3125 Loss_G: 6.0258\n",
            "[15/25][126/391] Loss_D1: 0.3688 Loss_D2: 0.2773 Loss_G: 5.3777\n",
            "[15/25][127/391] Loss_D1: 0.4825 Loss_D2: 0.4557 Loss_G: 4.0760\n",
            "[15/25][128/391] Loss_D1: 0.5153 Loss_D2: 0.4397 Loss_G: 5.8789\n",
            "[15/25][129/391] Loss_D1: 0.2535 Loss_D2: 0.5364 Loss_G: 6.3556\n",
            "[15/25][130/391] Loss_D1: 0.6126 Loss_D2: 0.4976 Loss_G: 2.8425\n",
            "[15/25][131/391] Loss_D1: 0.9682 Loss_D2: 0.5279 Loss_G: 7.9629\n",
            "[15/25][132/391] Loss_D1: 0.6923 Loss_D2: 0.4773 Loss_G: 3.3589\n",
            "[15/25][133/391] Loss_D1: 0.9844 Loss_D2: 0.5130 Loss_G: 8.4150\n",
            "[15/25][134/391] Loss_D1: 0.8170 Loss_D2: 0.5256 Loss_G: 4.0195\n",
            "[15/25][135/391] Loss_D1: 0.7694 Loss_D2: 0.3902 Loss_G: 6.8304\n",
            "[15/25][136/391] Loss_D1: 0.5045 Loss_D2: 0.5163 Loss_G: 4.9948\n",
            "[15/25][137/391] Loss_D1: 0.6373 Loss_D2: 0.4567 Loss_G: 3.9477\n",
            "[15/25][138/391] Loss_D1: 1.4438 Loss_D2: 0.2975 Loss_G: 8.6311\n",
            "[15/25][139/391] Loss_D1: 2.4557 Loss_D2: 0.5136 Loss_G: 2.8792\n",
            "[15/25][140/391] Loss_D1: 0.9559 Loss_D2: 0.5205 Loss_G: 8.8037\n",
            "[15/25][141/391] Loss_D1: 0.9259 Loss_D2: 0.4962 Loss_G: 2.7822\n",
            "[15/25][142/391] Loss_D1: 0.6474 Loss_D2: 0.5767 Loss_G: 6.5761\n",
            "[15/25][143/391] Loss_D1: 0.5538 Loss_D2: 0.5823 Loss_G: 4.5867\n",
            "[15/25][144/391] Loss_D1: 0.6616 Loss_D2: 0.3868 Loss_G: 4.6683\n",
            "[15/25][145/391] Loss_D1: 0.8654 Loss_D2: 0.4166 Loss_G: 8.6036\n",
            "[15/25][146/391] Loss_D1: 1.3480 Loss_D2: 0.4912 Loss_G: 2.7870\n",
            "[15/25][147/391] Loss_D1: 1.0047 Loss_D2: 0.5085 Loss_G: 5.9503\n",
            "[15/25][148/391] Loss_D1: 0.5281 Loss_D2: 0.6248 Loss_G: 5.1964\n",
            "[15/25][149/391] Loss_D1: 0.7047 Loss_D2: 0.7028 Loss_G: 6.1506\n",
            "[15/25][150/391] Loss_D1: 0.8633 Loss_D2: 0.5879 Loss_G: 4.8499\n",
            "[15/25][151/391] Loss_D1: 0.6826 Loss_D2: 0.4424 Loss_G: 5.7560\n",
            "[15/25][152/391] Loss_D1: 0.7068 Loss_D2: 0.8648 Loss_G: 2.5563\n",
            "[15/25][153/391] Loss_D1: 0.4830 Loss_D2: 1.4045 Loss_G: 8.0007\n",
            "[15/25][154/391] Loss_D1: 0.4290 Loss_D2: 0.5842 Loss_G: 3.2376\n",
            "[15/25][155/391] Loss_D1: 0.4553 Loss_D2: 1.9317 Loss_G: 12.9784\n",
            "[15/25][156/391] Loss_D1: 0.6350 Loss_D2: 4.3195 Loss_G: 2.5284\n",
            "[15/25][157/391] Loss_D1: 0.6930 Loss_D2: 1.1172 Loss_G: 9.0483\n",
            "[15/25][158/391] Loss_D1: 0.8361 Loss_D2: 1.6966 Loss_G: 1.5460\n",
            "[15/25][159/391] Loss_D1: 0.6292 Loss_D2: 3.6717 Loss_G: 7.6452\n",
            "[15/25][160/391] Loss_D1: 0.4679 Loss_D2: 1.0128 Loss_G: 5.8779\n",
            "[15/25][161/391] Loss_D1: 0.3814 Loss_D2: 0.7135 Loss_G: 4.5228\n",
            "[15/25][162/391] Loss_D1: 0.3205 Loss_D2: 0.6838 Loss_G: 5.5101\n",
            "[15/25][163/391] Loss_D1: 0.4460 Loss_D2: 0.8878 Loss_G: 5.8735\n",
            "[15/25][164/391] Loss_D1: 0.4258 Loss_D2: 0.7416 Loss_G: 5.1252\n",
            "[15/25][165/391] Loss_D1: 0.5673 Loss_D2: 0.8201 Loss_G: 4.9522\n",
            "[15/25][166/391] Loss_D1: 0.3970 Loss_D2: 0.7349 Loss_G: 5.3806\n",
            "[15/25][167/391] Loss_D1: 0.5058 Loss_D2: 0.8610 Loss_G: 3.2016\n",
            "[15/25][168/391] Loss_D1: 0.5872 Loss_D2: 1.2565 Loss_G: 8.6054\n",
            "[15/25][169/391] Loss_D1: 0.6947 Loss_D2: 1.3781 Loss_G: 2.9294\n",
            "[15/25][170/391] Loss_D1: 0.5949 Loss_D2: 1.0719 Loss_G: 6.9035\n",
            "[15/25][171/391] Loss_D1: 0.3163 Loss_D2: 0.6541 Loss_G: 5.4684\n",
            "[15/25][172/391] Loss_D1: 0.4317 Loss_D2: 0.6695 Loss_G: 4.8507\n",
            "[15/25][173/391] Loss_D1: 0.4930 Loss_D2: 0.7062 Loss_G: 5.7941\n",
            "[15/25][174/391] Loss_D1: 0.5361 Loss_D2: 0.6199 Loss_G: 5.7003\n",
            "[15/25][175/391] Loss_D1: 0.4313 Loss_D2: 0.6089 Loss_G: 5.2117\n",
            "[15/25][176/391] Loss_D1: 0.3726 Loss_D2: 0.7730 Loss_G: 5.1729\n",
            "[15/25][177/391] Loss_D1: 0.3832 Loss_D2: 0.7305 Loss_G: 5.2198\n",
            "[15/25][178/391] Loss_D1: 0.2922 Loss_D2: 0.5431 Loss_G: 5.2370\n",
            "[15/25][179/391] Loss_D1: 0.2367 Loss_D2: 0.7187 Loss_G: 8.0013\n",
            "[15/25][180/391] Loss_D1: 0.4752 Loss_D2: 1.3805 Loss_G: 3.0011\n",
            "[15/25][181/391] Loss_D1: 0.6793 Loss_D2: 2.0549 Loss_G: 8.3738\n",
            "[15/25][182/391] Loss_D1: 0.2157 Loss_D2: 1.6987 Loss_G: 3.7981\n",
            "[15/25][183/391] Loss_D1: 0.4379 Loss_D2: 3.6123 Loss_G: 10.2816\n",
            "[15/25][184/391] Loss_D1: 0.4087 Loss_D2: 2.8144 Loss_G: 2.8748\n",
            "[15/25][185/391] Loss_D1: 0.4712 Loss_D2: 1.4400 Loss_G: 8.1569\n",
            "[15/25][186/391] Loss_D1: 0.3700 Loss_D2: 0.9676 Loss_G: 5.3831\n",
            "[15/25][187/391] Loss_D1: 0.5198 Loss_D2: 0.8012 Loss_G: 3.7181\n",
            "[15/25][188/391] Loss_D1: 0.7075 Loss_D2: 0.9343 Loss_G: 6.7283\n",
            "[15/25][189/391] Loss_D1: 0.5876 Loss_D2: 1.0025 Loss_G: 3.4059\n",
            "[15/25][190/391] Loss_D1: 0.5118 Loss_D2: 0.9073 Loss_G: 5.2722\n",
            "[15/25][191/391] Loss_D1: 0.4093 Loss_D2: 0.8317 Loss_G: 5.4015\n",
            "[15/25][192/391] Loss_D1: 0.3703 Loss_D2: 0.9907 Loss_G: 5.7510\n",
            "[15/25][193/391] Loss_D1: 0.4358 Loss_D2: 0.6690 Loss_G: 5.0397\n",
            "[15/25][194/391] Loss_D1: 0.3266 Loss_D2: 0.6758 Loss_G: 5.1969\n",
            "[15/25][195/391] Loss_D1: 0.3227 Loss_D2: 1.1099 Loss_G: 6.2593\n",
            "[15/25][196/391] Loss_D1: 0.5052 Loss_D2: 1.1339 Loss_G: 3.0910\n",
            "[15/25][197/391] Loss_D1: 0.3476 Loss_D2: 1.0817 Loss_G: 6.5028\n",
            "[15/25][198/391] Loss_D1: 0.4245 Loss_D2: 0.6609 Loss_G: 5.0402\n",
            "[15/25][199/391] Loss_D1: 0.3202 Loss_D2: 0.6424 Loss_G: 3.9797\n",
            "[15/25][200/391] Loss_D1: 0.4733 Loss_D2: 1.2071 Loss_G: 6.8109\n",
            "saving the output\n",
            "[15/25][201/391] Loss_D1: 0.4627 Loss_D2: 1.3362 Loss_G: 3.7909\n",
            "[15/25][202/391] Loss_D1: 0.3456 Loss_D2: 0.7134 Loss_G: 5.6825\n",
            "[15/25][203/391] Loss_D1: 0.4015 Loss_D2: 0.5069 Loss_G: 5.0874\n",
            "[15/25][204/391] Loss_D1: 0.5219 Loss_D2: 0.5567 Loss_G: 5.2413\n",
            "[15/25][205/391] Loss_D1: 0.4106 Loss_D2: 0.5671 Loss_G: 6.2155\n",
            "[15/25][206/391] Loss_D1: 0.2699 Loss_D2: 0.3784 Loss_G: 6.1998\n",
            "[15/25][207/391] Loss_D1: 0.5090 Loss_D2: 0.6143 Loss_G: 3.0505\n",
            "[15/25][208/391] Loss_D1: 0.9607 Loss_D2: 0.6848 Loss_G: 8.4369\n",
            "[15/25][209/391] Loss_D1: 0.8621 Loss_D2: 0.6147 Loss_G: 2.7041\n",
            "[15/25][210/391] Loss_D1: 1.1101 Loss_D2: 0.5705 Loss_G: 9.5139\n",
            "[15/25][211/391] Loss_D1: 1.3261 Loss_D2: 0.6472 Loss_G: 3.0913\n",
            "[15/25][212/391] Loss_D1: 0.7086 Loss_D2: 0.6044 Loss_G: 6.5549\n",
            "[15/25][213/391] Loss_D1: 0.5824 Loss_D2: 0.5029 Loss_G: 5.6461\n",
            "[15/25][214/391] Loss_D1: 0.4571 Loss_D2: 0.7975 Loss_G: 3.5633\n",
            "[15/25][215/391] Loss_D1: 0.3241 Loss_D2: 0.7848 Loss_G: 5.5859\n",
            "[15/25][216/391] Loss_D1: 0.4059 Loss_D2: 0.4548 Loss_G: 6.3463\n",
            "[15/25][217/391] Loss_D1: 0.4249 Loss_D2: 0.6348 Loss_G: 4.4157\n",
            "[15/25][218/391] Loss_D1: 0.3970 Loss_D2: 0.4091 Loss_G: 5.0279\n",
            "[15/25][219/391] Loss_D1: 0.6100 Loss_D2: 0.4918 Loss_G: 4.0591\n",
            "[15/25][220/391] Loss_D1: 0.6308 Loss_D2: 0.3245 Loss_G: 7.3296\n",
            "[15/25][221/391] Loss_D1: 1.1445 Loss_D2: 0.3315 Loss_G: 3.6459\n",
            "[15/25][222/391] Loss_D1: 0.8964 Loss_D2: 0.4692 Loss_G: 7.3052\n",
            "[15/25][223/391] Loss_D1: 0.9574 Loss_D2: 0.7174 Loss_G: 2.5178\n",
            "[15/25][224/391] Loss_D1: 1.0884 Loss_D2: 0.4854 Loss_G: 8.8526\n",
            "[15/25][225/391] Loss_D1: 1.0951 Loss_D2: 0.7974 Loss_G: 2.3942\n",
            "[15/25][226/391] Loss_D1: 0.9932 Loss_D2: 0.7650 Loss_G: 7.8368\n",
            "[15/25][227/391] Loss_D1: 0.8624 Loss_D2: 0.5905 Loss_G: 4.6753\n",
            "[15/25][228/391] Loss_D1: 0.4345 Loss_D2: 0.4728 Loss_G: 4.4897\n",
            "[15/25][229/391] Loss_D1: 0.7632 Loss_D2: 0.5443 Loss_G: 6.0877\n",
            "[15/25][230/391] Loss_D1: 0.6860 Loss_D2: 0.7208 Loss_G: 4.2390\n",
            "[15/25][231/391] Loss_D1: 0.6082 Loss_D2: 0.5170 Loss_G: 5.0683\n",
            "[15/25][232/391] Loss_D1: 0.5622 Loss_D2: 0.6031 Loss_G: 6.1989\n",
            "[15/25][233/391] Loss_D1: 0.3602 Loss_D2: 0.6289 Loss_G: 4.7901\n",
            "[15/25][234/391] Loss_D1: 0.6263 Loss_D2: 0.6701 Loss_G: 3.3250\n",
            "[15/25][235/391] Loss_D1: 0.8848 Loss_D2: 0.5438 Loss_G: 7.2660\n",
            "[15/25][236/391] Loss_D1: 1.5123 Loss_D2: 0.3880 Loss_G: 3.6119\n",
            "[15/25][237/391] Loss_D1: 1.1113 Loss_D2: 0.4219 Loss_G: 7.5386\n",
            "[15/25][238/391] Loss_D1: 0.5506 Loss_D2: 0.6031 Loss_G: 4.7673\n",
            "[15/25][239/391] Loss_D1: 0.5576 Loss_D2: 0.5096 Loss_G: 3.2598\n",
            "[15/25][240/391] Loss_D1: 1.1149 Loss_D2: 0.6047 Loss_G: 9.4432\n",
            "[15/25][241/391] Loss_D1: 1.5891 Loss_D2: 0.5615 Loss_G: 2.8027\n",
            "[15/25][242/391] Loss_D1: 1.3369 Loss_D2: 0.5013 Loss_G: 7.1655\n",
            "[15/25][243/391] Loss_D1: 1.0337 Loss_D2: 0.7629 Loss_G: 3.9508\n",
            "[15/25][244/391] Loss_D1: 0.7571 Loss_D2: 0.5605 Loss_G: 6.3407\n",
            "[15/25][245/391] Loss_D1: 0.5512 Loss_D2: 0.6801 Loss_G: 4.3015\n",
            "[15/25][246/391] Loss_D1: 0.5932 Loss_D2: 0.9118 Loss_G: 5.6713\n",
            "[15/25][247/391] Loss_D1: 0.5854 Loss_D2: 0.6551 Loss_G: 5.6216\n",
            "[15/25][248/391] Loss_D1: 0.6715 Loss_D2: 0.6699 Loss_G: 6.4963\n",
            "[15/25][249/391] Loss_D1: 0.3966 Loss_D2: 0.7477 Loss_G: 4.7625\n",
            "[15/25][250/391] Loss_D1: 0.6916 Loss_D2: 0.7601 Loss_G: 5.4834\n",
            "[15/25][251/391] Loss_D1: 0.4021 Loss_D2: 0.7136 Loss_G: 4.6343\n",
            "[15/25][252/391] Loss_D1: 0.6980 Loss_D2: 0.7657 Loss_G: 4.1302\n",
            "[15/25][253/391] Loss_D1: 0.7750 Loss_D2: 0.5534 Loss_G: 6.4533\n",
            "[15/25][254/391] Loss_D1: 0.5838 Loss_D2: 0.5346 Loss_G: 4.7179\n",
            "[15/25][255/391] Loss_D1: 0.5089 Loss_D2: 0.5809 Loss_G: 5.1118\n",
            "[15/25][256/391] Loss_D1: 0.3219 Loss_D2: 0.4752 Loss_G: 5.6990\n",
            "[15/25][257/391] Loss_D1: 0.5608 Loss_D2: 0.5112 Loss_G: 3.8709\n",
            "[15/25][258/391] Loss_D1: 0.6814 Loss_D2: 0.5322 Loss_G: 6.0872\n",
            "[15/25][259/391] Loss_D1: 0.4819 Loss_D2: 0.5234 Loss_G: 5.2812\n",
            "[15/25][260/391] Loss_D1: 0.4058 Loss_D2: 0.4875 Loss_G: 4.4291\n",
            "[15/25][261/391] Loss_D1: 0.4677 Loss_D2: 0.4392 Loss_G: 5.5864\n",
            "[15/25][262/391] Loss_D1: 0.6627 Loss_D2: 0.4513 Loss_G: 4.3134\n",
            "[15/25][263/391] Loss_D1: 0.6544 Loss_D2: 0.7048 Loss_G: 4.9127\n",
            "[15/25][264/391] Loss_D1: 0.3606 Loss_D2: 0.6096 Loss_G: 5.9844\n",
            "[15/25][265/391] Loss_D1: 0.2930 Loss_D2: 0.5173 Loss_G: 4.9715\n",
            "[15/25][266/391] Loss_D1: 0.4116 Loss_D2: 0.4470 Loss_G: 4.3472\n",
            "[15/25][267/391] Loss_D1: 0.4378 Loss_D2: 0.6050 Loss_G: 5.7793\n",
            "[15/25][268/391] Loss_D1: 0.2260 Loss_D2: 0.3890 Loss_G: 6.2944\n",
            "[15/25][269/391] Loss_D1: 0.4638 Loss_D2: 0.5523 Loss_G: 3.5959\n",
            "[15/25][270/391] Loss_D1: 0.4570 Loss_D2: 0.7999 Loss_G: 6.4542\n",
            "[15/25][271/391] Loss_D1: 0.3874 Loss_D2: 0.6430 Loss_G: 4.3536\n",
            "[15/25][272/391] Loss_D1: 0.3471 Loss_D2: 0.4595 Loss_G: 5.6613\n",
            "[15/25][273/391] Loss_D1: 0.5838 Loss_D2: 0.4570 Loss_G: 4.9498\n",
            "[15/25][274/391] Loss_D1: 0.3941 Loss_D2: 0.6090 Loss_G: 5.2456\n",
            "[15/25][275/391] Loss_D1: 0.4142 Loss_D2: 0.6301 Loss_G: 4.5002\n",
            "[15/25][276/391] Loss_D1: 0.4304 Loss_D2: 0.3873 Loss_G: 4.4465\n",
            "[15/25][277/391] Loss_D1: 0.3600 Loss_D2: 0.4002 Loss_G: 5.8366\n",
            "[15/25][278/391] Loss_D1: 0.3412 Loss_D2: 0.4456 Loss_G: 5.1895\n",
            "[15/25][279/391] Loss_D1: 0.5014 Loss_D2: 0.5267 Loss_G: 4.7854\n",
            "[15/25][280/391] Loss_D1: 0.3753 Loss_D2: 0.3977 Loss_G: 5.6740\n",
            "[15/25][281/391] Loss_D1: 0.4867 Loss_D2: 0.5862 Loss_G: 4.5510\n",
            "[15/25][282/391] Loss_D1: 0.4180 Loss_D2: 0.4110 Loss_G: 4.8445\n",
            "[15/25][283/391] Loss_D1: 0.2963 Loss_D2: 0.3292 Loss_G: 5.8367\n",
            "[15/25][284/391] Loss_D1: 0.4739 Loss_D2: 0.6213 Loss_G: 3.2904\n",
            "[15/25][285/391] Loss_D1: 0.4219 Loss_D2: 0.6750 Loss_G: 6.1454\n",
            "[15/25][286/391] Loss_D1: 0.3004 Loss_D2: 0.4934 Loss_G: 5.8233\n",
            "[15/25][287/391] Loss_D1: 0.3880 Loss_D2: 0.3928 Loss_G: 4.3883\n",
            "[15/25][288/391] Loss_D1: 0.3432 Loss_D2: 0.6050 Loss_G: 5.4855\n",
            "[15/25][289/391] Loss_D1: 0.4924 Loss_D2: 0.6509 Loss_G: 5.1646\n",
            "[15/25][290/391] Loss_D1: 0.4566 Loss_D2: 0.7891 Loss_G: 4.8911\n",
            "[15/25][291/391] Loss_D1: 0.4413 Loss_D2: 0.5034 Loss_G: 4.5772\n",
            "[15/25][292/391] Loss_D1: 0.3699 Loss_D2: 0.5079 Loss_G: 5.5116\n",
            "[15/25][293/391] Loss_D1: 0.4505 Loss_D2: 0.5564 Loss_G: 3.7573\n",
            "[15/25][294/391] Loss_D1: 0.5119 Loss_D2: 0.5090 Loss_G: 6.0333\n",
            "[15/25][295/391] Loss_D1: 0.4915 Loss_D2: 0.4557 Loss_G: 4.9281\n",
            "[15/25][296/391] Loss_D1: 0.3209 Loss_D2: 0.4035 Loss_G: 4.4675\n",
            "[15/25][297/391] Loss_D1: 0.4547 Loss_D2: 0.5058 Loss_G: 6.2310\n",
            "[15/25][298/391] Loss_D1: 0.4025 Loss_D2: 0.5316 Loss_G: 4.5262\n",
            "[15/25][299/391] Loss_D1: 0.3338 Loss_D2: 0.5922 Loss_G: 4.9701\n",
            "[15/25][300/391] Loss_D1: 0.4391 Loss_D2: 0.4586 Loss_G: 4.3322\n",
            "saving the output\n",
            "[15/25][301/391] Loss_D1: 0.3076 Loss_D2: 0.4070 Loss_G: 5.7481\n",
            "[15/25][302/391] Loss_D1: 0.3817 Loss_D2: 0.3860 Loss_G: 6.0049\n",
            "[15/25][303/391] Loss_D1: 0.5209 Loss_D2: 0.4886 Loss_G: 4.4410\n",
            "[15/25][304/391] Loss_D1: 0.4197 Loss_D2: 0.4346 Loss_G: 3.9511\n",
            "[15/25][305/391] Loss_D1: 0.4729 Loss_D2: 0.4069 Loss_G: 6.2488\n",
            "[15/25][306/391] Loss_D1: 0.3581 Loss_D2: 0.6158 Loss_G: 5.1608\n",
            "[15/25][307/391] Loss_D1: 0.5516 Loss_D2: 0.5239 Loss_G: 3.4245\n",
            "[15/25][308/391] Loss_D1: 0.5616 Loss_D2: 0.5871 Loss_G: 6.1402\n",
            "[15/25][309/391] Loss_D1: 0.2603 Loss_D2: 0.3489 Loss_G: 6.3584\n",
            "[15/25][310/391] Loss_D1: 0.3724 Loss_D2: 0.3631 Loss_G: 5.0141\n",
            "[15/25][311/391] Loss_D1: 0.3936 Loss_D2: 0.4031 Loss_G: 5.0291\n",
            "[15/25][312/391] Loss_D1: 0.6863 Loss_D2: 0.5667 Loss_G: 3.4408\n",
            "[15/25][313/391] Loss_D1: 0.6654 Loss_D2: 0.6554 Loss_G: 8.2695\n",
            "[15/25][314/391] Loss_D1: 0.5589 Loss_D2: 0.8471 Loss_G: 3.4208\n",
            "[15/25][315/391] Loss_D1: 0.4966 Loss_D2: 0.7408 Loss_G: 5.8479\n",
            "[15/25][316/391] Loss_D1: 0.3698 Loss_D2: 0.4594 Loss_G: 5.9881\n",
            "[15/25][317/391] Loss_D1: 0.3003 Loss_D2: 0.4277 Loss_G: 5.1227\n",
            "[15/25][318/391] Loss_D1: 0.5696 Loss_D2: 0.7661 Loss_G: 6.1324\n",
            "[15/25][319/391] Loss_D1: 0.4370 Loss_D2: 0.9816 Loss_G: 3.6561\n",
            "[15/25][320/391] Loss_D1: 0.5168 Loss_D2: 0.7415 Loss_G: 6.3506\n",
            "[15/25][321/391] Loss_D1: 0.4173 Loss_D2: 0.7436 Loss_G: 4.1514\n",
            "[15/25][322/391] Loss_D1: 0.3673 Loss_D2: 0.5316 Loss_G: 5.6131\n",
            "[15/25][323/391] Loss_D1: 0.2307 Loss_D2: 0.4628 Loss_G: 5.8268\n",
            "[15/25][324/391] Loss_D1: 0.5301 Loss_D2: 0.6152 Loss_G: 3.8984\n",
            "[15/25][325/391] Loss_D1: 0.4472 Loss_D2: 0.5607 Loss_G: 5.4181\n",
            "[15/25][326/391] Loss_D1: 0.4884 Loss_D2: 0.4610 Loss_G: 4.4716\n",
            "[15/25][327/391] Loss_D1: 0.3796 Loss_D2: 0.4789 Loss_G: 6.3793\n",
            "[15/25][328/391] Loss_D1: 0.3379 Loss_D2: 0.4660 Loss_G: 5.0304\n",
            "[15/25][329/391] Loss_D1: 0.2955 Loss_D2: 0.5389 Loss_G: 6.8946\n",
            "[15/25][330/391] Loss_D1: 0.3856 Loss_D2: 0.4787 Loss_G: 4.0417\n",
            "[15/25][331/391] Loss_D1: 0.3971 Loss_D2: 0.5699 Loss_G: 5.9979\n",
            "[15/25][332/391] Loss_D1: 0.4085 Loss_D2: 0.5627 Loss_G: 4.5844\n",
            "[15/25][333/391] Loss_D1: 0.3649 Loss_D2: 0.5597 Loss_G: 4.3121\n",
            "[15/25][334/391] Loss_D1: 0.5700 Loss_D2: 0.4999 Loss_G: 7.0859\n",
            "[15/25][335/391] Loss_D1: 0.5500 Loss_D2: 0.8328 Loss_G: 3.3220\n",
            "[15/25][336/391] Loss_D1: 0.4628 Loss_D2: 0.6336 Loss_G: 6.5496\n",
            "[15/25][337/391] Loss_D1: 0.2963 Loss_D2: 0.3440 Loss_G: 6.6780\n",
            "[15/25][338/391] Loss_D1: 0.4718 Loss_D2: 0.6337 Loss_G: 2.8278\n",
            "[15/25][339/391] Loss_D1: 0.5252 Loss_D2: 0.6550 Loss_G: 6.7426\n",
            "[15/25][340/391] Loss_D1: 0.3520 Loss_D2: 0.5233 Loss_G: 5.7750\n",
            "[15/25][341/391] Loss_D1: 0.2443 Loss_D2: 0.4705 Loss_G: 5.8803\n",
            "[15/25][342/391] Loss_D1: 0.3520 Loss_D2: 0.4990 Loss_G: 3.5688\n",
            "[15/25][343/391] Loss_D1: 0.3753 Loss_D2: 0.7059 Loss_G: 6.8465\n",
            "[15/25][344/391] Loss_D1: 0.5561 Loss_D2: 0.7556 Loss_G: 3.7272\n",
            "[15/25][345/391] Loss_D1: 0.4509 Loss_D2: 0.5617 Loss_G: 5.5614\n",
            "[15/25][346/391] Loss_D1: 0.2862 Loss_D2: 0.5062 Loss_G: 5.1073\n",
            "[15/25][347/391] Loss_D1: 0.4351 Loss_D2: 0.5895 Loss_G: 5.3816\n",
            "[15/25][348/391] Loss_D1: 0.3532 Loss_D2: 0.5600 Loss_G: 5.9445\n",
            "[15/25][349/391] Loss_D1: 0.3418 Loss_D2: 0.4498 Loss_G: 4.7560\n",
            "[15/25][350/391] Loss_D1: 0.4820 Loss_D2: 0.4748 Loss_G: 3.7196\n",
            "[15/25][351/391] Loss_D1: 0.4790 Loss_D2: 0.5679 Loss_G: 7.8131\n",
            "[15/25][352/391] Loss_D1: 0.5063 Loss_D2: 0.7442 Loss_G: 3.8474\n",
            "[15/25][353/391] Loss_D1: 0.3016 Loss_D2: 0.6223 Loss_G: 7.6326\n",
            "[15/25][354/391] Loss_D1: 0.4208 Loss_D2: 0.6910 Loss_G: 3.7680\n",
            "[15/25][355/391] Loss_D1: 0.3249 Loss_D2: 0.5993 Loss_G: 6.6754\n",
            "[15/25][356/391] Loss_D1: 0.4864 Loss_D2: 0.3730 Loss_G: 4.8234\n",
            "[15/25][357/391] Loss_D1: 0.5211 Loss_D2: 0.5811 Loss_G: 4.9468\n",
            "[15/25][358/391] Loss_D1: 0.2685 Loss_D2: 0.6159 Loss_G: 8.1920\n",
            "[15/25][359/391] Loss_D1: 0.5081 Loss_D2: 0.8065 Loss_G: 3.2522\n",
            "[15/25][360/391] Loss_D1: 0.4940 Loss_D2: 0.8237 Loss_G: 5.8085\n",
            "[15/25][361/391] Loss_D1: 0.4428 Loss_D2: 0.5516 Loss_G: 5.6267\n",
            "[15/25][362/391] Loss_D1: 0.8201 Loss_D2: 0.7007 Loss_G: 3.3566\n",
            "[15/25][363/391] Loss_D1: 0.6834 Loss_D2: 0.4523 Loss_G: 7.3789\n",
            "[15/25][364/391] Loss_D1: 0.2455 Loss_D2: 0.7999 Loss_G: 4.9852\n",
            "[15/25][365/391] Loss_D1: 0.6215 Loss_D2: 0.7356 Loss_G: 4.1771\n",
            "[15/25][366/391] Loss_D1: 1.0027 Loss_D2: 0.4949 Loss_G: 7.1288\n",
            "[15/25][367/391] Loss_D1: 0.8053 Loss_D2: 0.5316 Loss_G: 4.4491\n",
            "[15/25][368/391] Loss_D1: 1.6949 Loss_D2: 0.5324 Loss_G: 9.1749\n",
            "[15/25][369/391] Loss_D1: 2.3370 Loss_D2: 0.5898 Loss_G: 2.4035\n",
            "[15/25][370/391] Loss_D1: 3.2499 Loss_D2: 0.8272 Loss_G: 8.8604\n",
            "[15/25][371/391] Loss_D1: 3.0795 Loss_D2: 0.6691 Loss_G: 4.1819\n",
            "[15/25][372/391] Loss_D1: 4.6946 Loss_D2: 0.9014 Loss_G: 8.4456\n",
            "[15/25][373/391] Loss_D1: 3.2168 Loss_D2: 0.7251 Loss_G: 4.6093\n",
            "[15/25][374/391] Loss_D1: 1.5330 Loss_D2: 0.5689 Loss_G: 7.0468\n",
            "[15/25][375/391] Loss_D1: 1.5755 Loss_D2: 0.4771 Loss_G: 2.8602\n",
            "[15/25][376/391] Loss_D1: 1.9368 Loss_D2: 0.5398 Loss_G: 9.1913\n",
            "[15/25][377/391] Loss_D1: 2.7522 Loss_D2: 0.6145 Loss_G: 1.7352\n",
            "[15/25][378/391] Loss_D1: 2.8766 Loss_D2: 0.6924 Loss_G: 9.4034\n",
            "[15/25][379/391] Loss_D1: 1.3116 Loss_D2: 0.6227 Loss_G: 4.0052\n",
            "[15/25][380/391] Loss_D1: 0.6729 Loss_D2: 0.4572 Loss_G: 5.2196\n",
            "[15/25][381/391] Loss_D1: 0.7867 Loss_D2: 0.6067 Loss_G: 6.5391\n",
            "[15/25][382/391] Loss_D1: 0.7772 Loss_D2: 0.7996 Loss_G: 5.5403\n",
            "[15/25][383/391] Loss_D1: 1.0538 Loss_D2: 0.5589 Loss_G: 4.1693\n",
            "[15/25][384/391] Loss_D1: 0.8022 Loss_D2: 0.4575 Loss_G: 6.2669\n",
            "[15/25][385/391] Loss_D1: 0.7383 Loss_D2: 0.5910 Loss_G: 5.0156\n",
            "[15/25][386/391] Loss_D1: 0.7002 Loss_D2: 0.4874 Loss_G: 4.2482\n",
            "[15/25][387/391] Loss_D1: 0.5451 Loss_D2: 0.7507 Loss_G: 6.9725\n",
            "[15/25][388/391] Loss_D1: 0.3837 Loss_D2: 0.7524 Loss_G: 5.0927\n",
            "[15/25][389/391] Loss_D1: 1.0103 Loss_D2: 0.9299 Loss_G: 4.5760\n",
            "[15/25][390/391] Loss_D1: 1.0588 Loss_D2: 0.7202 Loss_G: 6.4532\n",
            "[16/25][0/391] Loss_D1: 0.7219 Loss_D2: 0.7101 Loss_G: 6.0891\n",
            "saving the output\n",
            "[16/25][1/391] Loss_D1: 0.6808 Loss_D2: 0.4569 Loss_G: 4.4136\n",
            "[16/25][2/391] Loss_D1: 0.5197 Loss_D2: 0.4187 Loss_G: 6.0099\n",
            "[16/25][3/391] Loss_D1: 0.4619 Loss_D2: 0.4868 Loss_G: 5.9758\n",
            "[16/25][4/391] Loss_D1: 0.4582 Loss_D2: 0.6450 Loss_G: 4.2111\n",
            "[16/25][5/391] Loss_D1: 0.5965 Loss_D2: 0.7183 Loss_G: 5.5419\n",
            "[16/25][6/391] Loss_D1: 0.4456 Loss_D2: 0.3798 Loss_G: 6.8981\n",
            "[16/25][7/391] Loss_D1: 0.5704 Loss_D2: 0.6089 Loss_G: 3.9593\n",
            "[16/25][8/391] Loss_D1: 0.5738 Loss_D2: 0.5154 Loss_G: 5.0583\n",
            "[16/25][9/391] Loss_D1: 0.5764 Loss_D2: 0.6395 Loss_G: 6.6748\n",
            "[16/25][10/391] Loss_D1: 0.4592 Loss_D2: 0.4655 Loss_G: 4.8973\n",
            "[16/25][11/391] Loss_D1: 0.3283 Loss_D2: 0.4347 Loss_G: 5.2527\n",
            "[16/25][12/391] Loss_D1: 0.6444 Loss_D2: 0.4639 Loss_G: 4.9360\n",
            "[16/25][13/391] Loss_D1: 0.3967 Loss_D2: 0.4522 Loss_G: 5.4905\n",
            "[16/25][14/391] Loss_D1: 0.5674 Loss_D2: 0.4095 Loss_G: 4.2424\n",
            "[16/25][15/391] Loss_D1: 0.8582 Loss_D2: 0.3841 Loss_G: 7.0960\n",
            "[16/25][16/391] Loss_D1: 0.8541 Loss_D2: 0.3420 Loss_G: 5.7216\n",
            "[16/25][17/391] Loss_D1: 0.3597 Loss_D2: 0.5050 Loss_G: 4.0472\n",
            "[16/25][18/391] Loss_D1: 0.6324 Loss_D2: 0.4522 Loss_G: 5.9801\n",
            "[16/25][19/391] Loss_D1: 0.3932 Loss_D2: 0.5788 Loss_G: 6.4332\n",
            "[16/25][20/391] Loss_D1: 0.5713 Loss_D2: 0.5959 Loss_G: 3.2091\n",
            "[16/25][21/391] Loss_D1: 0.5211 Loss_D2: 0.6174 Loss_G: 6.0283\n",
            "[16/25][22/391] Loss_D1: 0.3806 Loss_D2: 0.5848 Loss_G: 5.8755\n",
            "[16/25][23/391] Loss_D1: 0.4793 Loss_D2: 0.4512 Loss_G: 4.3578\n",
            "[16/25][24/391] Loss_D1: 0.6152 Loss_D2: 0.4079 Loss_G: 4.3509\n",
            "[16/25][25/391] Loss_D1: 0.6411 Loss_D2: 0.3837 Loss_G: 6.7274\n",
            "[16/25][26/391] Loss_D1: 0.4570 Loss_D2: 0.5153 Loss_G: 4.6608\n",
            "[16/25][27/391] Loss_D1: 0.4165 Loss_D2: 0.4847 Loss_G: 5.6879\n",
            "[16/25][28/391] Loss_D1: 0.4287 Loss_D2: 0.4559 Loss_G: 4.3533\n",
            "[16/25][29/391] Loss_D1: 0.3597 Loss_D2: 0.4881 Loss_G: 5.2868\n",
            "[16/25][30/391] Loss_D1: 0.3286 Loss_D2: 0.4900 Loss_G: 5.5111\n",
            "[16/25][31/391] Loss_D1: 0.5031 Loss_D2: 0.3517 Loss_G: 4.9687\n",
            "[16/25][32/391] Loss_D1: 0.2500 Loss_D2: 0.3990 Loss_G: 5.9167\n",
            "[16/25][33/391] Loss_D1: 0.3971 Loss_D2: 0.5698 Loss_G: 4.3033\n",
            "[16/25][34/391] Loss_D1: 0.5890 Loss_D2: 0.4581 Loss_G: 4.9548\n",
            "[16/25][35/391] Loss_D1: 0.5203 Loss_D2: 0.4697 Loss_G: 5.7475\n",
            "[16/25][36/391] Loss_D1: 0.4133 Loss_D2: 0.5873 Loss_G: 4.2377\n",
            "[16/25][37/391] Loss_D1: 0.4090 Loss_D2: 0.7604 Loss_G: 6.3461\n",
            "[16/25][38/391] Loss_D1: 0.5199 Loss_D2: 0.7283 Loss_G: 3.1908\n",
            "[16/25][39/391] Loss_D1: 0.3669 Loss_D2: 0.7550 Loss_G: 8.0436\n",
            "[16/25][40/391] Loss_D1: 0.2845 Loss_D2: 1.0108 Loss_G: 4.3057\n",
            "[16/25][41/391] Loss_D1: 0.3696 Loss_D2: 0.6075 Loss_G: 6.7050\n",
            "[16/25][42/391] Loss_D1: 0.5017 Loss_D2: 0.3635 Loss_G: 4.7002\n",
            "[16/25][43/391] Loss_D1: 0.5065 Loss_D2: 0.4982 Loss_G: 4.8021\n",
            "[16/25][44/391] Loss_D1: 0.5411 Loss_D2: 0.4046 Loss_G: 5.7195\n",
            "[16/25][45/391] Loss_D1: 0.5060 Loss_D2: 0.3762 Loss_G: 4.6630\n",
            "[16/25][46/391] Loss_D1: 0.3744 Loss_D2: 0.4854 Loss_G: 5.3429\n",
            "[16/25][47/391] Loss_D1: 0.3251 Loss_D2: 0.4704 Loss_G: 4.8027\n",
            "[16/25][48/391] Loss_D1: 0.3931 Loss_D2: 0.5515 Loss_G: 5.5720\n",
            "[16/25][49/391] Loss_D1: 0.4986 Loss_D2: 0.5575 Loss_G: 5.2942\n",
            "[16/25][50/391] Loss_D1: 0.5456 Loss_D2: 0.6178 Loss_G: 4.7775\n",
            "[16/25][51/391] Loss_D1: 0.2683 Loss_D2: 0.3765 Loss_G: 5.4674\n",
            "[16/25][52/391] Loss_D1: 0.5335 Loss_D2: 0.5048 Loss_G: 4.5153\n",
            "[16/25][53/391] Loss_D1: 0.2231 Loss_D2: 0.5304 Loss_G: 6.0729\n",
            "[16/25][54/391] Loss_D1: 0.4258 Loss_D2: 0.4863 Loss_G: 5.4748\n",
            "[16/25][55/391] Loss_D1: 0.5127 Loss_D2: 0.4473 Loss_G: 4.3883\n",
            "[16/25][56/391] Loss_D1: 0.6183 Loss_D2: 0.5891 Loss_G: 5.8501\n",
            "[16/25][57/391] Loss_D1: 0.3459 Loss_D2: 0.3733 Loss_G: 5.3230\n",
            "[16/25][58/391] Loss_D1: 0.5609 Loss_D2: 0.6302 Loss_G: 2.5734\n",
            "[16/25][59/391] Loss_D1: 1.1428 Loss_D2: 0.4030 Loss_G: 7.6816\n",
            "[16/25][60/391] Loss_D1: 0.7776 Loss_D2: 0.4875 Loss_G: 4.3181\n",
            "[16/25][61/391] Loss_D1: 1.0315 Loss_D2: 0.6061 Loss_G: 6.3635\n",
            "[16/25][62/391] Loss_D1: 2.0133 Loss_D2: 0.7387 Loss_G: 5.0902\n",
            "[16/25][63/391] Loss_D1: 0.8642 Loss_D2: 0.4102 Loss_G: 5.9885\n",
            "[16/25][64/391] Loss_D1: 0.7296 Loss_D2: 0.4148 Loss_G: 4.0813\n",
            "[16/25][65/391] Loss_D1: 0.6275 Loss_D2: 0.4760 Loss_G: 5.8208\n",
            "[16/25][66/391] Loss_D1: 0.4573 Loss_D2: 0.3801 Loss_G: 6.0263\n",
            "[16/25][67/391] Loss_D1: 0.4711 Loss_D2: 0.4683 Loss_G: 5.3523\n",
            "[16/25][68/391] Loss_D1: 0.5183 Loss_D2: 0.4071 Loss_G: 3.9860\n",
            "[16/25][69/391] Loss_D1: 0.6357 Loss_D2: 0.6048 Loss_G: 6.5847\n",
            "[16/25][70/391] Loss_D1: 0.8578 Loss_D2: 0.4273 Loss_G: 3.6146\n",
            "[16/25][71/391] Loss_D1: 1.1354 Loss_D2: 0.4864 Loss_G: 8.4023\n",
            "[16/25][72/391] Loss_D1: 0.7325 Loss_D2: 0.5886 Loss_G: 2.5372\n",
            "[16/25][73/391] Loss_D1: 1.2319 Loss_D2: 0.6096 Loss_G: 8.0709\n",
            "[16/25][74/391] Loss_D1: 1.2374 Loss_D2: 0.5925 Loss_G: 3.6914\n",
            "[16/25][75/391] Loss_D1: 0.5559 Loss_D2: 0.6053 Loss_G: 6.3843\n",
            "[16/25][76/391] Loss_D1: 0.7485 Loss_D2: 0.4794 Loss_G: 6.4303\n",
            "[16/25][77/391] Loss_D1: 1.0024 Loss_D2: 0.4784 Loss_G: 3.5572\n",
            "[16/25][78/391] Loss_D1: 1.3186 Loss_D2: 0.4571 Loss_G: 7.6614\n",
            "[16/25][79/391] Loss_D1: 1.0459 Loss_D2: 0.3525 Loss_G: 3.6783\n",
            "[16/25][80/391] Loss_D1: 1.0707 Loss_D2: 0.6106 Loss_G: 8.0973\n",
            "[16/25][81/391] Loss_D1: 0.7875 Loss_D2: 0.8094 Loss_G: 2.6817\n",
            "[16/25][82/391] Loss_D1: 0.6396 Loss_D2: 0.9407 Loss_G: 7.4305\n",
            "[16/25][83/391] Loss_D1: 0.5350 Loss_D2: 0.6880 Loss_G: 3.2479\n",
            "[16/25][84/391] Loss_D1: 0.5161 Loss_D2: 0.6411 Loss_G: 8.3805\n",
            "[16/25][85/391] Loss_D1: 0.5667 Loss_D2: 0.7452 Loss_G: 2.9286\n",
            "[16/25][86/391] Loss_D1: 0.6052 Loss_D2: 1.5449 Loss_G: 10.2004\n",
            "[16/25][87/391] Loss_D1: 0.6349 Loss_D2: 1.9736 Loss_G: 3.0503\n",
            "[16/25][88/391] Loss_D1: 0.5981 Loss_D2: 0.7263 Loss_G: 7.6905\n",
            "[16/25][89/391] Loss_D1: 0.6525 Loss_D2: 0.4150 Loss_G: 4.2632\n",
            "[16/25][90/391] Loss_D1: 0.4856 Loss_D2: 0.5390 Loss_G: 6.1212\n",
            "[16/25][91/391] Loss_D1: 0.5703 Loss_D2: 0.8331 Loss_G: 3.3798\n",
            "[16/25][92/391] Loss_D1: 0.5213 Loss_D2: 1.1945 Loss_G: 6.9393\n",
            "[16/25][93/391] Loss_D1: 0.4866 Loss_D2: 0.8914 Loss_G: 3.1820\n",
            "[16/25][94/391] Loss_D1: 0.4379 Loss_D2: 2.4733 Loss_G: 10.3906\n",
            "[16/25][95/391] Loss_D1: 0.4709 Loss_D2: 2.8436 Loss_G: 2.9538\n",
            "[16/25][96/391] Loss_D1: 0.5434 Loss_D2: 2.7018 Loss_G: 9.2481\n",
            "[16/25][97/391] Loss_D1: 0.5786 Loss_D2: 3.1919 Loss_G: 3.7746\n",
            "[16/25][98/391] Loss_D1: 0.4744 Loss_D2: 2.4483 Loss_G: 7.5007\n",
            "[16/25][99/391] Loss_D1: 0.5010 Loss_D2: 1.3522 Loss_G: 3.8495\n",
            "[16/25][100/391] Loss_D1: 0.3783 Loss_D2: 0.7817 Loss_G: 4.9345\n",
            "saving the output\n",
            "[16/25][101/391] Loss_D1: 0.2690 Loss_D2: 0.9962 Loss_G: 7.1089\n",
            "[16/25][102/391] Loss_D1: 0.5384 Loss_D2: 0.8389 Loss_G: 3.5235\n",
            "[16/25][103/391] Loss_D1: 0.6043 Loss_D2: 0.6797 Loss_G: 4.3344\n",
            "[16/25][104/391] Loss_D1: 0.5145 Loss_D2: 0.6597 Loss_G: 6.5402\n",
            "[16/25][105/391] Loss_D1: 0.7402 Loss_D2: 0.8233 Loss_G: 3.3472\n",
            "[16/25][106/391] Loss_D1: 0.6063 Loss_D2: 0.8029 Loss_G: 4.7517\n",
            "[16/25][107/391] Loss_D1: 0.3332 Loss_D2: 0.8666 Loss_G: 7.1261\n",
            "[16/25][108/391] Loss_D1: 0.7468 Loss_D2: 0.8344 Loss_G: 2.8340\n",
            "[16/25][109/391] Loss_D1: 0.6095 Loss_D2: 0.8821 Loss_G: 6.0564\n",
            "[16/25][110/391] Loss_D1: 0.3931 Loss_D2: 0.6369 Loss_G: 5.5659\n",
            "[16/25][111/391] Loss_D1: 0.3059 Loss_D2: 0.5774 Loss_G: 5.3701\n",
            "[16/25][112/391] Loss_D1: 0.5080 Loss_D2: 0.4205 Loss_G: 5.3930\n",
            "[16/25][113/391] Loss_D1: 0.2419 Loss_D2: 0.6155 Loss_G: 6.2010\n",
            "[16/25][114/391] Loss_D1: 0.5104 Loss_D2: 0.6203 Loss_G: 4.6555\n",
            "[16/25][115/391] Loss_D1: 0.6649 Loss_D2: 0.6627 Loss_G: 4.3861\n",
            "[16/25][116/391] Loss_D1: 0.3308 Loss_D2: 0.4295 Loss_G: 5.9296\n",
            "[16/25][117/391] Loss_D1: 0.2285 Loss_D2: 0.5371 Loss_G: 5.6470\n",
            "[16/25][118/391] Loss_D1: 0.4150 Loss_D2: 0.6219 Loss_G: 5.3495\n",
            "[16/25][119/391] Loss_D1: 0.3140 Loss_D2: 0.6838 Loss_G: 4.6722\n",
            "[16/25][120/391] Loss_D1: 0.2382 Loss_D2: 1.1209 Loss_G: 7.2651\n",
            "[16/25][121/391] Loss_D1: 0.2887 Loss_D2: 1.0609 Loss_G: 4.3354\n",
            "[16/25][122/391] Loss_D1: 0.4559 Loss_D2: 0.8640 Loss_G: 5.5987\n",
            "[16/25][123/391] Loss_D1: 0.5787 Loss_D2: 0.4666 Loss_G: 5.5589\n",
            "[16/25][124/391] Loss_D1: 0.3789 Loss_D2: 0.7923 Loss_G: 3.4392\n",
            "[16/25][125/391] Loss_D1: 0.4343 Loss_D2: 0.8435 Loss_G: 6.4006\n",
            "[16/25][126/391] Loss_D1: 0.4664 Loss_D2: 0.5464 Loss_G: 5.3540\n",
            "[16/25][127/391] Loss_D1: 0.2764 Loss_D2: 0.3671 Loss_G: 5.2616\n",
            "[16/25][128/391] Loss_D1: 0.4495 Loss_D2: 0.5629 Loss_G: 5.4905\n",
            "[16/25][129/391] Loss_D1: 0.5549 Loss_D2: 0.6831 Loss_G: 5.2365\n",
            "[16/25][130/391] Loss_D1: 0.4714 Loss_D2: 0.7342 Loss_G: 4.7990\n",
            "[16/25][131/391] Loss_D1: 0.3630 Loss_D2: 0.6893 Loss_G: 4.3747\n",
            "[16/25][132/391] Loss_D1: 0.4791 Loss_D2: 0.3997 Loss_G: 5.5893\n",
            "[16/25][133/391] Loss_D1: 0.3468 Loss_D2: 0.5257 Loss_G: 5.3718\n",
            "[16/25][134/391] Loss_D1: 0.5712 Loss_D2: 0.4214 Loss_G: 4.3095\n",
            "[16/25][135/391] Loss_D1: 0.3408 Loss_D2: 0.5014 Loss_G: 5.6933\n",
            "[16/25][136/391] Loss_D1: 0.3704 Loss_D2: 0.5220 Loss_G: 4.6629\n",
            "[16/25][137/391] Loss_D1: 0.4747 Loss_D2: 0.5134 Loss_G: 5.0575\n",
            "[16/25][138/391] Loss_D1: 0.5745 Loss_D2: 0.6202 Loss_G: 5.1282\n",
            "[16/25][139/391] Loss_D1: 0.2978 Loss_D2: 0.6119 Loss_G: 5.8439\n",
            "[16/25][140/391] Loss_D1: 0.4514 Loss_D2: 0.5839 Loss_G: 4.5246\n",
            "[16/25][141/391] Loss_D1: 0.4999 Loss_D2: 0.5947 Loss_G: 4.2793\n",
            "[16/25][142/391] Loss_D1: 0.3409 Loss_D2: 0.4987 Loss_G: 5.5704\n",
            "[16/25][143/391] Loss_D1: 0.4298 Loss_D2: 0.4716 Loss_G: 6.1053\n",
            "[16/25][144/391] Loss_D1: 0.8713 Loss_D2: 0.5181 Loss_G: 3.9491\n",
            "[16/25][145/391] Loss_D1: 0.6712 Loss_D2: 0.5148 Loss_G: 6.0294\n",
            "[16/25][146/391] Loss_D1: 0.3248 Loss_D2: 0.5135 Loss_G: 5.4392\n",
            "[16/25][147/391] Loss_D1: 0.2873 Loss_D2: 0.7156 Loss_G: 4.0874\n",
            "[16/25][148/391] Loss_D1: 0.4771 Loss_D2: 0.6860 Loss_G: 5.6390\n",
            "[16/25][149/391] Loss_D1: 0.3626 Loss_D2: 0.6308 Loss_G: 5.7330\n",
            "[16/25][150/391] Loss_D1: 0.7945 Loss_D2: 0.3830 Loss_G: 3.5408\n",
            "[16/25][151/391] Loss_D1: 0.8779 Loss_D2: 0.4744 Loss_G: 7.1005\n",
            "[16/25][152/391] Loss_D1: 0.7815 Loss_D2: 0.4004 Loss_G: 4.0005\n",
            "[16/25][153/391] Loss_D1: 0.6661 Loss_D2: 0.4122 Loss_G: 5.7142\n",
            "[16/25][154/391] Loss_D1: 0.5922 Loss_D2: 0.4249 Loss_G: 4.8651\n",
            "[16/25][155/391] Loss_D1: 0.7695 Loss_D2: 0.5434 Loss_G: 6.4171\n",
            "[16/25][156/391] Loss_D1: 1.1898 Loss_D2: 0.5458 Loss_G: 2.0565\n",
            "[16/25][157/391] Loss_D1: 1.8098 Loss_D2: 0.6883 Loss_G: 10.8830\n",
            "[16/25][158/391] Loss_D1: 1.6476 Loss_D2: 0.6149 Loss_G: 2.3441\n",
            "[16/25][159/391] Loss_D1: 1.5899 Loss_D2: 0.5053 Loss_G: 7.8098\n",
            "[16/25][160/391] Loss_D1: 0.6355 Loss_D2: 0.6924 Loss_G: 4.5124\n",
            "[16/25][161/391] Loss_D1: 0.5395 Loss_D2: 0.5043 Loss_G: 6.2430\n",
            "[16/25][162/391] Loss_D1: 1.0156 Loss_D2: 0.3988 Loss_G: 3.3280\n",
            "[16/25][163/391] Loss_D1: 1.1579 Loss_D2: 0.5499 Loss_G: 7.5991\n",
            "[16/25][164/391] Loss_D1: 1.0288 Loss_D2: 0.4851 Loss_G: 4.4521\n",
            "[16/25][165/391] Loss_D1: 0.8854 Loss_D2: 0.4980 Loss_G: 4.5416\n",
            "[16/25][166/391] Loss_D1: 0.7588 Loss_D2: 0.4825 Loss_G: 5.6865\n",
            "[16/25][167/391] Loss_D1: 0.8579 Loss_D2: 0.5232 Loss_G: 4.1941\n",
            "[16/25][168/391] Loss_D1: 0.8181 Loss_D2: 0.5765 Loss_G: 4.8772\n",
            "[16/25][169/391] Loss_D1: 0.6317 Loss_D2: 0.6049 Loss_G: 6.0317\n",
            "[16/25][170/391] Loss_D1: 0.4756 Loss_D2: 0.6587 Loss_G: 4.2339\n",
            "[16/25][171/391] Loss_D1: 0.4306 Loss_D2: 0.6128 Loss_G: 4.5526\n",
            "[16/25][172/391] Loss_D1: 0.4363 Loss_D2: 0.5561 Loss_G: 5.5247\n",
            "[16/25][173/391] Loss_D1: 0.5517 Loss_D2: 0.4702 Loss_G: 5.7858\n",
            "[16/25][174/391] Loss_D1: 0.6342 Loss_D2: 0.5489 Loss_G: 4.1498\n",
            "[16/25][175/391] Loss_D1: 0.6111 Loss_D2: 0.5239 Loss_G: 4.1288\n",
            "[16/25][176/391] Loss_D1: 0.3538 Loss_D2: 1.0862 Loss_G: 8.5762\n",
            "[16/25][177/391] Loss_D1: 0.3154 Loss_D2: 1.6463 Loss_G: 3.2750\n",
            "[16/25][178/391] Loss_D1: 0.3702 Loss_D2: 1.4822 Loss_G: 6.6585\n",
            "[16/25][179/391] Loss_D1: 0.5979 Loss_D2: 0.5945 Loss_G: 5.4178\n",
            "[16/25][180/391] Loss_D1: 0.5142 Loss_D2: 0.4909 Loss_G: 4.7955\n",
            "[16/25][181/391] Loss_D1: 0.4978 Loss_D2: 0.6252 Loss_G: 4.8381\n",
            "[16/25][182/391] Loss_D1: 0.4272 Loss_D2: 0.5573 Loss_G: 4.6556\n",
            "[16/25][183/391] Loss_D1: 0.4548 Loss_D2: 0.5441 Loss_G: 4.9151\n",
            "[16/25][184/391] Loss_D1: 0.5248 Loss_D2: 0.6094 Loss_G: 4.8908\n",
            "[16/25][185/391] Loss_D1: 0.3425 Loss_D2: 0.4838 Loss_G: 5.8185\n",
            "[16/25][186/391] Loss_D1: 0.3803 Loss_D2: 0.4042 Loss_G: 4.7613\n",
            "[16/25][187/391] Loss_D1: 0.2749 Loss_D2: 0.4615 Loss_G: 5.8801\n",
            "[16/25][188/391] Loss_D1: 0.2954 Loss_D2: 0.5686 Loss_G: 5.0296\n",
            "[16/25][189/391] Loss_D1: 0.4254 Loss_D2: 0.5775 Loss_G: 4.9965\n",
            "[16/25][190/391] Loss_D1: 0.3752 Loss_D2: 0.4078 Loss_G: 5.5884\n",
            "[16/25][191/391] Loss_D1: 0.3702 Loss_D2: 0.4476 Loss_G: 4.8011\n",
            "[16/25][192/391] Loss_D1: 0.4301 Loss_D2: 0.5267 Loss_G: 5.2672\n",
            "[16/25][193/391] Loss_D1: 0.3743 Loss_D2: 0.5802 Loss_G: 4.6322\n",
            "[16/25][194/391] Loss_D1: 0.4423 Loss_D2: 0.5135 Loss_G: 5.3393\n",
            "[16/25][195/391] Loss_D1: 0.3434 Loss_D2: 0.5922 Loss_G: 4.8958\n",
            "[16/25][196/391] Loss_D1: 0.4193 Loss_D2: 0.5820 Loss_G: 4.2498\n",
            "[16/25][197/391] Loss_D1: 0.4258 Loss_D2: 0.4696 Loss_G: 4.8937\n",
            "[16/25][198/391] Loss_D1: 0.4487 Loss_D2: 0.5060 Loss_G: 6.0240\n",
            "[16/25][199/391] Loss_D1: 0.4269 Loss_D2: 0.5711 Loss_G: 4.9662\n",
            "[16/25][200/391] Loss_D1: 0.5189 Loss_D2: 0.8018 Loss_G: 5.7594\n",
            "saving the output\n",
            "[16/25][201/391] Loss_D1: 0.5546 Loss_D2: 0.4915 Loss_G: 5.5471\n",
            "[16/25][202/391] Loss_D1: 0.4489 Loss_D2: 0.4785 Loss_G: 3.4704\n",
            "[16/25][203/391] Loss_D1: 0.5340 Loss_D2: 0.6230 Loss_G: 6.9444\n",
            "[16/25][204/391] Loss_D1: 0.3310 Loss_D2: 0.6064 Loss_G: 4.9464\n",
            "[16/25][205/391] Loss_D1: 0.3934 Loss_D2: 0.4693 Loss_G: 4.6311\n",
            "[16/25][206/391] Loss_D1: 0.2479 Loss_D2: 0.6221 Loss_G: 5.4815\n",
            "[16/25][207/391] Loss_D1: 0.4627 Loss_D2: 0.6137 Loss_G: 4.4121\n",
            "[16/25][208/391] Loss_D1: 0.4024 Loss_D2: 0.6108 Loss_G: 5.5742\n",
            "[16/25][209/391] Loss_D1: 0.4768 Loss_D2: 0.7273 Loss_G: 3.3486\n",
            "[16/25][210/391] Loss_D1: 0.3818 Loss_D2: 0.8559 Loss_G: 6.5971\n",
            "[16/25][211/391] Loss_D1: 0.4023 Loss_D2: 0.6113 Loss_G: 5.3114\n",
            "[16/25][212/391] Loss_D1: 0.4405 Loss_D2: 0.5464 Loss_G: 5.6489\n",
            "[16/25][213/391] Loss_D1: 0.4082 Loss_D2: 0.4163 Loss_G: 5.4532\n",
            "[16/25][214/391] Loss_D1: 0.3225 Loss_D2: 0.3873 Loss_G: 5.0642\n",
            "[16/25][215/391] Loss_D1: 0.3278 Loss_D2: 0.5058 Loss_G: 5.3023\n",
            "[16/25][216/391] Loss_D1: 0.5727 Loss_D2: 0.5995 Loss_G: 4.8288\n",
            "[16/25][217/391] Loss_D1: 0.5863 Loss_D2: 0.4095 Loss_G: 5.1657\n",
            "[16/25][218/391] Loss_D1: 0.3713 Loss_D2: 0.2836 Loss_G: 6.1152\n",
            "[16/25][219/391] Loss_D1: 0.4272 Loss_D2: 0.6724 Loss_G: 4.1070\n",
            "[16/25][220/391] Loss_D1: 0.4732 Loss_D2: 0.3262 Loss_G: 6.0630\n",
            "[16/25][221/391] Loss_D1: 0.3599 Loss_D2: 0.5790 Loss_G: 4.7629\n",
            "[16/25][222/391] Loss_D1: 0.3412 Loss_D2: 0.4251 Loss_G: 5.3962\n",
            "[16/25][223/391] Loss_D1: 0.3055 Loss_D2: 0.5789 Loss_G: 5.2483\n",
            "[16/25][224/391] Loss_D1: 0.4109 Loss_D2: 0.4210 Loss_G: 4.6934\n",
            "[16/25][225/391] Loss_D1: 0.5179 Loss_D2: 0.4564 Loss_G: 5.7109\n",
            "[16/25][226/391] Loss_D1: 0.1883 Loss_D2: 0.4193 Loss_G: 6.6730\n",
            "[16/25][227/391] Loss_D1: 0.8328 Loss_D2: 0.4819 Loss_G: 3.1638\n",
            "[16/25][228/391] Loss_D1: 0.9702 Loss_D2: 0.5117 Loss_G: 7.3605\n",
            "[16/25][229/391] Loss_D1: 0.5372 Loss_D2: 0.4110 Loss_G: 4.2794\n",
            "[16/25][230/391] Loss_D1: 0.5974 Loss_D2: 0.4671 Loss_G: 5.0886\n",
            "[16/25][231/391] Loss_D1: 0.3399 Loss_D2: 0.5279 Loss_G: 5.7244\n",
            "[16/25][232/391] Loss_D1: 0.3923 Loss_D2: 0.5650 Loss_G: 5.4567\n",
            "[16/25][233/391] Loss_D1: 0.4343 Loss_D2: 0.5204 Loss_G: 5.6626\n",
            "[16/25][234/391] Loss_D1: 0.4164 Loss_D2: 0.4036 Loss_G: 5.7826\n",
            "[16/25][235/391] Loss_D1: 0.3539 Loss_D2: 0.6090 Loss_G: 4.1141\n",
            "[16/25][236/391] Loss_D1: 0.4207 Loss_D2: 0.6055 Loss_G: 7.4734\n",
            "[16/25][237/391] Loss_D1: 0.4424 Loss_D2: 0.5796 Loss_G: 3.9615\n",
            "[16/25][238/391] Loss_D1: 0.3907 Loss_D2: 0.5466 Loss_G: 6.8573\n",
            "[16/25][239/391] Loss_D1: 0.2356 Loss_D2: 0.4933 Loss_G: 6.2574\n",
            "[16/25][240/391] Loss_D1: 0.5223 Loss_D2: 0.5738 Loss_G: 3.1296\n",
            "[16/25][241/391] Loss_D1: 0.7624 Loss_D2: 0.6739 Loss_G: 7.1831\n",
            "[16/25][242/391] Loss_D1: 0.5091 Loss_D2: 0.5052 Loss_G: 4.9786\n",
            "[16/25][243/391] Loss_D1: 0.5272 Loss_D2: 0.4795 Loss_G: 5.5582\n",
            "[16/25][244/391] Loss_D1: 0.3540 Loss_D2: 0.3773 Loss_G: 6.3865\n",
            "[16/25][245/391] Loss_D1: 0.3766 Loss_D2: 0.6085 Loss_G: 4.0698\n",
            "[16/25][246/391] Loss_D1: 0.5718 Loss_D2: 0.6969 Loss_G: 4.3350\n",
            "[16/25][247/391] Loss_D1: 0.4464 Loss_D2: 0.4942 Loss_G: 6.9604\n",
            "[16/25][248/391] Loss_D1: 0.4733 Loss_D2: 0.5221 Loss_G: 4.2410\n",
            "[16/25][249/391] Loss_D1: 0.7537 Loss_D2: 0.5220 Loss_G: 7.0484\n",
            "[16/25][250/391] Loss_D1: 1.0788 Loss_D2: 0.5411 Loss_G: 2.4865\n",
            "[16/25][251/391] Loss_D1: 2.4233 Loss_D2: 0.4291 Loss_G: 9.0804\n",
            "[16/25][252/391] Loss_D1: 1.4983 Loss_D2: 0.5434 Loss_G: 2.9440\n",
            "[16/25][253/391] Loss_D1: 1.6221 Loss_D2: 0.4479 Loss_G: 9.3277\n",
            "[16/25][254/391] Loss_D1: 1.0618 Loss_D2: 0.4883 Loss_G: 4.0656\n",
            "[16/25][255/391] Loss_D1: 1.2128 Loss_D2: 0.5664 Loss_G: 6.7675\n",
            "[16/25][256/391] Loss_D1: 1.9242 Loss_D2: 1.0144 Loss_G: 4.8747\n",
            "[16/25][257/391] Loss_D1: 1.4573 Loss_D2: 0.6493 Loss_G: 5.1283\n",
            "[16/25][258/391] Loss_D1: 0.8657 Loss_D2: 1.0614 Loss_G: 7.4228\n",
            "[16/25][259/391] Loss_D1: 0.6784 Loss_D2: 1.5960 Loss_G: 4.0773\n",
            "[16/25][260/391] Loss_D1: 0.8254 Loss_D2: 3.5220 Loss_G: 7.7070\n",
            "[16/25][261/391] Loss_D1: 0.6109 Loss_D2: 2.1287 Loss_G: 3.4176\n",
            "[16/25][262/391] Loss_D1: 0.5204 Loss_D2: 2.5878 Loss_G: 10.7495\n",
            "[16/25][263/391] Loss_D1: 0.4777 Loss_D2: 3.1216 Loss_G: 3.5577\n",
            "[16/25][264/391] Loss_D1: 0.4070 Loss_D2: 2.0846 Loss_G: 7.8225\n",
            "[16/25][265/391] Loss_D1: 1.2141 Loss_D2: 0.9794 Loss_G: 2.9655\n",
            "[16/25][266/391] Loss_D1: 1.8678 Loss_D2: 0.7223 Loss_G: 7.3157\n",
            "[16/25][267/391] Loss_D1: 0.7990 Loss_D2: 1.0602 Loss_G: 5.1119\n",
            "[16/25][268/391] Loss_D1: 0.3820 Loss_D2: 1.0288 Loss_G: 5.1989\n",
            "[16/25][269/391] Loss_D1: 0.6252 Loss_D2: 1.1394 Loss_G: 5.3024\n",
            "[16/25][270/391] Loss_D1: 1.1717 Loss_D2: 1.3281 Loss_G: 5.4054\n",
            "[16/25][271/391] Loss_D1: 0.9890 Loss_D2: 1.5625 Loss_G: 4.7250\n",
            "[16/25][272/391] Loss_D1: 0.5350 Loss_D2: 1.0673 Loss_G: 5.9645\n",
            "[16/25][273/391] Loss_D1: 0.5439 Loss_D2: 0.4593 Loss_G: 4.8178\n",
            "[16/25][274/391] Loss_D1: 0.6009 Loss_D2: 0.4974 Loss_G: 7.8741\n",
            "[16/25][275/391] Loss_D1: 0.9102 Loss_D2: 0.6705 Loss_G: 3.4647\n",
            "[16/25][276/391] Loss_D1: 0.6917 Loss_D2: 0.7651 Loss_G: 6.8012\n",
            "[16/25][277/391] Loss_D1: 0.4836 Loss_D2: 0.6327 Loss_G: 4.2797\n",
            "[16/25][278/391] Loss_D1: 0.5623 Loss_D2: 0.5507 Loss_G: 5.9422\n",
            "[16/25][279/391] Loss_D1: 0.4820 Loss_D2: 0.6911 Loss_G: 4.9168\n",
            "[16/25][280/391] Loss_D1: 0.6797 Loss_D2: 0.5180 Loss_G: 3.2171\n",
            "[16/25][281/391] Loss_D1: 0.6289 Loss_D2: 0.6251 Loss_G: 7.0781\n",
            "[16/25][282/391] Loss_D1: 0.4466 Loss_D2: 0.7333 Loss_G: 5.0085\n",
            "[16/25][283/391] Loss_D1: 0.6292 Loss_D2: 0.7219 Loss_G: 3.8700\n",
            "[16/25][284/391] Loss_D1: 0.7221 Loss_D2: 0.5479 Loss_G: 6.5377\n",
            "[16/25][285/391] Loss_D1: 0.3636 Loss_D2: 0.7737 Loss_G: 5.4085\n",
            "[16/25][286/391] Loss_D1: 0.4526 Loss_D2: 0.6671 Loss_G: 5.3614\n",
            "[16/25][287/391] Loss_D1: 0.6147 Loss_D2: 0.3576 Loss_G: 6.3941\n",
            "[16/25][288/391] Loss_D1: 0.3236 Loss_D2: 0.4457 Loss_G: 5.7473\n",
            "[16/25][289/391] Loss_D1: 0.6441 Loss_D2: 0.6010 Loss_G: 3.5290\n",
            "[16/25][290/391] Loss_D1: 1.0179 Loss_D2: 0.4968 Loss_G: 7.5355\n",
            "[16/25][291/391] Loss_D1: 0.6218 Loss_D2: 0.4668 Loss_G: 4.2585\n",
            "[16/25][292/391] Loss_D1: 0.5072 Loss_D2: 0.4893 Loss_G: 6.2076\n",
            "[16/25][293/391] Loss_D1: 0.3911 Loss_D2: 0.5988 Loss_G: 5.8767\n",
            "[16/25][294/391] Loss_D1: 0.5475 Loss_D2: 0.6799 Loss_G: 3.3059\n",
            "[16/25][295/391] Loss_D1: 0.8720 Loss_D2: 0.4923 Loss_G: 7.2482\n",
            "[16/25][296/391] Loss_D1: 0.4670 Loss_D2: 0.5305 Loss_G: 5.0231\n",
            "[16/25][297/391] Loss_D1: 0.3032 Loss_D2: 0.5066 Loss_G: 4.2446\n",
            "[16/25][298/391] Loss_D1: 0.5587 Loss_D2: 0.6214 Loss_G: 6.2145\n",
            "[16/25][299/391] Loss_D1: 0.7909 Loss_D2: 0.6891 Loss_G: 3.2033\n",
            "[16/25][300/391] Loss_D1: 0.6493 Loss_D2: 0.6108 Loss_G: 5.1663\n",
            "saving the output\n",
            "[16/25][301/391] Loss_D1: 0.3534 Loss_D2: 0.4796 Loss_G: 6.2150\n",
            "[16/25][302/391] Loss_D1: 0.5429 Loss_D2: 0.5395 Loss_G: 5.0585\n",
            "[16/25][303/391] Loss_D1: 0.4235 Loss_D2: 0.7709 Loss_G: 3.7782\n",
            "[16/25][304/391] Loss_D1: 0.3836 Loss_D2: 0.5229 Loss_G: 5.9550\n",
            "[16/25][305/391] Loss_D1: 0.3709 Loss_D2: 0.4342 Loss_G: 6.1125\n",
            "[16/25][306/391] Loss_D1: 0.5559 Loss_D2: 0.6577 Loss_G: 3.0589\n",
            "[16/25][307/391] Loss_D1: 0.6914 Loss_D2: 0.5200 Loss_G: 6.3906\n",
            "[16/25][308/391] Loss_D1: 0.3081 Loss_D2: 0.5916 Loss_G: 6.5565\n",
            "[16/25][309/391] Loss_D1: 0.4261 Loss_D2: 0.5077 Loss_G: 3.9387\n",
            "[16/25][310/391] Loss_D1: 0.6817 Loss_D2: 0.4245 Loss_G: 5.8245\n",
            "[16/25][311/391] Loss_D1: 0.4846 Loss_D2: 0.7581 Loss_G: 6.0876\n",
            "[16/25][312/391] Loss_D1: 0.2705 Loss_D2: 0.4270 Loss_G: 5.2742\n",
            "[16/25][313/391] Loss_D1: 0.3530 Loss_D2: 0.5451 Loss_G: 4.5377\n",
            "[16/25][314/391] Loss_D1: 0.3567 Loss_D2: 0.6930 Loss_G: 6.8807\n",
            "[16/25][315/391] Loss_D1: 0.4883 Loss_D2: 0.9280 Loss_G: 3.0541\n",
            "[16/25][316/391] Loss_D1: 0.6023 Loss_D2: 0.7300 Loss_G: 6.7731\n",
            "[16/25][317/391] Loss_D1: 0.6531 Loss_D2: 0.5088 Loss_G: 4.1872\n",
            "[16/25][318/391] Loss_D1: 0.4156 Loss_D2: 0.3172 Loss_G: 5.1654\n",
            "[16/25][319/391] Loss_D1: 0.3431 Loss_D2: 0.4709 Loss_G: 6.6182\n",
            "[16/25][320/391] Loss_D1: 0.6050 Loss_D2: 0.3922 Loss_G: 4.1202\n",
            "[16/25][321/391] Loss_D1: 0.4800 Loss_D2: 0.5248 Loss_G: 4.7512\n",
            "[16/25][322/391] Loss_D1: 0.4207 Loss_D2: 0.6886 Loss_G: 6.3318\n",
            "[16/25][323/391] Loss_D1: 0.5698 Loss_D2: 0.6666 Loss_G: 2.9365\n",
            "[16/25][324/391] Loss_D1: 0.7409 Loss_D2: 0.6467 Loss_G: 6.1245\n",
            "[16/25][325/391] Loss_D1: 0.4985 Loss_D2: 0.4862 Loss_G: 5.1894\n",
            "[16/25][326/391] Loss_D1: 0.4307 Loss_D2: 0.5959 Loss_G: 5.7359\n",
            "[16/25][327/391] Loss_D1: 0.5968 Loss_D2: 0.5692 Loss_G: 3.9566\n",
            "[16/25][328/391] Loss_D1: 0.5561 Loss_D2: 0.4787 Loss_G: 5.2750\n",
            "[16/25][329/391] Loss_D1: 0.4944 Loss_D2: 0.7109 Loss_G: 5.0356\n",
            "[16/25][330/391] Loss_D1: 0.5363 Loss_D2: 0.3095 Loss_G: 5.7674\n",
            "[16/25][331/391] Loss_D1: 0.6468 Loss_D2: 0.5246 Loss_G: 4.1664\n",
            "[16/25][332/391] Loss_D1: 0.4033 Loss_D2: 0.5131 Loss_G: 5.6686\n",
            "[16/25][333/391] Loss_D1: 0.4035 Loss_D2: 0.4175 Loss_G: 6.0231\n",
            "[16/25][334/391] Loss_D1: 0.8456 Loss_D2: 0.4867 Loss_G: 3.6328\n",
            "[16/25][335/391] Loss_D1: 0.8187 Loss_D2: 0.5520 Loss_G: 6.0370\n",
            "[16/25][336/391] Loss_D1: 0.2270 Loss_D2: 0.4738 Loss_G: 5.8231\n",
            "[16/25][337/391] Loss_D1: 0.5442 Loss_D2: 0.4353 Loss_G: 3.6875\n",
            "[16/25][338/391] Loss_D1: 0.7342 Loss_D2: 0.6153 Loss_G: 7.3206\n",
            "[16/25][339/391] Loss_D1: 0.8400 Loss_D2: 0.4760 Loss_G: 4.4908\n",
            "[16/25][340/391] Loss_D1: 0.5006 Loss_D2: 0.5224 Loss_G: 5.4869\n",
            "[16/25][341/391] Loss_D1: 0.4635 Loss_D2: 0.4579 Loss_G: 4.7850\n",
            "[16/25][342/391] Loss_D1: 0.3785 Loss_D2: 0.5729 Loss_G: 5.0433\n",
            "[16/25][343/391] Loss_D1: 0.4410 Loss_D2: 0.4801 Loss_G: 5.7730\n",
            "[16/25][344/391] Loss_D1: 0.3486 Loss_D2: 0.4243 Loss_G: 5.0295\n",
            "[16/25][345/391] Loss_D1: 0.4256 Loss_D2: 0.4091 Loss_G: 3.9845\n",
            "[16/25][346/391] Loss_D1: 0.4743 Loss_D2: 0.5346 Loss_G: 7.1297\n",
            "[16/25][347/391] Loss_D1: 0.2224 Loss_D2: 0.3938 Loss_G: 6.1866\n",
            "[16/25][348/391] Loss_D1: 0.5819 Loss_D2: 0.5244 Loss_G: 3.1223\n",
            "[16/25][349/391] Loss_D1: 0.9753 Loss_D2: 0.6155 Loss_G: 6.9232\n",
            "[16/25][350/391] Loss_D1: 0.5470 Loss_D2: 0.5710 Loss_G: 4.4367\n",
            "[16/25][351/391] Loss_D1: 0.4794 Loss_D2: 0.4936 Loss_G: 4.4194\n",
            "[16/25][352/391] Loss_D1: 0.2704 Loss_D2: 0.6561 Loss_G: 7.0544\n",
            "[16/25][353/391] Loss_D1: 0.4429 Loss_D2: 0.5205 Loss_G: 4.1231\n",
            "[16/25][354/391] Loss_D1: 0.5886 Loss_D2: 0.5800 Loss_G: 4.3154\n",
            "[16/25][355/391] Loss_D1: 0.2348 Loss_D2: 0.6707 Loss_G: 7.6360\n",
            "[16/25][356/391] Loss_D1: 0.5279 Loss_D2: 0.4276 Loss_G: 3.9772\n",
            "[16/25][357/391] Loss_D1: 0.5757 Loss_D2: 0.4435 Loss_G: 4.9726\n",
            "[16/25][358/391] Loss_D1: 0.3445 Loss_D2: 0.6609 Loss_G: 7.0634\n",
            "[16/25][359/391] Loss_D1: 0.6139 Loss_D2: 0.7585 Loss_G: 2.6026\n",
            "[16/25][360/391] Loss_D1: 0.5555 Loss_D2: 0.8756 Loss_G: 8.0183\n",
            "[16/25][361/391] Loss_D1: 0.3169 Loss_D2: 0.6591 Loss_G: 5.1754\n",
            "[16/25][362/391] Loss_D1: 0.6549 Loss_D2: 0.4288 Loss_G: 2.9349\n",
            "[16/25][363/391] Loss_D1: 0.9333 Loss_D2: 0.5446 Loss_G: 6.9740\n",
            "[16/25][364/391] Loss_D1: 0.5307 Loss_D2: 0.3713 Loss_G: 4.1285\n",
            "[16/25][365/391] Loss_D1: 0.8456 Loss_D2: 0.4560 Loss_G: 7.9040\n",
            "[16/25][366/391] Loss_D1: 1.3286 Loss_D2: 0.3434 Loss_G: 3.4984\n",
            "[16/25][367/391] Loss_D1: 0.7991 Loss_D2: 0.4488 Loss_G: 7.1025\n",
            "[16/25][368/391] Loss_D1: 0.2678 Loss_D2: 0.3476 Loss_G: 6.8286\n",
            "[16/25][369/391] Loss_D1: 0.7528 Loss_D2: 0.6013 Loss_G: 1.9761\n",
            "[16/25][370/391] Loss_D1: 1.4699 Loss_D2: 0.6755 Loss_G: 8.2947\n",
            "[16/25][371/391] Loss_D1: 0.9277 Loss_D2: 0.6163 Loss_G: 3.0918\n",
            "[16/25][372/391] Loss_D1: 0.7850 Loss_D2: 0.5615 Loss_G: 5.9883\n",
            "[16/25][373/391] Loss_D1: 0.7161 Loss_D2: 0.4830 Loss_G: 4.5852\n",
            "[16/25][374/391] Loss_D1: 0.5025 Loss_D2: 0.3594 Loss_G: 4.9487\n",
            "[16/25][375/391] Loss_D1: 1.0080 Loss_D2: 0.5134 Loss_G: 6.3930\n",
            "[16/25][376/391] Loss_D1: 0.9524 Loss_D2: 0.5221 Loss_G: 4.5006\n",
            "[16/25][377/391] Loss_D1: 0.7647 Loss_D2: 0.5041 Loss_G: 6.1965\n",
            "[16/25][378/391] Loss_D1: 0.5449 Loss_D2: 0.4080 Loss_G: 4.5795\n",
            "[16/25][379/391] Loss_D1: 0.4949 Loss_D2: 0.5613 Loss_G: 3.9842\n",
            "[16/25][380/391] Loss_D1: 0.6868 Loss_D2: 0.6172 Loss_G: 7.0747\n",
            "[16/25][381/391] Loss_D1: 1.1611 Loss_D2: 0.4321 Loss_G: 3.4977\n",
            "[16/25][382/391] Loss_D1: 0.6684 Loss_D2: 0.4742 Loss_G: 6.3739\n",
            "[16/25][383/391] Loss_D1: 0.3944 Loss_D2: 0.5352 Loss_G: 5.3850\n",
            "[16/25][384/391] Loss_D1: 0.3193 Loss_D2: 0.4645 Loss_G: 5.9249\n",
            "[16/25][385/391] Loss_D1: 0.6276 Loss_D2: 0.6060 Loss_G: 3.9885\n",
            "[16/25][386/391] Loss_D1: 0.6153 Loss_D2: 0.5846 Loss_G: 4.9098\n",
            "[16/25][387/391] Loss_D1: 0.3612 Loss_D2: 0.4949 Loss_G: 6.2734\n",
            "[16/25][388/391] Loss_D1: 0.6259 Loss_D2: 0.4586 Loss_G: 4.6912\n",
            "[16/25][389/391] Loss_D1: 0.5324 Loss_D2: 0.4225 Loss_G: 5.7453\n",
            "[16/25][390/391] Loss_D1: 0.5007 Loss_D2: 0.8343 Loss_G: 3.8785\n",
            "[17/25][0/391] Loss_D1: 0.5230 Loss_D2: 0.7888 Loss_G: 7.2167\n",
            "saving the output\n",
            "[17/25][1/391] Loss_D1: 0.3395 Loss_D2: 0.7492 Loss_G: 4.2633\n",
            "[17/25][2/391] Loss_D1: 0.3762 Loss_D2: 0.6544 Loss_G: 6.8699\n",
            "[17/25][3/391] Loss_D1: 0.5885 Loss_D2: 0.3980 Loss_G: 4.1982\n",
            "[17/25][4/391] Loss_D1: 0.3800 Loss_D2: 0.4519 Loss_G: 5.1032\n",
            "[17/25][5/391] Loss_D1: 0.4373 Loss_D2: 0.3768 Loss_G: 5.3724\n",
            "[17/25][6/391] Loss_D1: 0.2980 Loss_D2: 0.5399 Loss_G: 5.6965\n",
            "[17/25][7/391] Loss_D1: 0.3404 Loss_D2: 0.5782 Loss_G: 5.0808\n",
            "[17/25][8/391] Loss_D1: 0.4684 Loss_D2: 0.5082 Loss_G: 4.7444\n",
            "[17/25][9/391] Loss_D1: 0.3685 Loss_D2: 0.5012 Loss_G: 5.2603\n",
            "[17/25][10/391] Loss_D1: 0.3332 Loss_D2: 0.4908 Loss_G: 5.1081\n",
            "[17/25][11/391] Loss_D1: 0.4410 Loss_D2: 0.6052 Loss_G: 6.6753\n",
            "[17/25][12/391] Loss_D1: 0.2690 Loss_D2: 0.9264 Loss_G: 4.0871\n",
            "[17/25][13/391] Loss_D1: 0.3847 Loss_D2: 0.5815 Loss_G: 5.4108\n",
            "[17/25][14/391] Loss_D1: 0.2967 Loss_D2: 0.3785 Loss_G: 6.0513\n",
            "[17/25][15/391] Loss_D1: 0.3475 Loss_D2: 0.4538 Loss_G: 4.8722\n",
            "[17/25][16/391] Loss_D1: 0.4470 Loss_D2: 0.5210 Loss_G: 4.6023\n",
            "[17/25][17/391] Loss_D1: 0.3681 Loss_D2: 0.4378 Loss_G: 5.2306\n",
            "[17/25][18/391] Loss_D1: 0.4483 Loss_D2: 0.4672 Loss_G: 6.5109\n",
            "[17/25][19/391] Loss_D1: 0.3321 Loss_D2: 0.5039 Loss_G: 4.5187\n",
            "[17/25][20/391] Loss_D1: 0.2887 Loss_D2: 0.5635 Loss_G: 5.5230\n",
            "[17/25][21/391] Loss_D1: 0.5932 Loss_D2: 0.6846 Loss_G: 5.0567\n",
            "[17/25][22/391] Loss_D1: 0.3714 Loss_D2: 0.6152 Loss_G: 4.3691\n",
            "[17/25][23/391] Loss_D1: 0.4419 Loss_D2: 0.5489 Loss_G: 4.7014\n",
            "[17/25][24/391] Loss_D1: 0.4183 Loss_D2: 0.6743 Loss_G: 6.6386\n",
            "[17/25][25/391] Loss_D1: 0.5967 Loss_D2: 0.8425 Loss_G: 2.6490\n",
            "[17/25][26/391] Loss_D1: 0.5422 Loss_D2: 0.9481 Loss_G: 9.2125\n",
            "[17/25][27/391] Loss_D1: 0.6982 Loss_D2: 0.6293 Loss_G: 4.2693\n",
            "[17/25][28/391] Loss_D1: 0.7198 Loss_D2: 0.4144 Loss_G: 5.1611\n",
            "[17/25][29/391] Loss_D1: 0.4358 Loss_D2: 0.4802 Loss_G: 4.8904\n",
            "[17/25][30/391] Loss_D1: 0.4153 Loss_D2: 0.5311 Loss_G: 6.0524\n",
            "[17/25][31/391] Loss_D1: 0.3585 Loss_D2: 0.5925 Loss_G: 4.4821\n",
            "[17/25][32/391] Loss_D1: 0.3170 Loss_D2: 0.6316 Loss_G: 5.7337\n",
            "[17/25][33/391] Loss_D1: 0.4416 Loss_D2: 0.5271 Loss_G: 5.1854\n",
            "[17/25][34/391] Loss_D1: 0.4851 Loss_D2: 0.4513 Loss_G: 4.6477\n",
            "[17/25][35/391] Loss_D1: 0.4201 Loss_D2: 0.4995 Loss_G: 5.4545\n",
            "[17/25][36/391] Loss_D1: 0.5133 Loss_D2: 0.3386 Loss_G: 6.2824\n",
            "[17/25][37/391] Loss_D1: 0.2349 Loss_D2: 0.3532 Loss_G: 6.9545\n",
            "[17/25][38/391] Loss_D1: 0.6851 Loss_D2: 0.5162 Loss_G: 2.7197\n",
            "[17/25][39/391] Loss_D1: 0.8749 Loss_D2: 0.6119 Loss_G: 7.9377\n",
            "[17/25][40/391] Loss_D1: 0.5562 Loss_D2: 0.8172 Loss_G: 3.5722\n",
            "[17/25][41/391] Loss_D1: 0.3134 Loss_D2: 0.5547 Loss_G: 6.0904\n",
            "[17/25][42/391] Loss_D1: 0.3556 Loss_D2: 0.4777 Loss_G: 4.9177\n",
            "[17/25][43/391] Loss_D1: 0.4164 Loss_D2: 0.5077 Loss_G: 4.8064\n",
            "[17/25][44/391] Loss_D1: 0.5512 Loss_D2: 0.4937 Loss_G: 7.2756\n",
            "[17/25][45/391] Loss_D1: 0.8042 Loss_D2: 0.7527 Loss_G: 2.5834\n",
            "[17/25][46/391] Loss_D1: 0.5568 Loss_D2: 0.9407 Loss_G: 7.4265\n",
            "[17/25][47/391] Loss_D1: 0.3957 Loss_D2: 0.7730 Loss_G: 3.5773\n",
            "[17/25][48/391] Loss_D1: 0.1996 Loss_D2: 1.2184 Loss_G: 8.5287\n",
            "[17/25][49/391] Loss_D1: 0.5902 Loss_D2: 1.6638 Loss_G: 2.1205\n",
            "[17/25][50/391] Loss_D1: 0.8063 Loss_D2: 0.9279 Loss_G: 8.5233\n",
            "[17/25][51/391] Loss_D1: 0.6248 Loss_D2: 0.8538 Loss_G: 3.2964\n",
            "[17/25][52/391] Loss_D1: 0.6121 Loss_D2: 0.7464 Loss_G: 6.7996\n",
            "[17/25][53/391] Loss_D1: 0.6052 Loss_D2: 0.4884 Loss_G: 6.1132\n",
            "[17/25][54/391] Loss_D1: 0.3225 Loss_D2: 0.6827 Loss_G: 3.7816\n",
            "[17/25][55/391] Loss_D1: 0.6147 Loss_D2: 0.7508 Loss_G: 7.2218\n",
            "[17/25][56/391] Loss_D1: 0.6121 Loss_D2: 0.5946 Loss_G: 4.1073\n",
            "[17/25][57/391] Loss_D1: 0.4747 Loss_D2: 0.4472 Loss_G: 7.3124\n",
            "[17/25][58/391] Loss_D1: 0.8695 Loss_D2: 0.5160 Loss_G: 3.6698\n",
            "[17/25][59/391] Loss_D1: 0.8774 Loss_D2: 0.6169 Loss_G: 8.2682\n",
            "[17/25][60/391] Loss_D1: 1.0552 Loss_D2: 0.4070 Loss_G: 2.9407\n",
            "[17/25][61/391] Loss_D1: 0.9813 Loss_D2: 0.4796 Loss_G: 6.9395\n",
            "[17/25][62/391] Loss_D1: 0.5161 Loss_D2: 0.6881 Loss_G: 4.5503\n",
            "[17/25][63/391] Loss_D1: 0.4953 Loss_D2: 0.3975 Loss_G: 6.9611\n",
            "[17/25][64/391] Loss_D1: 0.6966 Loss_D2: 0.5550 Loss_G: 3.5803\n",
            "[17/25][65/391] Loss_D1: 1.1323 Loss_D2: 0.7167 Loss_G: 6.6407\n",
            "[17/25][66/391] Loss_D1: 0.9009 Loss_D2: 0.7769 Loss_G: 4.1772\n",
            "[17/25][67/391] Loss_D1: 1.4779 Loss_D2: 0.4410 Loss_G: 9.2840\n",
            "[17/25][68/391] Loss_D1: 1.5311 Loss_D2: 0.5778 Loss_G: 3.2602\n",
            "[17/25][69/391] Loss_D1: 0.9609 Loss_D2: 0.6884 Loss_G: 5.2687\n",
            "[17/25][70/391] Loss_D1: 0.6411 Loss_D2: 0.5662 Loss_G: 5.2628\n",
            "[17/25][71/391] Loss_D1: 0.5387 Loss_D2: 0.5144 Loss_G: 4.8043\n",
            "[17/25][72/391] Loss_D1: 0.3999 Loss_D2: 0.3203 Loss_G: 5.6694\n",
            "[17/25][73/391] Loss_D1: 0.6399 Loss_D2: 0.4463 Loss_G: 4.4029\n",
            "[17/25][74/391] Loss_D1: 0.6165 Loss_D2: 0.5195 Loss_G: 6.6631\n",
            "[17/25][75/391] Loss_D1: 0.4623 Loss_D2: 0.7545 Loss_G: 3.7794\n",
            "[17/25][76/391] Loss_D1: 0.4549 Loss_D2: 0.7367 Loss_G: 5.1528\n",
            "[17/25][77/391] Loss_D1: 0.9062 Loss_D2: 0.5116 Loss_G: 7.0170\n",
            "[17/25][78/391] Loss_D1: 0.8515 Loss_D2: 0.5048 Loss_G: 3.6780\n",
            "[17/25][79/391] Loss_D1: 0.5698 Loss_D2: 0.5804 Loss_G: 6.0377\n",
            "[17/25][80/391] Loss_D1: 0.6421 Loss_D2: 0.4142 Loss_G: 6.4297\n",
            "[17/25][81/391] Loss_D1: 0.9656 Loss_D2: 0.4894 Loss_G: 3.3689\n",
            "[17/25][82/391] Loss_D1: 0.6624 Loss_D2: 0.5883 Loss_G: 7.0830\n",
            "[17/25][83/391] Loss_D1: 1.0228 Loss_D2: 0.8890 Loss_G: 4.9907\n",
            "[17/25][84/391] Loss_D1: 0.4945 Loss_D2: 1.0601 Loss_G: 7.0695\n",
            "[17/25][85/391] Loss_D1: 0.7070 Loss_D2: 0.5181 Loss_G: 2.8704\n",
            "[17/25][86/391] Loss_D1: 1.3839 Loss_D2: 0.4492 Loss_G: 8.2534\n",
            "[17/25][87/391] Loss_D1: 0.3595 Loss_D2: 0.3260 Loss_G: 7.3070\n",
            "[17/25][88/391] Loss_D1: 0.4808 Loss_D2: 0.5098 Loss_G: 3.0770\n",
            "[17/25][89/391] Loss_D1: 0.6478 Loss_D2: 0.6311 Loss_G: 7.2302\n",
            "[17/25][90/391] Loss_D1: 0.2465 Loss_D2: 0.6733 Loss_G: 5.0533\n",
            "[17/25][91/391] Loss_D1: 0.7914 Loss_D2: 0.7121 Loss_G: 4.1731\n",
            "[17/25][92/391] Loss_D1: 0.5013 Loss_D2: 0.6336 Loss_G: 4.8429\n",
            "[17/25][93/391] Loss_D1: 0.5484 Loss_D2: 0.5885 Loss_G: 6.5683\n",
            "[17/25][94/391] Loss_D1: 1.4730 Loss_D2: 0.5737 Loss_G: 4.7109\n",
            "[17/25][95/391] Loss_D1: 0.8157 Loss_D2: 0.7251 Loss_G: 5.4731\n",
            "[17/25][96/391] Loss_D1: 0.5278 Loss_D2: 0.4439 Loss_G: 4.4049\n",
            "[17/25][97/391] Loss_D1: 0.5780 Loss_D2: 0.5445 Loss_G: 5.8181\n",
            "[17/25][98/391] Loss_D1: 0.4239 Loss_D2: 0.5466 Loss_G: 5.5175\n",
            "[17/25][99/391] Loss_D1: 0.3142 Loss_D2: 0.3828 Loss_G: 5.1371\n",
            "[17/25][100/391] Loss_D1: 0.2783 Loss_D2: 0.5004 Loss_G: 6.1169\n",
            "saving the output\n",
            "[17/25][101/391] Loss_D1: 0.5288 Loss_D2: 0.3364 Loss_G: 5.1279\n",
            "[17/25][102/391] Loss_D1: 0.4622 Loss_D2: 0.3500 Loss_G: 4.9666\n",
            "[17/25][103/391] Loss_D1: 0.4476 Loss_D2: 0.5376 Loss_G: 5.1692\n",
            "[17/25][104/391] Loss_D1: 0.5514 Loss_D2: 0.4231 Loss_G: 5.5892\n",
            "[17/25][105/391] Loss_D1: 0.6130 Loss_D2: 0.4438 Loss_G: 3.7384\n",
            "[17/25][106/391] Loss_D1: 0.7204 Loss_D2: 0.3339 Loss_G: 7.3146\n",
            "[17/25][107/391] Loss_D1: 1.1219 Loss_D2: 0.4061 Loss_G: 4.2034\n",
            "[17/25][108/391] Loss_D1: 0.6609 Loss_D2: 0.4805 Loss_G: 5.0787\n",
            "[17/25][109/391] Loss_D1: 0.3391 Loss_D2: 0.7198 Loss_G: 8.9599\n",
            "[17/25][110/391] Loss_D1: 0.7584 Loss_D2: 0.5644 Loss_G: 3.6079\n",
            "[17/25][111/391] Loss_D1: 0.5090 Loss_D2: 0.4095 Loss_G: 5.5380\n",
            "[17/25][112/391] Loss_D1: 0.5030 Loss_D2: 0.4092 Loss_G: 6.6303\n",
            "[17/25][113/391] Loss_D1: 0.4655 Loss_D2: 0.3742 Loss_G: 4.5959\n",
            "[17/25][114/391] Loss_D1: 0.4401 Loss_D2: 0.3917 Loss_G: 4.6437\n",
            "[17/25][115/391] Loss_D1: 0.3825 Loss_D2: 0.3903 Loss_G: 6.0124\n",
            "[17/25][116/391] Loss_D1: 0.4093 Loss_D2: 0.4211 Loss_G: 5.5244\n",
            "[17/25][117/391] Loss_D1: 0.3108 Loss_D2: 0.4811 Loss_G: 4.6690\n",
            "[17/25][118/391] Loss_D1: 0.3999 Loss_D2: 0.5995 Loss_G: 5.8337\n",
            "[17/25][119/391] Loss_D1: 0.4618 Loss_D2: 0.5498 Loss_G: 4.2197\n",
            "[17/25][120/391] Loss_D1: 0.5202 Loss_D2: 0.5646 Loss_G: 4.7121\n",
            "[17/25][121/391] Loss_D1: 0.5593 Loss_D2: 0.5904 Loss_G: 6.3275\n",
            "[17/25][122/391] Loss_D1: 0.3582 Loss_D2: 0.3722 Loss_G: 5.2590\n",
            "[17/25][123/391] Loss_D1: 0.2379 Loss_D2: 0.4046 Loss_G: 5.2188\n",
            "[17/25][124/391] Loss_D1: 0.4042 Loss_D2: 0.4653 Loss_G: 5.0408\n",
            "[17/25][125/391] Loss_D1: 0.4734 Loss_D2: 0.4722 Loss_G: 5.4628\n",
            "[17/25][126/391] Loss_D1: 0.3689 Loss_D2: 0.4208 Loss_G: 5.7524\n",
            "[17/25][127/391] Loss_D1: 0.5047 Loss_D2: 0.4806 Loss_G: 3.3331\n",
            "[17/25][128/391] Loss_D1: 0.7041 Loss_D2: 0.4614 Loss_G: 7.3392\n",
            "[17/25][129/391] Loss_D1: 0.8038 Loss_D2: 0.4181 Loss_G: 3.2848\n",
            "[17/25][130/391] Loss_D1: 1.4531 Loss_D2: 0.4927 Loss_G: 7.5710\n",
            "[17/25][131/391] Loss_D1: 0.9272 Loss_D2: 0.6215 Loss_G: 5.4489\n",
            "[17/25][132/391] Loss_D1: 0.4315 Loss_D2: 0.6934 Loss_G: 3.2729\n",
            "[17/25][133/391] Loss_D1: 0.5506 Loss_D2: 0.7766 Loss_G: 7.2191\n",
            "[17/25][134/391] Loss_D1: 0.7143 Loss_D2: 0.4451 Loss_G: 4.4443\n",
            "[17/25][135/391] Loss_D1: 0.5583 Loss_D2: 0.4479 Loss_G: 5.5315\n",
            "[17/25][136/391] Loss_D1: 0.7885 Loss_D2: 0.4179 Loss_G: 4.1569\n",
            "[17/25][137/391] Loss_D1: 0.6946 Loss_D2: 0.4599 Loss_G: 7.5595\n",
            "[17/25][138/391] Loss_D1: 0.6356 Loss_D2: 0.3184 Loss_G: 4.8382\n",
            "[17/25][139/391] Loss_D1: 0.3505 Loss_D2: 0.3816 Loss_G: 4.7965\n",
            "[17/25][140/391] Loss_D1: 0.4870 Loss_D2: 0.4165 Loss_G: 5.5608\n",
            "[17/25][141/391] Loss_D1: 0.3674 Loss_D2: 0.4740 Loss_G: 5.5397\n",
            "[17/25][142/391] Loss_D1: 0.3341 Loss_D2: 0.4675 Loss_G: 5.3037\n",
            "[17/25][143/391] Loss_D1: 0.3597 Loss_D2: 0.3602 Loss_G: 5.4361\n",
            "[17/25][144/391] Loss_D1: 0.4935 Loss_D2: 0.3248 Loss_G: 4.7120\n",
            "[17/25][145/391] Loss_D1: 0.4712 Loss_D2: 0.3879 Loss_G: 6.4684\n",
            "[17/25][146/391] Loss_D1: 0.3807 Loss_D2: 0.5261 Loss_G: 5.4645\n",
            "[17/25][147/391] Loss_D1: 0.7446 Loss_D2: 0.4411 Loss_G: 4.4499\n",
            "[17/25][148/391] Loss_D1: 0.5505 Loss_D2: 0.4658 Loss_G: 6.9830\n",
            "[17/25][149/391] Loss_D1: 0.3997 Loss_D2: 0.5441 Loss_G: 4.9289\n",
            "[17/25][150/391] Loss_D1: 0.2301 Loss_D2: 0.4874 Loss_G: 6.2079\n",
            "[17/25][151/391] Loss_D1: 0.3135 Loss_D2: 0.4259 Loss_G: 5.1291\n",
            "[17/25][152/391] Loss_D1: 0.4558 Loss_D2: 0.4910 Loss_G: 5.0326\n",
            "[17/25][153/391] Loss_D1: 0.5771 Loss_D2: 0.5136 Loss_G: 5.8251\n",
            "[17/25][154/391] Loss_D1: 0.5405 Loss_D2: 0.4383 Loss_G: 5.0905\n",
            "[17/25][155/391] Loss_D1: 0.4548 Loss_D2: 0.5797 Loss_G: 6.1957\n",
            "[17/25][156/391] Loss_D1: 0.5663 Loss_D2: 0.7412 Loss_G: 4.6401\n",
            "[17/25][157/391] Loss_D1: 0.4480 Loss_D2: 0.6344 Loss_G: 7.1416\n",
            "[17/25][158/391] Loss_D1: 0.3571 Loss_D2: 0.6573 Loss_G: 3.8401\n",
            "[17/25][159/391] Loss_D1: 0.3997 Loss_D2: 0.6635 Loss_G: 6.6917\n",
            "[17/25][160/391] Loss_D1: 0.2452 Loss_D2: 0.4481 Loss_G: 5.9690\n",
            "[17/25][161/391] Loss_D1: 0.2694 Loss_D2: 0.3880 Loss_G: 4.9173\n",
            "[17/25][162/391] Loss_D1: 0.7150 Loss_D2: 0.5486 Loss_G: 5.8391\n",
            "[17/25][163/391] Loss_D1: 0.8364 Loss_D2: 0.8069 Loss_G: 5.8197\n",
            "[17/25][164/391] Loss_D1: 0.5172 Loss_D2: 0.9987 Loss_G: 5.6797\n",
            "[17/25][165/391] Loss_D1: 0.7643 Loss_D2: 0.7366 Loss_G: 3.3757\n",
            "[17/25][166/391] Loss_D1: 0.7161 Loss_D2: 1.0305 Loss_G: 8.3973\n",
            "[17/25][167/391] Loss_D1: 0.3582 Loss_D2: 1.0374 Loss_G: 3.8084\n",
            "[17/25][168/391] Loss_D1: 0.4759 Loss_D2: 1.1331 Loss_G: 7.4078\n",
            "[17/25][169/391] Loss_D1: 0.6011 Loss_D2: 0.8295 Loss_G: 3.4902\n",
            "[17/25][170/391] Loss_D1: 0.4920 Loss_D2: 0.9095 Loss_G: 7.2706\n",
            "[17/25][171/391] Loss_D1: 0.4278 Loss_D2: 1.1507 Loss_G: 4.2148\n",
            "[17/25][172/391] Loss_D1: 0.9444 Loss_D2: 0.9951 Loss_G: 4.6679\n",
            "[17/25][173/391] Loss_D1: 1.2331 Loss_D2: 0.5089 Loss_G: 7.7722\n",
            "[17/25][174/391] Loss_D1: 0.9828 Loss_D2: 0.6722 Loss_G: 2.4620\n",
            "[17/25][175/391] Loss_D1: 0.9789 Loss_D2: 1.0673 Loss_G: 9.1606\n",
            "[17/25][176/391] Loss_D1: 0.5041 Loss_D2: 0.8473 Loss_G: 5.1104\n",
            "[17/25][177/391] Loss_D1: 0.4547 Loss_D2: 0.8274 Loss_G: 4.3625\n",
            "[17/25][178/391] Loss_D1: 0.9508 Loss_D2: 0.6300 Loss_G: 7.0848\n",
            "[17/25][179/391] Loss_D1: 1.2621 Loss_D2: 0.5262 Loss_G: 2.4326\n",
            "[17/25][180/391] Loss_D1: 1.7626 Loss_D2: 0.5743 Loss_G: 9.8480\n",
            "[17/25][181/391] Loss_D1: 1.5642 Loss_D2: 0.9695 Loss_G: 1.3957\n",
            "[17/25][182/391] Loss_D1: 1.5028 Loss_D2: 1.0487 Loss_G: 11.5296\n",
            "[17/25][183/391] Loss_D1: 1.5717 Loss_D2: 0.6588 Loss_G: 2.7885\n",
            "[17/25][184/391] Loss_D1: 0.7296 Loss_D2: 0.6223 Loss_G: 6.9174\n",
            "[17/25][185/391] Loss_D1: 0.3081 Loss_D2: 0.5302 Loss_G: 6.5462\n",
            "[17/25][186/391] Loss_D1: 0.8276 Loss_D2: 0.5059 Loss_G: 4.3096\n",
            "[17/25][187/391] Loss_D1: 1.4489 Loss_D2: 0.6138 Loss_G: 5.6583\n",
            "[17/25][188/391] Loss_D1: 0.8521 Loss_D2: 1.0783 Loss_G: 6.9809\n",
            "[17/25][189/391] Loss_D1: 0.5354 Loss_D2: 1.1319 Loss_G: 2.4560\n",
            "[17/25][190/391] Loss_D1: 0.6604 Loss_D2: 1.0001 Loss_G: 7.6880\n",
            "[17/25][191/391] Loss_D1: 0.7478 Loss_D2: 0.9020 Loss_G: 3.4260\n",
            "[17/25][192/391] Loss_D1: 0.8382 Loss_D2: 0.6195 Loss_G: 5.5941\n",
            "[17/25][193/391] Loss_D1: 0.5998 Loss_D2: 0.5014 Loss_G: 5.4315\n",
            "[17/25][194/391] Loss_D1: 0.5905 Loss_D2: 0.4397 Loss_G: 4.7531\n",
            "[17/25][195/391] Loss_D1: 0.3394 Loss_D2: 0.5911 Loss_G: 4.8502\n",
            "[17/25][196/391] Loss_D1: 0.4447 Loss_D2: 0.6326 Loss_G: 5.7995\n",
            "[17/25][197/391] Loss_D1: 0.4060 Loss_D2: 0.4656 Loss_G: 5.4161\n",
            "[17/25][198/391] Loss_D1: 0.4636 Loss_D2: 0.5222 Loss_G: 4.8088\n",
            "[17/25][199/391] Loss_D1: 0.3624 Loss_D2: 0.6467 Loss_G: 5.0741\n",
            "[17/25][200/391] Loss_D1: 0.7178 Loss_D2: 0.6266 Loss_G: 4.1729\n",
            "saving the output\n",
            "[17/25][201/391] Loss_D1: 0.4953 Loss_D2: 0.6540 Loss_G: 5.2976\n",
            "[17/25][202/391] Loss_D1: 0.4784 Loss_D2: 0.5676 Loss_G: 6.0373\n",
            "[17/25][203/391] Loss_D1: 0.3573 Loss_D2: 0.3737 Loss_G: 5.4404\n",
            "[17/25][204/391] Loss_D1: 0.5775 Loss_D2: 0.4228 Loss_G: 4.0037\n",
            "[17/25][205/391] Loss_D1: 0.4308 Loss_D2: 0.5102 Loss_G: 6.7792\n",
            "[17/25][206/391] Loss_D1: 0.5469 Loss_D2: 0.4959 Loss_G: 5.0637\n",
            "[17/25][207/391] Loss_D1: 0.3899 Loss_D2: 0.7415 Loss_G: 3.9778\n",
            "[17/25][208/391] Loss_D1: 0.1766 Loss_D2: 1.0265 Loss_G: 8.5235\n",
            "[17/25][209/391] Loss_D1: 0.3392 Loss_D2: 1.0327 Loss_G: 3.7047\n",
            "[17/25][210/391] Loss_D1: 0.3498 Loss_D2: 0.8761 Loss_G: 6.1236\n",
            "[17/25][211/391] Loss_D1: 0.3771 Loss_D2: 0.6336 Loss_G: 4.6937\n",
            "[17/25][212/391] Loss_D1: 0.3911 Loss_D2: 0.6328 Loss_G: 5.2519\n",
            "[17/25][213/391] Loss_D1: 0.3966 Loss_D2: 0.5285 Loss_G: 5.4753\n",
            "[17/25][214/391] Loss_D1: 0.3875 Loss_D2: 0.8543 Loss_G: 4.0968\n",
            "[17/25][215/391] Loss_D1: 0.5353 Loss_D2: 0.6160 Loss_G: 5.3085\n",
            "[17/25][216/391] Loss_D1: 0.4994 Loss_D2: 0.3879 Loss_G: 6.2834\n",
            "[17/25][217/391] Loss_D1: 0.4203 Loss_D2: 0.5265 Loss_G: 4.3598\n",
            "[17/25][218/391] Loss_D1: 0.4251 Loss_D2: 0.6214 Loss_G: 5.6056\n",
            "[17/25][219/391] Loss_D1: 0.4083 Loss_D2: 0.6140 Loss_G: 5.1840\n",
            "[17/25][220/391] Loss_D1: 0.4460 Loss_D2: 0.5561 Loss_G: 5.6380\n",
            "[17/25][221/391] Loss_D1: 0.4479 Loss_D2: 0.5533 Loss_G: 4.4839\n",
            "[17/25][222/391] Loss_D1: 0.5224 Loss_D2: 0.5061 Loss_G: 5.7795\n",
            "[17/25][223/391] Loss_D1: 0.3238 Loss_D2: 0.4994 Loss_G: 5.7509\n",
            "[17/25][224/391] Loss_D1: 0.4722 Loss_D2: 0.4963 Loss_G: 4.6632\n",
            "[17/25][225/391] Loss_D1: 0.4169 Loss_D2: 0.6026 Loss_G: 6.3732\n",
            "[17/25][226/391] Loss_D1: 0.2350 Loss_D2: 0.5689 Loss_G: 4.9637\n",
            "[17/25][227/391] Loss_D1: 0.7117 Loss_D2: 0.6158 Loss_G: 4.1210\n",
            "[17/25][228/391] Loss_D1: 0.9868 Loss_D2: 0.4502 Loss_G: 7.0916\n",
            "[17/25][229/391] Loss_D1: 0.6622 Loss_D2: 0.4232 Loss_G: 2.6391\n",
            "[17/25][230/391] Loss_D1: 1.8498 Loss_D2: 0.4790 Loss_G: 11.5912\n",
            "[17/25][231/391] Loss_D1: 2.8284 Loss_D2: 0.4672 Loss_G: 2.7538\n",
            "[17/25][232/391] Loss_D1: 1.7361 Loss_D2: 0.4714 Loss_G: 7.8509\n",
            "[17/25][233/391] Loss_D1: 2.3836 Loss_D2: 0.3798 Loss_G: 3.9359\n",
            "[17/25][234/391] Loss_D1: 1.5535 Loss_D2: 0.4475 Loss_G: 6.6809\n",
            "[17/25][235/391] Loss_D1: 1.8028 Loss_D2: 0.5988 Loss_G: 2.3124\n",
            "[17/25][236/391] Loss_D1: 1.6162 Loss_D2: 0.5912 Loss_G: 8.2417\n",
            "[17/25][237/391] Loss_D1: 0.8350 Loss_D2: 0.4941 Loss_G: 4.6246\n",
            "[17/25][238/391] Loss_D1: 0.5467 Loss_D2: 0.4458 Loss_G: 3.7621\n",
            "[17/25][239/391] Loss_D1: 0.8819 Loss_D2: 0.6750 Loss_G: 8.3868\n",
            "[17/25][240/391] Loss_D1: 0.6019 Loss_D2: 0.9952 Loss_G: 3.8653\n",
            "[17/25][241/391] Loss_D1: 0.6387 Loss_D2: 0.9408 Loss_G: 5.1625\n",
            "[17/25][242/391] Loss_D1: 0.6243 Loss_D2: 0.6066 Loss_G: 5.9306\n",
            "[17/25][243/391] Loss_D1: 0.5426 Loss_D2: 0.4534 Loss_G: 4.7478\n",
            "[17/25][244/391] Loss_D1: 0.9076 Loss_D2: 0.4329 Loss_G: 4.0741\n",
            "[17/25][245/391] Loss_D1: 0.5873 Loss_D2: 0.3798 Loss_G: 6.8161\n",
            "[17/25][246/391] Loss_D1: 0.8759 Loss_D2: 0.5509 Loss_G: 4.0848\n",
            "[17/25][247/391] Loss_D1: 0.6613 Loss_D2: 0.3891 Loss_G: 5.8042\n",
            "[17/25][248/391] Loss_D1: 0.3417 Loss_D2: 0.5310 Loss_G: 5.0848\n",
            "[17/25][249/391] Loss_D1: 0.4755 Loss_D2: 0.4582 Loss_G: 4.6313\n",
            "[17/25][250/391] Loss_D1: 0.3859 Loss_D2: 0.5085 Loss_G: 6.5963\n",
            "[17/25][251/391] Loss_D1: 0.4533 Loss_D2: 0.9395 Loss_G: 3.3751\n",
            "[17/25][252/391] Loss_D1: 0.5237 Loss_D2: 0.9607 Loss_G: 7.2630\n",
            "[17/25][253/391] Loss_D1: 0.3384 Loss_D2: 1.0169 Loss_G: 3.6852\n",
            "[17/25][254/391] Loss_D1: 0.5237 Loss_D2: 1.2955 Loss_G: 7.2331\n",
            "[17/25][255/391] Loss_D1: 0.4017 Loss_D2: 0.8592 Loss_G: 3.2729\n",
            "[17/25][256/391] Loss_D1: 0.4295 Loss_D2: 1.2048 Loss_G: 9.0420\n",
            "[17/25][257/391] Loss_D1: 0.5972 Loss_D2: 1.5815 Loss_G: 2.4443\n",
            "[17/25][258/391] Loss_D1: 0.4588 Loss_D2: 1.4891 Loss_G: 7.8574\n",
            "[17/25][259/391] Loss_D1: 0.8262 Loss_D2: 1.5778 Loss_G: 4.5143\n",
            "[17/25][260/391] Loss_D1: 0.5050 Loss_D2: 1.3046 Loss_G: 6.1385\n",
            "[17/25][261/391] Loss_D1: 0.3667 Loss_D2: 0.8822 Loss_G: 3.9108\n",
            "[17/25][262/391] Loss_D1: 0.7229 Loss_D2: 0.5867 Loss_G: 6.2625\n",
            "[17/25][263/391] Loss_D1: 0.6714 Loss_D2: 0.8478 Loss_G: 4.0436\n",
            "[17/25][264/391] Loss_D1: 0.4251 Loss_D2: 0.6871 Loss_G: 5.6776\n",
            "[17/25][265/391] Loss_D1: 0.4836 Loss_D2: 0.7596 Loss_G: 5.4690\n",
            "[17/25][266/391] Loss_D1: 0.8012 Loss_D2: 0.6890 Loss_G: 4.1849\n",
            "[17/25][267/391] Loss_D1: 0.6337 Loss_D2: 0.5769 Loss_G: 5.0470\n",
            "[17/25][268/391] Loss_D1: 0.5432 Loss_D2: 0.6926 Loss_G: 4.8720\n",
            "[17/25][269/391] Loss_D1: 0.6092 Loss_D2: 0.9053 Loss_G: 6.3668\n",
            "[17/25][270/391] Loss_D1: 0.5686 Loss_D2: 1.1609 Loss_G: 5.5356\n",
            "[17/25][271/391] Loss_D1: 0.4799 Loss_D2: 0.9420 Loss_G: 5.8832\n",
            "[17/25][272/391] Loss_D1: 0.4873 Loss_D2: 1.0256 Loss_G: 3.9961\n",
            "[17/25][273/391] Loss_D1: 0.3631 Loss_D2: 0.5349 Loss_G: 6.9197\n",
            "[17/25][274/391] Loss_D1: 0.6072 Loss_D2: 0.5216 Loss_G: 4.3891\n",
            "[17/25][275/391] Loss_D1: 0.5933 Loss_D2: 0.4262 Loss_G: 5.8659\n",
            "[17/25][276/391] Loss_D1: 0.5169 Loss_D2: 0.4769 Loss_G: 4.7804\n",
            "[17/25][277/391] Loss_D1: 0.6638 Loss_D2: 0.5951 Loss_G: 4.3515\n",
            "[17/25][278/391] Loss_D1: 0.5232 Loss_D2: 0.5507 Loss_G: 5.8087\n",
            "[17/25][279/391] Loss_D1: 0.4552 Loss_D2: 0.4935 Loss_G: 4.9421\n",
            "[17/25][280/391] Loss_D1: 0.5326 Loss_D2: 0.6461 Loss_G: 4.9646\n",
            "[17/25][281/391] Loss_D1: 0.5522 Loss_D2: 0.9393 Loss_G: 3.5461\n",
            "[17/25][282/391] Loss_D1: 0.2764 Loss_D2: 0.8465 Loss_G: 7.4221\n",
            "[17/25][283/391] Loss_D1: 0.3569 Loss_D2: 0.5984 Loss_G: 4.9394\n",
            "[17/25][284/391] Loss_D1: 0.4276 Loss_D2: 0.4579 Loss_G: 4.5415\n",
            "[17/25][285/391] Loss_D1: 0.4732 Loss_D2: 0.5729 Loss_G: 4.4482\n",
            "[17/25][286/391] Loss_D1: 0.5170 Loss_D2: 0.5917 Loss_G: 4.6258\n",
            "[17/25][287/391] Loss_D1: 0.5216 Loss_D2: 0.7399 Loss_G: 5.4865\n",
            "[17/25][288/391] Loss_D1: 0.4796 Loss_D2: 0.5632 Loss_G: 5.0602\n",
            "[17/25][289/391] Loss_D1: 0.4712 Loss_D2: 0.4204 Loss_G: 5.3090\n",
            "[17/25][290/391] Loss_D1: 0.4049 Loss_D2: 0.4894 Loss_G: 4.6831\n",
            "[17/25][291/391] Loss_D1: 0.4493 Loss_D2: 0.4426 Loss_G: 6.0687\n",
            "[17/25][292/391] Loss_D1: 0.4968 Loss_D2: 0.4166 Loss_G: 5.4496\n",
            "[17/25][293/391] Loss_D1: 0.3370 Loss_D2: 0.5435 Loss_G: 4.3127\n",
            "[17/25][294/391] Loss_D1: 0.3397 Loss_D2: 0.4389 Loss_G: 7.3747\n",
            "[17/25][295/391] Loss_D1: 0.7913 Loss_D2: 0.4831 Loss_G: 3.8632\n",
            "[17/25][296/391] Loss_D1: 0.5974 Loss_D2: 0.3891 Loss_G: 6.1772\n",
            "[17/25][297/391] Loss_D1: 0.2885 Loss_D2: 0.4124 Loss_G: 6.3435\n",
            "[17/25][298/391] Loss_D1: 0.5549 Loss_D2: 0.6110 Loss_G: 2.3742\n",
            "[17/25][299/391] Loss_D1: 1.1643 Loss_D2: 0.7559 Loss_G: 8.4781\n",
            "[17/25][300/391] Loss_D1: 1.1268 Loss_D2: 1.1331 Loss_G: 1.2326\n",
            "saving the output\n",
            "[17/25][301/391] Loss_D1: 1.7901 Loss_D2: 1.3687 Loss_G: 10.4755\n",
            "[17/25][302/391] Loss_D1: 1.0015 Loss_D2: 0.5003 Loss_G: 4.6587\n",
            "[17/25][303/391] Loss_D1: 0.6084 Loss_D2: 0.6986 Loss_G: 7.0015\n",
            "[17/25][304/391] Loss_D1: 0.9961 Loss_D2: 1.8832 Loss_G: 4.4863\n",
            "[17/25][305/391] Loss_D1: 1.0753 Loss_D2: 1.8781 Loss_G: 5.7177\n",
            "[17/25][306/391] Loss_D1: 1.3452 Loss_D2: 1.1444 Loss_G: 5.0071\n",
            "[17/25][307/391] Loss_D1: 1.2907 Loss_D2: 1.8800 Loss_G: 8.2021\n",
            "[17/25][308/391] Loss_D1: 0.6345 Loss_D2: 2.2553 Loss_G: 2.5095\n",
            "[17/25][309/391] Loss_D1: 0.6251 Loss_D2: 1.9691 Loss_G: 10.0574\n",
            "[17/25][310/391] Loss_D1: 0.9313 Loss_D2: 1.4198 Loss_G: 2.4411\n",
            "[17/25][311/391] Loss_D1: 0.9408 Loss_D2: 0.8643 Loss_G: 5.9097\n",
            "[17/25][312/391] Loss_D1: 0.4131 Loss_D2: 1.0005 Loss_G: 6.2039\n",
            "[17/25][313/391] Loss_D1: 0.6326 Loss_D2: 0.9064 Loss_G: 3.1076\n",
            "[17/25][314/391] Loss_D1: 0.6574 Loss_D2: 1.0574 Loss_G: 6.1186\n",
            "[17/25][315/391] Loss_D1: 0.7511 Loss_D2: 0.8710 Loss_G: 3.0725\n",
            "[17/25][316/391] Loss_D1: 0.8019 Loss_D2: 0.7481 Loss_G: 7.2943\n",
            "[17/25][317/391] Loss_D1: 0.6868 Loss_D2: 0.5357 Loss_G: 4.9313\n",
            "[17/25][318/391] Loss_D1: 0.4429 Loss_D2: 0.5601 Loss_G: 4.3854\n",
            "[17/25][319/391] Loss_D1: 0.4439 Loss_D2: 0.4960 Loss_G: 6.1979\n",
            "[17/25][320/391] Loss_D1: 0.5476 Loss_D2: 0.8953 Loss_G: 4.3225\n",
            "[17/25][321/391] Loss_D1: 0.3620 Loss_D2: 0.7719 Loss_G: 6.6421\n",
            "[17/25][322/391] Loss_D1: 0.3571 Loss_D2: 0.5641 Loss_G: 5.0894\n",
            "[17/25][323/391] Loss_D1: 0.4809 Loss_D2: 0.5506 Loss_G: 4.1965\n",
            "[17/25][324/391] Loss_D1: 0.5023 Loss_D2: 0.5992 Loss_G: 5.6061\n",
            "[17/25][325/391] Loss_D1: 0.4817 Loss_D2: 0.7483 Loss_G: 5.8248\n",
            "[17/25][326/391] Loss_D1: 0.8570 Loss_D2: 0.5276 Loss_G: 4.1528\n",
            "[17/25][327/391] Loss_D1: 0.6944 Loss_D2: 0.4582 Loss_G: 6.4377\n",
            "[17/25][328/391] Loss_D1: 0.3047 Loss_D2: 0.4703 Loss_G: 6.5711\n",
            "[17/25][329/391] Loss_D1: 0.6152 Loss_D2: 0.6183 Loss_G: 2.9375\n",
            "[17/25][330/391] Loss_D1: 0.8644 Loss_D2: 0.5954 Loss_G: 7.3855\n",
            "[17/25][331/391] Loss_D1: 1.1233 Loss_D2: 0.6528 Loss_G: 4.0020\n",
            "[17/25][332/391] Loss_D1: 0.6944 Loss_D2: 0.5650 Loss_G: 5.5932\n",
            "[17/25][333/391] Loss_D1: 0.3089 Loss_D2: 0.4459 Loss_G: 5.5174\n",
            "[17/25][334/391] Loss_D1: 0.3770 Loss_D2: 0.5212 Loss_G: 5.4156\n",
            "[17/25][335/391] Loss_D1: 0.3380 Loss_D2: 0.4976 Loss_G: 4.6031\n",
            "[17/25][336/391] Loss_D1: 0.4418 Loss_D2: 0.6129 Loss_G: 7.0290\n",
            "[17/25][337/391] Loss_D1: 0.5283 Loss_D2: 0.8772 Loss_G: 3.5773\n",
            "[17/25][338/391] Loss_D1: 0.3102 Loss_D2: 0.5997 Loss_G: 6.0884\n",
            "[17/25][339/391] Loss_D1: 0.4231 Loss_D2: 0.4943 Loss_G: 6.1196\n",
            "[17/25][340/391] Loss_D1: 0.6745 Loss_D2: 0.4310 Loss_G: 3.4745\n",
            "[17/25][341/391] Loss_D1: 0.7675 Loss_D2: 0.5245 Loss_G: 7.1041\n",
            "[17/25][342/391] Loss_D1: 0.4088 Loss_D2: 0.5470 Loss_G: 5.6165\n",
            "[17/25][343/391] Loss_D1: 0.3539 Loss_D2: 0.7251 Loss_G: 3.3828\n",
            "[17/25][344/391] Loss_D1: 0.3908 Loss_D2: 0.9335 Loss_G: 7.3060\n",
            "[17/25][345/391] Loss_D1: 0.3025 Loss_D2: 0.8790 Loss_G: 4.5979\n",
            "[17/25][346/391] Loss_D1: 0.3875 Loss_D2: 0.7700 Loss_G: 5.7671\n",
            "[17/25][347/391] Loss_D1: 0.4560 Loss_D2: 0.3501 Loss_G: 5.3863\n",
            "[17/25][348/391] Loss_D1: 0.3934 Loss_D2: 0.5080 Loss_G: 4.1369\n",
            "[17/25][349/391] Loss_D1: 0.7148 Loss_D2: 0.5118 Loss_G: 5.2134\n",
            "[17/25][350/391] Loss_D1: 0.3647 Loss_D2: 0.5384 Loss_G: 5.9160\n",
            "[17/25][351/391] Loss_D1: 0.5333 Loss_D2: 0.4880 Loss_G: 4.1567\n",
            "[17/25][352/391] Loss_D1: 0.4396 Loss_D2: 0.6950 Loss_G: 5.9941\n",
            "[17/25][353/391] Loss_D1: 0.3698 Loss_D2: 0.5143 Loss_G: 5.0037\n",
            "[17/25][354/391] Loss_D1: 0.6011 Loss_D2: 0.4370 Loss_G: 3.8587\n",
            "[17/25][355/391] Loss_D1: 0.5374 Loss_D2: 0.4929 Loss_G: 7.2422\n",
            "[17/25][356/391] Loss_D1: 0.3917 Loss_D2: 0.4707 Loss_G: 5.2827\n",
            "[17/25][357/391] Loss_D1: 0.6042 Loss_D2: 0.5771 Loss_G: 3.1809\n",
            "[17/25][358/391] Loss_D1: 0.6596 Loss_D2: 0.6117 Loss_G: 7.3989\n",
            "[17/25][359/391] Loss_D1: 0.5535 Loss_D2: 0.7800 Loss_G: 3.8227\n",
            "[17/25][360/391] Loss_D1: 0.5122 Loss_D2: 0.6582 Loss_G: 5.8253\n",
            "[17/25][361/391] Loss_D1: 0.5097 Loss_D2: 0.5595 Loss_G: 5.3768\n",
            "[17/25][362/391] Loss_D1: 0.3468 Loss_D2: 0.5568 Loss_G: 4.9666\n",
            "[17/25][363/391] Loss_D1: 0.4689 Loss_D2: 0.4498 Loss_G: 4.7103\n",
            "[17/25][364/391] Loss_D1: 0.4827 Loss_D2: 0.5824 Loss_G: 5.0708\n",
            "[17/25][365/391] Loss_D1: 0.3016 Loss_D2: 0.6200 Loss_G: 5.8632\n",
            "[17/25][366/391] Loss_D1: 0.5378 Loss_D2: 0.7240 Loss_G: 3.2505\n",
            "[17/25][367/391] Loss_D1: 0.6843 Loss_D2: 0.6763 Loss_G: 6.5894\n",
            "[17/25][368/391] Loss_D1: 0.3647 Loss_D2: 0.4492 Loss_G: 5.5838\n",
            "[17/25][369/391] Loss_D1: 0.3749 Loss_D2: 0.4248 Loss_G: 4.0216\n",
            "[17/25][370/391] Loss_D1: 0.4221 Loss_D2: 0.4515 Loss_G: 6.1818\n",
            "[17/25][371/391] Loss_D1: 0.3087 Loss_D2: 0.5059 Loss_G: 5.4947\n",
            "[17/25][372/391] Loss_D1: 0.5015 Loss_D2: 0.6966 Loss_G: 3.7294\n",
            "[17/25][373/391] Loss_D1: 0.5166 Loss_D2: 0.5892 Loss_G: 6.4712\n",
            "[17/25][374/391] Loss_D1: 0.4448 Loss_D2: 0.5052 Loss_G: 5.0346\n",
            "[17/25][375/391] Loss_D1: 0.4781 Loss_D2: 0.5106 Loss_G: 4.6056\n",
            "[17/25][376/391] Loss_D1: 0.4060 Loss_D2: 0.5986 Loss_G: 5.7759\n",
            "[17/25][377/391] Loss_D1: 0.4281 Loss_D2: 0.5358 Loss_G: 5.0443\n",
            "[17/25][378/391] Loss_D1: 0.4753 Loss_D2: 0.4673 Loss_G: 6.8548\n",
            "[17/25][379/391] Loss_D1: 0.6986 Loss_D2: 0.4779 Loss_G: 3.6263\n",
            "[17/25][380/391] Loss_D1: 0.5587 Loss_D2: 0.5097 Loss_G: 5.4197\n",
            "[17/25][381/391] Loss_D1: 0.2602 Loss_D2: 0.4565 Loss_G: 6.3315\n",
            "[17/25][382/391] Loss_D1: 0.6682 Loss_D2: 0.5232 Loss_G: 3.1290\n",
            "[17/25][383/391] Loss_D1: 0.9131 Loss_D2: 0.4257 Loss_G: 6.9536\n",
            "[17/25][384/391] Loss_D1: 0.6407 Loss_D2: 0.3855 Loss_G: 4.6867\n",
            "[17/25][385/391] Loss_D1: 0.5147 Loss_D2: 0.5419 Loss_G: 4.4968\n",
            "[17/25][386/391] Loss_D1: 0.3927 Loss_D2: 0.7738 Loss_G: 6.6160\n",
            "[17/25][387/391] Loss_D1: 0.3432 Loss_D2: 0.3947 Loss_G: 5.8482\n",
            "[17/25][388/391] Loss_D1: 0.6225 Loss_D2: 0.4794 Loss_G: 2.6853\n",
            "[17/25][389/391] Loss_D1: 0.6164 Loss_D2: 0.8425 Loss_G: 7.9160\n",
            "[17/25][390/391] Loss_D1: 0.8515 Loss_D2: 0.8736 Loss_G: 2.6606\n",
            "[18/25][0/391] Loss_D1: 0.5039 Loss_D2: 0.8481 Loss_G: 7.1693\n",
            "saving the output\n",
            "[18/25][1/391] Loss_D1: 0.3854 Loss_D2: 0.5680 Loss_G: 4.9975\n",
            "[18/25][2/391] Loss_D1: 0.2919 Loss_D2: 0.6038 Loss_G: 5.2065\n",
            "[18/25][3/391] Loss_D1: 0.2727 Loss_D2: 0.3742 Loss_G: 5.8314\n",
            "[18/25][4/391] Loss_D1: 0.3603 Loss_D2: 0.4028 Loss_G: 5.5763\n",
            "[18/25][5/391] Loss_D1: 0.5980 Loss_D2: 0.4575 Loss_G: 3.8226\n",
            "[18/25][6/391] Loss_D1: 0.6323 Loss_D2: 0.6805 Loss_G: 7.2437\n",
            "[18/25][7/391] Loss_D1: 0.3357 Loss_D2: 0.7037 Loss_G: 4.6875\n",
            "[18/25][8/391] Loss_D1: 0.4593 Loss_D2: 0.8207 Loss_G: 5.1640\n",
            "[18/25][9/391] Loss_D1: 0.6518 Loss_D2: 0.6476 Loss_G: 4.6948\n",
            "[18/25][10/391] Loss_D1: 0.3942 Loss_D2: 0.5361 Loss_G: 6.1805\n",
            "[18/25][11/391] Loss_D1: 0.3910 Loss_D2: 0.2911 Loss_G: 6.4989\n",
            "[18/25][12/391] Loss_D1: 0.4410 Loss_D2: 0.6457 Loss_G: 3.5540\n",
            "[18/25][13/391] Loss_D1: 0.3250 Loss_D2: 0.5137 Loss_G: 6.3767\n",
            "[18/25][14/391] Loss_D1: 0.2393 Loss_D2: 0.4711 Loss_G: 6.2166\n",
            "[18/25][15/391] Loss_D1: 0.4424 Loss_D2: 0.5080 Loss_G: 3.9762\n",
            "[18/25][16/391] Loss_D1: 0.5419 Loss_D2: 0.4317 Loss_G: 6.2424\n",
            "[18/25][17/391] Loss_D1: 0.3806 Loss_D2: 0.4559 Loss_G: 5.4675\n",
            "[18/25][18/391] Loss_D1: 0.3982 Loss_D2: 0.4372 Loss_G: 4.7348\n",
            "[18/25][19/391] Loss_D1: 0.3754 Loss_D2: 0.4195 Loss_G: 5.9704\n",
            "[18/25][20/391] Loss_D1: 0.2995 Loss_D2: 0.4023 Loss_G: 6.4085\n",
            "[18/25][21/391] Loss_D1: 0.2987 Loss_D2: 0.4200 Loss_G: 4.8507\n",
            "[18/25][22/391] Loss_D1: 0.4745 Loss_D2: 0.6318 Loss_G: 4.5865\n",
            "[18/25][23/391] Loss_D1: 0.4046 Loss_D2: 0.5791 Loss_G: 5.1562\n",
            "[18/25][24/391] Loss_D1: 0.4562 Loss_D2: 0.4894 Loss_G: 5.3962\n",
            "[18/25][25/391] Loss_D1: 0.6764 Loss_D2: 0.6106 Loss_G: 4.9638\n",
            "[18/25][26/391] Loss_D1: 0.5879 Loss_D2: 0.6778 Loss_G: 4.8711\n",
            "[18/25][27/391] Loss_D1: 0.3134 Loss_D2: 0.6992 Loss_G: 6.3412\n",
            "[18/25][28/391] Loss_D1: 0.4138 Loss_D2: 0.5820 Loss_G: 4.6927\n",
            "[18/25][29/391] Loss_D1: 0.3161 Loss_D2: 0.4041 Loss_G: 5.3960\n",
            "[18/25][30/391] Loss_D1: 0.4298 Loss_D2: 0.4929 Loss_G: 5.9979\n",
            "[18/25][31/391] Loss_D1: 0.6261 Loss_D2: 0.4298 Loss_G: 4.6233\n",
            "[18/25][32/391] Loss_D1: 0.4524 Loss_D2: 0.4030 Loss_G: 5.4549\n",
            "[18/25][33/391] Loss_D1: 0.2716 Loss_D2: 0.3089 Loss_G: 6.0513\n",
            "[18/25][34/391] Loss_D1: 0.3648 Loss_D2: 0.5047 Loss_G: 6.0877\n",
            "[18/25][35/391] Loss_D1: 0.3764 Loss_D2: 0.4693 Loss_G: 3.9374\n",
            "[18/25][36/391] Loss_D1: 0.4960 Loss_D2: 0.4507 Loss_G: 6.8726\n",
            "[18/25][37/391] Loss_D1: 0.3278 Loss_D2: 0.4153 Loss_G: 5.1931\n",
            "[18/25][38/391] Loss_D1: 0.3717 Loss_D2: 0.4097 Loss_G: 4.5924\n",
            "[18/25][39/391] Loss_D1: 0.3509 Loss_D2: 0.5026 Loss_G: 6.0764\n",
            "[18/25][40/391] Loss_D1: 0.4050 Loss_D2: 0.4316 Loss_G: 4.9295\n",
            "[18/25][41/391] Loss_D1: 0.7171 Loss_D2: 0.3769 Loss_G: 6.5061\n",
            "[18/25][42/391] Loss_D1: 0.4457 Loss_D2: 0.4507 Loss_G: 5.7160\n",
            "[18/25][43/391] Loss_D1: 0.2780 Loss_D2: 0.4647 Loss_G: 4.1941\n",
            "[18/25][44/391] Loss_D1: 0.4406 Loss_D2: 0.5856 Loss_G: 5.2804\n",
            "[18/25][45/391] Loss_D1: 0.3467 Loss_D2: 0.3613 Loss_G: 5.9732\n",
            "[18/25][46/391] Loss_D1: 0.5843 Loss_D2: 0.4884 Loss_G: 4.0789\n",
            "[18/25][47/391] Loss_D1: 0.4490 Loss_D2: 0.3170 Loss_G: 7.8326\n",
            "[18/25][48/391] Loss_D1: 0.4849 Loss_D2: 0.5965 Loss_G: 3.7308\n",
            "[18/25][49/391] Loss_D1: 0.4077 Loss_D2: 0.6434 Loss_G: 5.9465\n",
            "[18/25][50/391] Loss_D1: 0.4001 Loss_D2: 0.4764 Loss_G: 5.8651\n",
            "[18/25][51/391] Loss_D1: 0.3388 Loss_D2: 0.4584 Loss_G: 4.7742\n",
            "[18/25][52/391] Loss_D1: 0.3329 Loss_D2: 0.4671 Loss_G: 5.7124\n",
            "[18/25][53/391] Loss_D1: 0.3354 Loss_D2: 0.3830 Loss_G: 5.8240\n",
            "[18/25][54/391] Loss_D1: 0.4440 Loss_D2: 0.4883 Loss_G: 4.1315\n",
            "[18/25][55/391] Loss_D1: 0.3411 Loss_D2: 0.4972 Loss_G: 6.1253\n",
            "[18/25][56/391] Loss_D1: 0.2481 Loss_D2: 0.4736 Loss_G: 5.4444\n",
            "[18/25][57/391] Loss_D1: 0.4262 Loss_D2: 0.3361 Loss_G: 5.2348\n",
            "[18/25][58/391] Loss_D1: 0.5268 Loss_D2: 0.4337 Loss_G: 4.6725\n",
            "[18/25][59/391] Loss_D1: 0.3561 Loss_D2: 0.4185 Loss_G: 5.1757\n",
            "[18/25][60/391] Loss_D1: 0.3705 Loss_D2: 0.2422 Loss_G: 6.2374\n",
            "[18/25][61/391] Loss_D1: 0.4690 Loss_D2: 0.4002 Loss_G: 4.4823\n",
            "[18/25][62/391] Loss_D1: 0.3311 Loss_D2: 0.3502 Loss_G: 5.7144\n",
            "[18/25][63/391] Loss_D1: 0.3187 Loss_D2: 0.5824 Loss_G: 4.4505\n",
            "[18/25][64/391] Loss_D1: 0.3285 Loss_D2: 0.4503 Loss_G: 6.3620\n",
            "[18/25][65/391] Loss_D1: 0.3960 Loss_D2: 0.4297 Loss_G: 4.8198\n",
            "[18/25][66/391] Loss_D1: 0.4052 Loss_D2: 0.3587 Loss_G: 4.0693\n",
            "[18/25][67/391] Loss_D1: 0.5085 Loss_D2: 0.6624 Loss_G: 7.6293\n",
            "[18/25][68/391] Loss_D1: 0.4116 Loss_D2: 0.6563 Loss_G: 3.7883\n",
            "[18/25][69/391] Loss_D1: 0.4283 Loss_D2: 0.8628 Loss_G: 6.7743\n",
            "[18/25][70/391] Loss_D1: 0.5102 Loss_D2: 0.9846 Loss_G: 3.2749\n",
            "[18/25][71/391] Loss_D1: 0.3942 Loss_D2: 1.6933 Loss_G: 9.3131\n",
            "[18/25][72/391] Loss_D1: 0.5535 Loss_D2: 1.4506 Loss_G: 2.2596\n",
            "[18/25][73/391] Loss_D1: 0.7556 Loss_D2: 0.5200 Loss_G: 6.9481\n",
            "[18/25][74/391] Loss_D1: 0.3552 Loss_D2: 0.4012 Loss_G: 5.1058\n",
            "[18/25][75/391] Loss_D1: 0.7541 Loss_D2: 0.5394 Loss_G: 5.2168\n",
            "[18/25][76/391] Loss_D1: 0.5682 Loss_D2: 0.7212 Loss_G: 5.9372\n",
            "[18/25][77/391] Loss_D1: 0.6671 Loss_D2: 1.0782 Loss_G: 5.1423\n",
            "[18/25][78/391] Loss_D1: 0.8126 Loss_D2: 1.2702 Loss_G: 6.0316\n",
            "[18/25][79/391] Loss_D1: 0.7482 Loss_D2: 1.0778 Loss_G: 4.8464\n",
            "[18/25][80/391] Loss_D1: 0.4514 Loss_D2: 0.7868 Loss_G: 4.7970\n",
            "[18/25][81/391] Loss_D1: 1.0141 Loss_D2: 0.7816 Loss_G: 8.3714\n",
            "[18/25][82/391] Loss_D1: 2.1134 Loss_D2: 0.7404 Loss_G: 4.0213\n",
            "[18/25][83/391] Loss_D1: 1.0195 Loss_D2: 0.4800 Loss_G: 10.0324\n",
            "[18/25][84/391] Loss_D1: 2.4911 Loss_D2: 0.7036 Loss_G: 2.2352\n",
            "[18/25][85/391] Loss_D1: 6.8053 Loss_D2: 0.4752 Loss_G: 5.6303\n",
            "[18/25][86/391] Loss_D1: 0.5842 Loss_D2: 0.5227 Loss_G: 8.7976\n",
            "[18/25][87/391] Loss_D1: 1.3977 Loss_D2: 0.6768 Loss_G: 2.1127\n",
            "[18/25][88/391] Loss_D1: 2.4859 Loss_D2: 0.5086 Loss_G: 7.9612\n",
            "[18/25][89/391] Loss_D1: 1.6219 Loss_D2: 0.8217 Loss_G: 4.2613\n",
            "[18/25][90/391] Loss_D1: 1.6230 Loss_D2: 0.7270 Loss_G: 6.7206\n",
            "[18/25][91/391] Loss_D1: 0.9786 Loss_D2: 0.5640 Loss_G: 4.0340\n",
            "[18/25][92/391] Loss_D1: 0.8211 Loss_D2: 0.5640 Loss_G: 5.1448\n",
            "[18/25][93/391] Loss_D1: 0.7986 Loss_D2: 0.5147 Loss_G: 5.8347\n",
            "[18/25][94/391] Loss_D1: 0.7508 Loss_D2: 0.4165 Loss_G: 3.7106\n",
            "[18/25][95/391] Loss_D1: 0.5410 Loss_D2: 0.6069 Loss_G: 5.5827\n",
            "[18/25][96/391] Loss_D1: 0.8773 Loss_D2: 0.5594 Loss_G: 4.4917\n",
            "[18/25][97/391] Loss_D1: 0.7926 Loss_D2: 0.5001 Loss_G: 5.9777\n",
            "[18/25][98/391] Loss_D1: 0.7321 Loss_D2: 0.5532 Loss_G: 3.9416\n",
            "[18/25][99/391] Loss_D1: 0.4826 Loss_D2: 0.6024 Loss_G: 6.4883\n",
            "[18/25][100/391] Loss_D1: 0.6168 Loss_D2: 0.6046 Loss_G: 4.5659\n",
            "saving the output\n",
            "[18/25][101/391] Loss_D1: 0.3188 Loss_D2: 0.5088 Loss_G: 6.2271\n",
            "[18/25][102/391] Loss_D1: 0.6103 Loss_D2: 0.4613 Loss_G: 5.0159\n",
            "[18/25][103/391] Loss_D1: 0.6297 Loss_D2: 0.8414 Loss_G: 3.2469\n",
            "[18/25][104/391] Loss_D1: 0.4879 Loss_D2: 0.6325 Loss_G: 8.1412\n",
            "[18/25][105/391] Loss_D1: 0.3461 Loss_D2: 0.3600 Loss_G: 6.8638\n",
            "[18/25][106/391] Loss_D1: 0.5408 Loss_D2: 0.4753 Loss_G: 2.8074\n",
            "[18/25][107/391] Loss_D1: 0.9231 Loss_D2: 0.5363 Loss_G: 7.7832\n",
            "[18/25][108/391] Loss_D1: 0.5154 Loss_D2: 0.5265 Loss_G: 4.3357\n",
            "[18/25][109/391] Loss_D1: 0.5672 Loss_D2: 0.4005 Loss_G: 5.2160\n",
            "[18/25][110/391] Loss_D1: 0.5431 Loss_D2: 0.4635 Loss_G: 6.5872\n",
            "[18/25][111/391] Loss_D1: 0.8814 Loss_D2: 0.4044 Loss_G: 3.5297\n",
            "[18/25][112/391] Loss_D1: 0.8476 Loss_D2: 0.4975 Loss_G: 6.6136\n",
            "[18/25][113/391] Loss_D1: 1.1632 Loss_D2: 0.4821 Loss_G: 4.2575\n",
            "[18/25][114/391] Loss_D1: 0.5076 Loss_D2: 0.4116 Loss_G: 6.1510\n",
            "[18/25][115/391] Loss_D1: 0.5557 Loss_D2: 0.5073 Loss_G: 4.2229\n",
            "[18/25][116/391] Loss_D1: 0.4339 Loss_D2: 0.4097 Loss_G: 5.7275\n",
            "[18/25][117/391] Loss_D1: 0.3473 Loss_D2: 0.3927 Loss_G: 6.0147\n",
            "[18/25][118/391] Loss_D1: 0.5348 Loss_D2: 0.4701 Loss_G: 4.4745\n",
            "[18/25][119/391] Loss_D1: 0.6169 Loss_D2: 0.6740 Loss_G: 4.6694\n",
            "[18/25][120/391] Loss_D1: 0.5864 Loss_D2: 0.6310 Loss_G: 3.1255\n",
            "[18/25][121/391] Loss_D1: 0.6139 Loss_D2: 0.7260 Loss_G: 7.2211\n",
            "[18/25][122/391] Loss_D1: 0.3894 Loss_D2: 0.6323 Loss_G: 4.3078\n",
            "[18/25][123/391] Loss_D1: 0.3813 Loss_D2: 0.4404 Loss_G: 6.2717\n",
            "[18/25][124/391] Loss_D1: 0.3341 Loss_D2: 0.4276 Loss_G: 5.2896\n",
            "[18/25][125/391] Loss_D1: 0.2775 Loss_D2: 0.3907 Loss_G: 5.6525\n",
            "[18/25][126/391] Loss_D1: 0.3173 Loss_D2: 0.4114 Loss_G: 5.2424\n",
            "[18/25][127/391] Loss_D1: 0.2518 Loss_D2: 0.6091 Loss_G: 5.9913\n",
            "[18/25][128/391] Loss_D1: 0.5209 Loss_D2: 0.6694 Loss_G: 3.8928\n",
            "[18/25][129/391] Loss_D1: 0.3213 Loss_D2: 0.6414 Loss_G: 6.0784\n",
            "[18/25][130/391] Loss_D1: 0.6274 Loss_D2: 0.4792 Loss_G: 4.6370\n",
            "[18/25][131/391] Loss_D1: 0.4852 Loss_D2: 0.3927 Loss_G: 5.8080\n",
            "[18/25][132/391] Loss_D1: 0.5167 Loss_D2: 0.4654 Loss_G: 5.5874\n",
            "[18/25][133/391] Loss_D1: 0.4172 Loss_D2: 0.3710 Loss_G: 4.6318\n",
            "[18/25][134/391] Loss_D1: 0.4917 Loss_D2: 0.4260 Loss_G: 5.0919\n",
            "[18/25][135/391] Loss_D1: 0.3736 Loss_D2: 0.5407 Loss_G: 5.2195\n",
            "[18/25][136/391] Loss_D1: 0.3750 Loss_D2: 0.3796 Loss_G: 5.5977\n",
            "[18/25][137/391] Loss_D1: 0.5296 Loss_D2: 0.4864 Loss_G: 4.5382\n",
            "[18/25][138/391] Loss_D1: 0.5181 Loss_D2: 0.4354 Loss_G: 5.2237\n",
            "[18/25][139/391] Loss_D1: 0.3192 Loss_D2: 0.4045 Loss_G: 5.5612\n",
            "[18/25][140/391] Loss_D1: 0.2711 Loss_D2: 0.5644 Loss_G: 5.2197\n",
            "[18/25][141/391] Loss_D1: 0.3233 Loss_D2: 0.4046 Loss_G: 5.5946\n",
            "[18/25][142/391] Loss_D1: 0.4066 Loss_D2: 0.3972 Loss_G: 5.4957\n",
            "[18/25][143/391] Loss_D1: 0.3725 Loss_D2: 0.6372 Loss_G: 4.1634\n",
            "[18/25][144/391] Loss_D1: 0.4326 Loss_D2: 0.4984 Loss_G: 6.1732\n",
            "[18/25][145/391] Loss_D1: 0.3390 Loss_D2: 0.5496 Loss_G: 4.2939\n",
            "[18/25][146/391] Loss_D1: 0.3774 Loss_D2: 0.4589 Loss_G: 6.6542\n",
            "[18/25][147/391] Loss_D1: 0.5129 Loss_D2: 0.4527 Loss_G: 4.4241\n",
            "[18/25][148/391] Loss_D1: 0.3992 Loss_D2: 0.4256 Loss_G: 4.3414\n",
            "[18/25][149/391] Loss_D1: 0.5216 Loss_D2: 0.3628 Loss_G: 7.4073\n",
            "[18/25][150/391] Loss_D1: 0.3736 Loss_D2: 0.4705 Loss_G: 5.2139\n",
            "[18/25][151/391] Loss_D1: 0.3024 Loss_D2: 0.4925 Loss_G: 4.6258\n",
            "[18/25][152/391] Loss_D1: 0.4564 Loss_D2: 0.5339 Loss_G: 5.5351\n",
            "[18/25][153/391] Loss_D1: 0.6371 Loss_D2: 0.7388 Loss_G: 5.3867\n",
            "[18/25][154/391] Loss_D1: 0.2416 Loss_D2: 0.8076 Loss_G: 7.1437\n",
            "[18/25][155/391] Loss_D1: 0.4285 Loss_D2: 0.3622 Loss_G: 4.6312\n",
            "[18/25][156/391] Loss_D1: 0.6379 Loss_D2: 0.4622 Loss_G: 5.4988\n",
            "[18/25][157/391] Loss_D1: 0.4658 Loss_D2: 0.7906 Loss_G: 5.9470\n",
            "[18/25][158/391] Loss_D1: 0.6030 Loss_D2: 0.6271 Loss_G: 5.0255\n",
            "[18/25][159/391] Loss_D1: 0.3065 Loss_D2: 0.8386 Loss_G: 7.4282\n",
            "[18/25][160/391] Loss_D1: 0.3650 Loss_D2: 0.8191 Loss_G: 3.3615\n",
            "[18/25][161/391] Loss_D1: 0.3825 Loss_D2: 0.6638 Loss_G: 6.4919\n",
            "[18/25][162/391] Loss_D1: 0.3490 Loss_D2: 0.5484 Loss_G: 5.6132\n",
            "[18/25][163/391] Loss_D1: 0.4209 Loss_D2: 0.3992 Loss_G: 4.5154\n",
            "[18/25][164/391] Loss_D1: 0.3534 Loss_D2: 0.5546 Loss_G: 6.2603\n",
            "[18/25][165/391] Loss_D1: 0.4846 Loss_D2: 0.8732 Loss_G: 2.8354\n",
            "[18/25][166/391] Loss_D1: 0.6106 Loss_D2: 0.7106 Loss_G: 7.8053\n",
            "[18/25][167/391] Loss_D1: 0.6024 Loss_D2: 0.6011 Loss_G: 4.4981\n",
            "[18/25][168/391] Loss_D1: 0.4388 Loss_D2: 0.3914 Loss_G: 5.7077\n",
            "[18/25][169/391] Loss_D1: 0.5759 Loss_D2: 0.5719 Loss_G: 3.3736\n",
            "[18/25][170/391] Loss_D1: 0.5854 Loss_D2: 0.6580 Loss_G: 8.3478\n",
            "[18/25][171/391] Loss_D1: 0.5181 Loss_D2: 0.7749 Loss_G: 3.5411\n",
            "[18/25][172/391] Loss_D1: 0.5266 Loss_D2: 0.6000 Loss_G: 6.8541\n",
            "[18/25][173/391] Loss_D1: 0.3889 Loss_D2: 0.5006 Loss_G: 5.2777\n",
            "[18/25][174/391] Loss_D1: 0.2891 Loss_D2: 0.4698 Loss_G: 4.9983\n",
            "[18/25][175/391] Loss_D1: 0.2992 Loss_D2: 0.5483 Loss_G: 6.1469\n",
            "[18/25][176/391] Loss_D1: 0.6590 Loss_D2: 0.4236 Loss_G: 4.0776\n",
            "[18/25][177/391] Loss_D1: 0.5702 Loss_D2: 0.4215 Loss_G: 5.9517\n",
            "[18/25][178/391] Loss_D1: 0.2690 Loss_D2: 0.4183 Loss_G: 7.2779\n",
            "[18/25][179/391] Loss_D1: 0.5040 Loss_D2: 0.5980 Loss_G: 3.3194\n",
            "[18/25][180/391] Loss_D1: 0.5889 Loss_D2: 0.6539 Loss_G: 7.8537\n",
            "[18/25][181/391] Loss_D1: 0.3575 Loss_D2: 0.6765 Loss_G: 4.8281\n",
            "[18/25][182/391] Loss_D1: 0.3768 Loss_D2: 0.5849 Loss_G: 5.0156\n",
            "[18/25][183/391] Loss_D1: 0.4247 Loss_D2: 0.4988 Loss_G: 5.7687\n",
            "[18/25][184/391] Loss_D1: 0.6822 Loss_D2: 0.4775 Loss_G: 3.1548\n",
            "[18/25][185/391] Loss_D1: 0.8330 Loss_D2: 0.7276 Loss_G: 8.3366\n",
            "[18/25][186/391] Loss_D1: 0.9260 Loss_D2: 0.5985 Loss_G: 2.6195\n",
            "[18/25][187/391] Loss_D1: 1.3290 Loss_D2: 0.3950 Loss_G: 8.7237\n",
            "[18/25][188/391] Loss_D1: 1.2345 Loss_D2: 0.5415 Loss_G: 4.1536\n",
            "[18/25][189/391] Loss_D1: 0.4311 Loss_D2: 0.3935 Loss_G: 5.9614\n",
            "[18/25][190/391] Loss_D1: 0.4551 Loss_D2: 0.3635 Loss_G: 5.6574\n",
            "[18/25][191/391] Loss_D1: 0.4009 Loss_D2: 0.5563 Loss_G: 3.7618\n",
            "[18/25][192/391] Loss_D1: 0.4405 Loss_D2: 0.3670 Loss_G: 5.9311\n",
            "[18/25][193/391] Loss_D1: 0.3914 Loss_D2: 0.6551 Loss_G: 6.5818\n",
            "[18/25][194/391] Loss_D1: 0.4466 Loss_D2: 0.5899 Loss_G: 4.0556\n",
            "[18/25][195/391] Loss_D1: 0.4865 Loss_D2: 0.5531 Loss_G: 4.8911\n",
            "[18/25][196/391] Loss_D1: 0.4948 Loss_D2: 0.7040 Loss_G: 5.9747\n",
            "[18/25][197/391] Loss_D1: 0.3988 Loss_D2: 0.5884 Loss_G: 4.1082\n",
            "[18/25][198/391] Loss_D1: 0.5800 Loss_D2: 0.6769 Loss_G: 6.8911\n",
            "[18/25][199/391] Loss_D1: 0.3799 Loss_D2: 0.3320 Loss_G: 5.3823\n",
            "[18/25][200/391] Loss_D1: 0.3557 Loss_D2: 0.5008 Loss_G: 5.5806\n",
            "saving the output\n",
            "[18/25][201/391] Loss_D1: 0.4062 Loss_D2: 0.4049 Loss_G: 4.5917\n",
            "[18/25][202/391] Loss_D1: 0.4949 Loss_D2: 0.4792 Loss_G: 5.8600\n",
            "[18/25][203/391] Loss_D1: 0.5172 Loss_D2: 0.4761 Loss_G: 4.9639\n",
            "[18/25][204/391] Loss_D1: 0.3833 Loss_D2: 0.4816 Loss_G: 6.1880\n",
            "[18/25][205/391] Loss_D1: 0.2888 Loss_D2: 0.4948 Loss_G: 5.1066\n",
            "[18/25][206/391] Loss_D1: 0.5478 Loss_D2: 0.4717 Loss_G: 4.5646\n",
            "[18/25][207/391] Loss_D1: 0.2925 Loss_D2: 0.2951 Loss_G: 5.8637\n",
            "[18/25][208/391] Loss_D1: 0.3910 Loss_D2: 0.4138 Loss_G: 4.4918\n",
            "[18/25][209/391] Loss_D1: 0.3525 Loss_D2: 0.3320 Loss_G: 6.4197\n",
            "[18/25][210/391] Loss_D1: 0.3635 Loss_D2: 0.3852 Loss_G: 5.7867\n",
            "[18/25][211/391] Loss_D1: 0.4261 Loss_D2: 0.3970 Loss_G: 4.8817\n",
            "[18/25][212/391] Loss_D1: 0.3302 Loss_D2: 0.5972 Loss_G: 4.0928\n",
            "[18/25][213/391] Loss_D1: 0.2863 Loss_D2: 0.8219 Loss_G: 7.4769\n",
            "[18/25][214/391] Loss_D1: 0.3296 Loss_D2: 0.8252 Loss_G: 3.3820\n",
            "[18/25][215/391] Loss_D1: 0.2924 Loss_D2: 1.4001 Loss_G: 8.9309\n",
            "[18/25][216/391] Loss_D1: 0.3908 Loss_D2: 1.7082 Loss_G: 2.2899\n",
            "[18/25][217/391] Loss_D1: 0.6167 Loss_D2: 2.6602 Loss_G: 10.9514\n",
            "[18/25][218/391] Loss_D1: 0.5703 Loss_D2: 2.0802 Loss_G: 2.2855\n",
            "[18/25][219/391] Loss_D1: 0.4945 Loss_D2: 3.5454 Loss_G: 9.3194\n",
            "[18/25][220/391] Loss_D1: 0.4807 Loss_D2: 3.1191 Loss_G: 2.6841\n",
            "[18/25][221/391] Loss_D1: 0.5314 Loss_D2: 1.7393 Loss_G: 10.2918\n",
            "[18/25][222/391] Loss_D1: 0.4789 Loss_D2: 2.3528 Loss_G: 2.7206\n",
            "[18/25][223/391] Loss_D1: 0.3718 Loss_D2: 2.1389 Loss_G: 7.4596\n",
            "[18/25][224/391] Loss_D1: 0.4015 Loss_D2: 1.1290 Loss_G: 4.1883\n",
            "[18/25][225/391] Loss_D1: 0.4847 Loss_D2: 0.9439 Loss_G: 5.4481\n",
            "[18/25][226/391] Loss_D1: 0.6191 Loss_D2: 0.7398 Loss_G: 3.6726\n",
            "[18/25][227/391] Loss_D1: 0.6177 Loss_D2: 0.9104 Loss_G: 5.6290\n",
            "[18/25][228/391] Loss_D1: 0.4625 Loss_D2: 0.7725 Loss_G: 6.2964\n",
            "[18/25][229/391] Loss_D1: 0.6865 Loss_D2: 1.0562 Loss_G: 3.8244\n",
            "[18/25][230/391] Loss_D1: 0.4275 Loss_D2: 1.1115 Loss_G: 6.4628\n",
            "[18/25][231/391] Loss_D1: 0.5962 Loss_D2: 0.8010 Loss_G: 6.0147\n",
            "[18/25][232/391] Loss_D1: 0.4307 Loss_D2: 0.4407 Loss_G: 4.4100\n",
            "[18/25][233/391] Loss_D1: 0.3459 Loss_D2: 0.5135 Loss_G: 6.8421\n",
            "[18/25][234/391] Loss_D1: 0.4099 Loss_D2: 0.6995 Loss_G: 4.4573\n",
            "[18/25][235/391] Loss_D1: 0.6211 Loss_D2: 0.5691 Loss_G: 5.7232\n",
            "[18/25][236/391] Loss_D1: 0.3649 Loss_D2: 0.5826 Loss_G: 5.0299\n",
            "[18/25][237/391] Loss_D1: 0.3515 Loss_D2: 0.4505 Loss_G: 5.6097\n",
            "[18/25][238/391] Loss_D1: 0.4356 Loss_D2: 0.5193 Loss_G: 3.9691\n",
            "[18/25][239/391] Loss_D1: 0.4617 Loss_D2: 0.7951 Loss_G: 7.3043\n",
            "[18/25][240/391] Loss_D1: 0.4218 Loss_D2: 0.6297 Loss_G: 4.1818\n",
            "[18/25][241/391] Loss_D1: 0.4267 Loss_D2: 0.4901 Loss_G: 5.8397\n",
            "[18/25][242/391] Loss_D1: 0.2614 Loss_D2: 0.5125 Loss_G: 6.0490\n",
            "[18/25][243/391] Loss_D1: 0.3798 Loss_D2: 0.5589 Loss_G: 4.7941\n",
            "[18/25][244/391] Loss_D1: 0.2956 Loss_D2: 0.4358 Loss_G: 5.9593\n",
            "[18/25][245/391] Loss_D1: 0.3856 Loss_D2: 0.7000 Loss_G: 5.5014\n",
            "[18/25][246/391] Loss_D1: 0.2785 Loss_D2: 0.5589 Loss_G: 4.3758\n",
            "[18/25][247/391] Loss_D1: 0.5160 Loss_D2: 0.6252 Loss_G: 6.7322\n",
            "[18/25][248/391] Loss_D1: 0.4545 Loss_D2: 0.4700 Loss_G: 4.2901\n",
            "[18/25][249/391] Loss_D1: 0.5207 Loss_D2: 0.4555 Loss_G: 5.5859\n",
            "[18/25][250/391] Loss_D1: 0.1828 Loss_D2: 0.5785 Loss_G: 8.5405\n",
            "[18/25][251/391] Loss_D1: 0.6256 Loss_D2: 1.3090 Loss_G: 1.7299\n",
            "[18/25][252/391] Loss_D1: 0.6624 Loss_D2: 0.9181 Loss_G: 8.3883\n",
            "[18/25][253/391] Loss_D1: 0.4346 Loss_D2: 1.0222 Loss_G: 2.8977\n",
            "[18/25][254/391] Loss_D1: 0.4913 Loss_D2: 1.5656 Loss_G: 5.2743\n",
            "[18/25][255/391] Loss_D1: 0.6867 Loss_D2: 0.6147 Loss_G: 6.1773\n",
            "[18/25][256/391] Loss_D1: 0.7370 Loss_D2: 1.0191 Loss_G: 6.2316\n",
            "[18/25][257/391] Loss_D1: 0.8401 Loss_D2: 1.7763 Loss_G: 4.5808\n",
            "[18/25][258/391] Loss_D1: 1.0685 Loss_D2: 1.9250 Loss_G: 6.3837\n",
            "[18/25][259/391] Loss_D1: 0.9132 Loss_D2: 1.5910 Loss_G: 5.2378\n",
            "[18/25][260/391] Loss_D1: 0.5414 Loss_D2: 1.1079 Loss_G: 6.1982\n",
            "[18/25][261/391] Loss_D1: 0.5188 Loss_D2: 1.0254 Loss_G: 5.0760\n",
            "[18/25][262/391] Loss_D1: 0.5461 Loss_D2: 0.4711 Loss_G: 4.0072\n",
            "[18/25][263/391] Loss_D1: 0.6555 Loss_D2: 0.6796 Loss_G: 8.2612\n",
            "[18/25][264/391] Loss_D1: 0.9654 Loss_D2: 1.3229 Loss_G: 1.4979\n",
            "[18/25][265/391] Loss_D1: 0.7433 Loss_D2: 1.8114 Loss_G: 8.1735\n",
            "[18/25][266/391] Loss_D1: 0.4798 Loss_D2: 1.0883 Loss_G: 4.9856\n",
            "[18/25][267/391] Loss_D1: 0.5154 Loss_D2: 0.5944 Loss_G: 4.4094\n",
            "[18/25][268/391] Loss_D1: 0.4974 Loss_D2: 0.6611 Loss_G: 4.8928\n",
            "[18/25][269/391] Loss_D1: 0.6484 Loss_D2: 0.7948 Loss_G: 5.7904\n",
            "[18/25][270/391] Loss_D1: 0.4666 Loss_D2: 0.7454 Loss_G: 4.8303\n",
            "[18/25][271/391] Loss_D1: 0.3492 Loss_D2: 0.5691 Loss_G: 6.1816\n",
            "[18/25][272/391] Loss_D1: 0.5232 Loss_D2: 0.6450 Loss_G: 4.7202\n",
            "[18/25][273/391] Loss_D1: 0.5398 Loss_D2: 0.6662 Loss_G: 4.4569\n",
            "[18/25][274/391] Loss_D1: 0.8870 Loss_D2: 0.5098 Loss_G: 7.6587\n",
            "[18/25][275/391] Loss_D1: 1.0616 Loss_D2: 0.6071 Loss_G: 3.4928\n",
            "[18/25][276/391] Loss_D1: 0.9787 Loss_D2: 0.5069 Loss_G: 7.4659\n",
            "[18/25][277/391] Loss_D1: 0.8799 Loss_D2: 0.5507 Loss_G: 2.7357\n",
            "[18/25][278/391] Loss_D1: 1.8829 Loss_D2: 0.4619 Loss_G: 10.3848\n",
            "[18/25][279/391] Loss_D1: 1.3169 Loss_D2: 0.3371 Loss_G: 3.2955\n",
            "[18/25][280/391] Loss_D1: 2.2201 Loss_D2: 0.7359 Loss_G: 9.2416\n",
            "[18/25][281/391] Loss_D1: 3.9242 Loss_D2: 1.1783 Loss_G: 5.5089\n",
            "[18/25][282/391] Loss_D1: 1.4229 Loss_D2: 1.1501 Loss_G: 5.7819\n",
            "[18/25][283/391] Loss_D1: 1.4546 Loss_D2: 1.4159 Loss_G: 4.2678\n",
            "[18/25][284/391] Loss_D1: 2.7304 Loss_D2: 0.7492 Loss_G: 7.8480\n",
            "[18/25][285/391] Loss_D1: 1.8165 Loss_D2: 0.4442 Loss_G: 2.7062\n",
            "[18/25][286/391] Loss_D1: 1.2005 Loss_D2: 0.8545 Loss_G: 6.9421\n",
            "[18/25][287/391] Loss_D1: 0.8034 Loss_D2: 0.3882 Loss_G: 6.9175\n",
            "[18/25][288/391] Loss_D1: 1.3374 Loss_D2: 0.4898 Loss_G: 2.3046\n",
            "[18/25][289/391] Loss_D1: 2.0956 Loss_D2: 0.5742 Loss_G: 8.6046\n",
            "[18/25][290/391] Loss_D1: 1.5358 Loss_D2: 0.8486 Loss_G: 2.5493\n",
            "[18/25][291/391] Loss_D1: 1.0880 Loss_D2: 0.9295 Loss_G: 8.5082\n",
            "[18/25][292/391] Loss_D1: 1.0979 Loss_D2: 0.6533 Loss_G: 4.0014\n",
            "[18/25][293/391] Loss_D1: 1.4150 Loss_D2: 0.4865 Loss_G: 6.1485\n",
            "[18/25][294/391] Loss_D1: 0.9741 Loss_D2: 0.6726 Loss_G: 5.4896\n",
            "[18/25][295/391] Loss_D1: 0.8570 Loss_D2: 0.8412 Loss_G: 4.7972\n",
            "[18/25][296/391] Loss_D1: 0.3769 Loss_D2: 0.6388 Loss_G: 6.0328\n",
            "[18/25][297/391] Loss_D1: 0.6539 Loss_D2: 0.5274 Loss_G: 6.2311\n",
            "[18/25][298/391] Loss_D1: 0.7793 Loss_D2: 0.7626 Loss_G: 3.6483\n",
            "[18/25][299/391] Loss_D1: 0.6825 Loss_D2: 0.4983 Loss_G: 4.7277\n",
            "[18/25][300/391] Loss_D1: 0.5529 Loss_D2: 0.5971 Loss_G: 6.5486\n",
            "saving the output\n",
            "[18/25][301/391] Loss_D1: 0.5207 Loss_D2: 0.6161 Loss_G: 4.4400\n",
            "[18/25][302/391] Loss_D1: 0.4972 Loss_D2: 0.5831 Loss_G: 6.1280\n",
            "[18/25][303/391] Loss_D1: 0.6023 Loss_D2: 0.8907 Loss_G: 4.7470\n",
            "[18/25][304/391] Loss_D1: 0.8908 Loss_D2: 0.6908 Loss_G: 4.2813\n",
            "[18/25][305/391] Loss_D1: 0.9098 Loss_D2: 0.4816 Loss_G: 6.7644\n",
            "[18/25][306/391] Loss_D1: 0.5602 Loss_D2: 0.5585 Loss_G: 4.7897\n",
            "[18/25][307/391] Loss_D1: 0.4821 Loss_D2: 0.5018 Loss_G: 5.0525\n",
            "[18/25][308/391] Loss_D1: 0.8049 Loss_D2: 0.4558 Loss_G: 6.6173\n",
            "[18/25][309/391] Loss_D1: 0.6132 Loss_D2: 0.4747 Loss_G: 4.5660\n",
            "[18/25][310/391] Loss_D1: 0.4111 Loss_D2: 0.6999 Loss_G: 4.3433\n",
            "[18/25][311/391] Loss_D1: 0.6234 Loss_D2: 0.3898 Loss_G: 7.9937\n",
            "[18/25][312/391] Loss_D1: 0.5012 Loss_D2: 0.4864 Loss_G: 4.4672\n",
            "[18/25][313/391] Loss_D1: 0.5629 Loss_D2: 0.4356 Loss_G: 3.4835\n",
            "[18/25][314/391] Loss_D1: 0.9239 Loss_D2: 0.5403 Loss_G: 7.8520\n",
            "[18/25][315/391] Loss_D1: 1.2312 Loss_D2: 0.4718 Loss_G: 3.0868\n",
            "[18/25][316/391] Loss_D1: 1.4241 Loss_D2: 0.5371 Loss_G: 7.3265\n",
            "[18/25][317/391] Loss_D1: 0.7240 Loss_D2: 0.5256 Loss_G: 4.8233\n",
            "[18/25][318/391] Loss_D1: 0.4922 Loss_D2: 0.8541 Loss_G: 3.7350\n",
            "[18/25][319/391] Loss_D1: 0.3711 Loss_D2: 0.9687 Loss_G: 7.2098\n",
            "[18/25][320/391] Loss_D1: 0.6598 Loss_D2: 0.7830 Loss_G: 2.9925\n",
            "[18/25][321/391] Loss_D1: 0.5780 Loss_D2: 0.8418 Loss_G: 5.6164\n",
            "[18/25][322/391] Loss_D1: 0.4826 Loss_D2: 0.5186 Loss_G: 4.7968\n",
            "[18/25][323/391] Loss_D1: 0.5828 Loss_D2: 0.4330 Loss_G: 6.2657\n",
            "[18/25][324/391] Loss_D1: 0.7568 Loss_D2: 0.5579 Loss_G: 4.2157\n",
            "[18/25][325/391] Loss_D1: 0.7434 Loss_D2: 0.4349 Loss_G: 3.9739\n",
            "[18/25][326/391] Loss_D1: 0.9251 Loss_D2: 0.3955 Loss_G: 7.4988\n",
            "[18/25][327/391] Loss_D1: 0.8089 Loss_D2: 0.6990 Loss_G: 3.6049\n",
            "[18/25][328/391] Loss_D1: 0.5185 Loss_D2: 1.3345 Loss_G: 7.7342\n",
            "[18/25][329/391] Loss_D1: 0.5147 Loss_D2: 1.0613 Loss_G: 5.2245\n",
            "[18/25][330/391] Loss_D1: 0.5235 Loss_D2: 0.6344 Loss_G: 4.1731\n",
            "[18/25][331/391] Loss_D1: 0.5974 Loss_D2: 0.7008 Loss_G: 5.8697\n",
            "[18/25][332/391] Loss_D1: 0.6022 Loss_D2: 0.9927 Loss_G: 3.6312\n",
            "[18/25][333/391] Loss_D1: 0.5221 Loss_D2: 0.8320 Loss_G: 7.3013\n",
            "[18/25][334/391] Loss_D1: 0.4969 Loss_D2: 0.5008 Loss_G: 5.3862\n",
            "[18/25][335/391] Loss_D1: 0.4334 Loss_D2: 0.6496 Loss_G: 3.7309\n",
            "[18/25][336/391] Loss_D1: 0.6759 Loss_D2: 0.8793 Loss_G: 4.7987\n",
            "[18/25][337/391] Loss_D1: 0.6847 Loss_D2: 0.5783 Loss_G: 6.0954\n",
            "[18/25][338/391] Loss_D1: 0.3122 Loss_D2: 0.3916 Loss_G: 5.9361\n",
            "[18/25][339/391] Loss_D1: 0.4544 Loss_D2: 0.4298 Loss_G: 4.9882\n",
            "[18/25][340/391] Loss_D1: 0.5394 Loss_D2: 0.4198 Loss_G: 5.2471\n",
            "[18/25][341/391] Loss_D1: 0.4668 Loss_D2: 0.5558 Loss_G: 4.2631\n",
            "[18/25][342/391] Loss_D1: 0.5890 Loss_D2: 0.5849 Loss_G: 7.0075\n",
            "[18/25][343/391] Loss_D1: 0.4533 Loss_D2: 0.4993 Loss_G: 5.2164\n",
            "[18/25][344/391] Loss_D1: 0.5291 Loss_D2: 0.3282 Loss_G: 3.5981\n",
            "[18/25][345/391] Loss_D1: 0.5940 Loss_D2: 0.4196 Loss_G: 6.9734\n",
            "[18/25][346/391] Loss_D1: 0.3246 Loss_D2: 0.5254 Loss_G: 5.6413\n",
            "[18/25][347/391] Loss_D1: 0.5819 Loss_D2: 0.5507 Loss_G: 3.0248\n",
            "[18/25][348/391] Loss_D1: 0.6491 Loss_D2: 0.7514 Loss_G: 6.9590\n",
            "[18/25][349/391] Loss_D1: 0.3739 Loss_D2: 0.6609 Loss_G: 4.1572\n",
            "[18/25][350/391] Loss_D1: 0.3640 Loss_D2: 0.3919 Loss_G: 5.9327\n",
            "[18/25][351/391] Loss_D1: 0.4334 Loss_D2: 0.6735 Loss_G: 4.3923\n",
            "[18/25][352/391] Loss_D1: 0.4786 Loss_D2: 0.3608 Loss_G: 5.8719\n",
            "[18/25][353/391] Loss_D1: 0.5814 Loss_D2: 0.3685 Loss_G: 4.7725\n",
            "[18/25][354/391] Loss_D1: 0.4543 Loss_D2: 0.5110 Loss_G: 4.9980\n",
            "[18/25][355/391] Loss_D1: 0.5176 Loss_D2: 0.5395 Loss_G: 5.6705\n",
            "[18/25][356/391] Loss_D1: 0.3611 Loss_D2: 0.5314 Loss_G: 5.5545\n",
            "[18/25][357/391] Loss_D1: 0.3361 Loss_D2: 0.4752 Loss_G: 5.4010\n",
            "[18/25][358/391] Loss_D1: 0.4582 Loss_D2: 0.6407 Loss_G: 3.3020\n",
            "[18/25][359/391] Loss_D1: 0.4352 Loss_D2: 0.7407 Loss_G: 7.1031\n",
            "[18/25][360/391] Loss_D1: 0.3360 Loss_D2: 0.7790 Loss_G: 4.8480\n",
            "[18/25][361/391] Loss_D1: 0.4042 Loss_D2: 0.5053 Loss_G: 4.2036\n",
            "[18/25][362/391] Loss_D1: 0.2973 Loss_D2: 0.3277 Loss_G: 6.3840\n",
            "[18/25][363/391] Loss_D1: 0.4107 Loss_D2: 0.4615 Loss_G: 5.2897\n",
            "[18/25][364/391] Loss_D1: 0.2907 Loss_D2: 0.5413 Loss_G: 5.3512\n",
            "[18/25][365/391] Loss_D1: 0.2718 Loss_D2: 0.5763 Loss_G: 4.8005\n",
            "[18/25][366/391] Loss_D1: 0.4670 Loss_D2: 0.4722 Loss_G: 5.5893\n",
            "[18/25][367/391] Loss_D1: 0.3357 Loss_D2: 0.4349 Loss_G: 4.7818\n",
            "[18/25][368/391] Loss_D1: 0.5254 Loss_D2: 0.4856 Loss_G: 4.9175\n",
            "[18/25][369/391] Loss_D1: 0.5153 Loss_D2: 0.3852 Loss_G: 5.9072\n",
            "[18/25][370/391] Loss_D1: 0.2434 Loss_D2: 0.3808 Loss_G: 5.9717\n",
            "[18/25][371/391] Loss_D1: 0.5965 Loss_D2: 0.3808 Loss_G: 4.7825\n",
            "[18/25][372/391] Loss_D1: 0.4074 Loss_D2: 0.5558 Loss_G: 5.4495\n",
            "[18/25][373/391] Loss_D1: 0.4076 Loss_D2: 0.3791 Loss_G: 4.9330\n",
            "[18/25][374/391] Loss_D1: 0.4504 Loss_D2: 0.5089 Loss_G: 4.9026\n",
            "[18/25][375/391] Loss_D1: 0.4857 Loss_D2: 0.6133 Loss_G: 3.7271\n",
            "[18/25][376/391] Loss_D1: 0.3774 Loss_D2: 0.6090 Loss_G: 6.1853\n",
            "[18/25][377/391] Loss_D1: 0.7037 Loss_D2: 0.4477 Loss_G: 4.0196\n",
            "[18/25][378/391] Loss_D1: 0.5959 Loss_D2: 0.4000 Loss_G: 6.0009\n",
            "[18/25][379/391] Loss_D1: 0.2426 Loss_D2: 0.4814 Loss_G: 5.9385\n",
            "[18/25][380/391] Loss_D1: 0.7136 Loss_D2: 0.6809 Loss_G: 2.5702\n",
            "[18/25][381/391] Loss_D1: 0.7218 Loss_D2: 0.5594 Loss_G: 7.6625\n",
            "[18/25][382/391] Loss_D1: 0.3106 Loss_D2: 0.5577 Loss_G: 4.8167\n",
            "[18/25][383/391] Loss_D1: 0.4072 Loss_D2: 0.4244 Loss_G: 3.9977\n",
            "[18/25][384/391] Loss_D1: 0.5253 Loss_D2: 0.4884 Loss_G: 6.1832\n",
            "[18/25][385/391] Loss_D1: 0.6148 Loss_D2: 0.5267 Loss_G: 4.6499\n",
            "[18/25][386/391] Loss_D1: 0.6449 Loss_D2: 0.5070 Loss_G: 5.3488\n",
            "[18/25][387/391] Loss_D1: 0.4001 Loss_D2: 0.4029 Loss_G: 4.6778\n",
            "[18/25][388/391] Loss_D1: 0.3783 Loss_D2: 0.4112 Loss_G: 4.9259\n",
            "[18/25][389/391] Loss_D1: 0.6295 Loss_D2: 0.3998 Loss_G: 4.1400\n",
            "[18/25][390/391] Loss_D1: 0.8138 Loss_D2: 0.7047 Loss_G: 7.0659\n",
            "[19/25][0/391] Loss_D1: 0.7334 Loss_D2: 0.5446 Loss_G: 4.3718\n",
            "saving the output\n",
            "[19/25][1/391] Loss_D1: 0.9868 Loss_D2: 0.4895 Loss_G: 6.8728\n",
            "[19/25][2/391] Loss_D1: 1.0747 Loss_D2: 0.5437 Loss_G: 2.7409\n",
            "[19/25][3/391] Loss_D1: 0.8630 Loss_D2: 0.7193 Loss_G: 7.3779\n",
            "[19/25][4/391] Loss_D1: 0.6694 Loss_D2: 0.4549 Loss_G: 4.3283\n",
            "[19/25][5/391] Loss_D1: 0.7521 Loss_D2: 0.3532 Loss_G: 6.4303\n",
            "[19/25][6/391] Loss_D1: 0.4397 Loss_D2: 0.3681 Loss_G: 5.6932\n",
            "[19/25][7/391] Loss_D1: 0.4662 Loss_D2: 0.3997 Loss_G: 5.1146\n",
            "[19/25][8/391] Loss_D1: 0.6013 Loss_D2: 0.5734 Loss_G: 4.3755\n",
            "[19/25][9/391] Loss_D1: 0.3049 Loss_D2: 0.4913 Loss_G: 6.5233\n",
            "[19/25][10/391] Loss_D1: 0.5019 Loss_D2: 0.5681 Loss_G: 4.0437\n",
            "[19/25][11/391] Loss_D1: 0.3869 Loss_D2: 0.4235 Loss_G: 6.8899\n",
            "[19/25][12/391] Loss_D1: 0.5636 Loss_D2: 0.4268 Loss_G: 4.5904\n",
            "[19/25][13/391] Loss_D1: 0.4154 Loss_D2: 0.4200 Loss_G: 6.1994\n",
            "[19/25][14/391] Loss_D1: 0.2276 Loss_D2: 0.3045 Loss_G: 6.4113\n",
            "[19/25][15/391] Loss_D1: 0.4720 Loss_D2: 0.4657 Loss_G: 4.5615\n",
            "[19/25][16/391] Loss_D1: 0.4086 Loss_D2: 0.5272 Loss_G: 6.3755\n",
            "[19/25][17/391] Loss_D1: 0.2232 Loss_D2: 0.5976 Loss_G: 5.3703\n",
            "[19/25][18/391] Loss_D1: 0.4502 Loss_D2: 0.5632 Loss_G: 5.1434\n",
            "[19/25][19/391] Loss_D1: 0.5686 Loss_D2: 0.3638 Loss_G: 6.1997\n",
            "[19/25][20/391] Loss_D1: 0.2726 Loss_D2: 0.5298 Loss_G: 4.9853\n",
            "[19/25][21/391] Loss_D1: 0.2782 Loss_D2: 0.3650 Loss_G: 5.1906\n",
            "[19/25][22/391] Loss_D1: 0.3615 Loss_D2: 0.4001 Loss_G: 5.4068\n",
            "[19/25][23/391] Loss_D1: 0.4157 Loss_D2: 0.3182 Loss_G: 6.5380\n",
            "[19/25][24/391] Loss_D1: 0.3615 Loss_D2: 0.4173 Loss_G: 4.9705\n",
            "[19/25][25/391] Loss_D1: 0.5218 Loss_D2: 0.5426 Loss_G: 4.7173\n",
            "[19/25][26/391] Loss_D1: 0.4587 Loss_D2: 0.5678 Loss_G: 5.3103\n",
            "[19/25][27/391] Loss_D1: 0.3653 Loss_D2: 0.3820 Loss_G: 5.3194\n",
            "[19/25][28/391] Loss_D1: 0.3216 Loss_D2: 0.4828 Loss_G: 5.6378\n",
            "[19/25][29/391] Loss_D1: 0.4361 Loss_D2: 0.4081 Loss_G: 6.2548\n",
            "[19/25][30/391] Loss_D1: 0.5905 Loss_D2: 0.4757 Loss_G: 3.4889\n",
            "[19/25][31/391] Loss_D1: 0.6440 Loss_D2: 0.3364 Loss_G: 7.6891\n",
            "[19/25][32/391] Loss_D1: 0.3332 Loss_D2: 0.3516 Loss_G: 5.2391\n",
            "[19/25][33/391] Loss_D1: 0.4662 Loss_D2: 0.4713 Loss_G: 3.9820\n",
            "[19/25][34/391] Loss_D1: 0.6990 Loss_D2: 0.4239 Loss_G: 7.4262\n",
            "[19/25][35/391] Loss_D1: 0.4215 Loss_D2: 0.3571 Loss_G: 4.8482\n",
            "[19/25][36/391] Loss_D1: 0.3254 Loss_D2: 0.3533 Loss_G: 4.7248\n",
            "[19/25][37/391] Loss_D1: 0.4216 Loss_D2: 0.4721 Loss_G: 5.8396\n",
            "[19/25][38/391] Loss_D1: 0.5144 Loss_D2: 0.5059 Loss_G: 4.5584\n",
            "[19/25][39/391] Loss_D1: 0.2807 Loss_D2: 0.3072 Loss_G: 5.7377\n",
            "[19/25][40/391] Loss_D1: 0.3040 Loss_D2: 0.3503 Loss_G: 5.0634\n",
            "[19/25][41/391] Loss_D1: 0.4996 Loss_D2: 0.5030 Loss_G: 5.1862\n",
            "[19/25][42/391] Loss_D1: 0.3670 Loss_D2: 0.4136 Loss_G: 5.9294\n",
            "[19/25][43/391] Loss_D1: 0.3684 Loss_D2: 0.5192 Loss_G: 4.2898\n",
            "[19/25][44/391] Loss_D1: 0.4503 Loss_D2: 0.4512 Loss_G: 4.1104\n",
            "[19/25][45/391] Loss_D1: 1.0636 Loss_D2: 0.4921 Loss_G: 7.6491\n",
            "[19/25][46/391] Loss_D1: 0.5274 Loss_D2: 0.3025 Loss_G: 4.5085\n",
            "[19/25][47/391] Loss_D1: 0.7085 Loss_D2: 0.3736 Loss_G: 5.9764\n",
            "[19/25][48/391] Loss_D1: 1.0952 Loss_D2: 0.4995 Loss_G: 3.7456\n",
            "[19/25][49/391] Loss_D1: 1.4741 Loss_D2: 0.4956 Loss_G: 7.6383\n",
            "[19/25][50/391] Loss_D1: 1.0725 Loss_D2: 0.4533 Loss_G: 3.5828\n",
            "[19/25][51/391] Loss_D1: 0.8339 Loss_D2: 0.3481 Loss_G: 7.3795\n",
            "[19/25][52/391] Loss_D1: 0.8634 Loss_D2: 0.3555 Loss_G: 3.8119\n",
            "[19/25][53/391] Loss_D1: 0.9863 Loss_D2: 0.4669 Loss_G: 6.6654\n",
            "[19/25][54/391] Loss_D1: 0.9933 Loss_D2: 0.3626 Loss_G: 4.2071\n",
            "[19/25][55/391] Loss_D1: 1.0191 Loss_D2: 0.5417 Loss_G: 6.0025\n",
            "[19/25][56/391] Loss_D1: 1.0343 Loss_D2: 0.6304 Loss_G: 4.6112\n",
            "[19/25][57/391] Loss_D1: 0.5464 Loss_D2: 0.6183 Loss_G: 4.7988\n",
            "[19/25][58/391] Loss_D1: 0.4055 Loss_D2: 0.3540 Loss_G: 6.2156\n",
            "[19/25][59/391] Loss_D1: 0.5205 Loss_D2: 0.3977 Loss_G: 4.7597\n",
            "[19/25][60/391] Loss_D1: 0.4557 Loss_D2: 0.4207 Loss_G: 4.8521\n",
            "[19/25][61/391] Loss_D1: 0.6006 Loss_D2: 0.3947 Loss_G: 5.6300\n",
            "[19/25][62/391] Loss_D1: 0.3610 Loss_D2: 0.4029 Loss_G: 5.9872\n",
            "[19/25][63/391] Loss_D1: 0.5823 Loss_D2: 0.3582 Loss_G: 4.0029\n",
            "[19/25][64/391] Loss_D1: 0.6037 Loss_D2: 0.2420 Loss_G: 6.3247\n",
            "[19/25][65/391] Loss_D1: 0.4227 Loss_D2: 0.3172 Loss_G: 5.6220\n",
            "[19/25][66/391] Loss_D1: 0.5443 Loss_D2: 0.4074 Loss_G: 4.4032\n",
            "[19/25][67/391] Loss_D1: 0.6314 Loss_D2: 0.3110 Loss_G: 5.8799\n",
            "[19/25][68/391] Loss_D1: 0.5685 Loss_D2: 0.3646 Loss_G: 4.9028\n",
            "[19/25][69/391] Loss_D1: 0.3273 Loss_D2: 0.3715 Loss_G: 6.9199\n",
            "[19/25][70/391] Loss_D1: 0.3517 Loss_D2: 0.3511 Loss_G: 5.4024\n",
            "[19/25][71/391] Loss_D1: 0.3727 Loss_D2: 0.3600 Loss_G: 4.3260\n",
            "[19/25][72/391] Loss_D1: 0.4130 Loss_D2: 0.5109 Loss_G: 6.6429\n",
            "[19/25][73/391] Loss_D1: 0.3216 Loss_D2: 0.4801 Loss_G: 5.3924\n",
            "[19/25][74/391] Loss_D1: 0.4635 Loss_D2: 0.3902 Loss_G: 4.0069\n",
            "[19/25][75/391] Loss_D1: 0.4004 Loss_D2: 0.4095 Loss_G: 5.5498\n",
            "[19/25][76/391] Loss_D1: 0.2589 Loss_D2: 0.3507 Loss_G: 6.0434\n",
            "[19/25][77/391] Loss_D1: 0.6257 Loss_D2: 0.3529 Loss_G: 4.5589\n",
            "[19/25][78/391] Loss_D1: 0.6448 Loss_D2: 0.3265 Loss_G: 5.7804\n",
            "[19/25][79/391] Loss_D1: 0.3910 Loss_D2: 0.3855 Loss_G: 5.3024\n",
            "[19/25][80/391] Loss_D1: 0.4801 Loss_D2: 0.4077 Loss_G: 4.6455\n",
            "[19/25][81/391] Loss_D1: 0.4387 Loss_D2: 0.4738 Loss_G: 5.4433\n",
            "[19/25][82/391] Loss_D1: 0.4196 Loss_D2: 0.3360 Loss_G: 5.6448\n",
            "[19/25][83/391] Loss_D1: 0.3561 Loss_D2: 0.3310 Loss_G: 5.7978\n",
            "[19/25][84/391] Loss_D1: 0.3146 Loss_D2: 0.3452 Loss_G: 4.7741\n",
            "[19/25][85/391] Loss_D1: 0.3914 Loss_D2: 0.4051 Loss_G: 6.0192\n",
            "[19/25][86/391] Loss_D1: 0.7782 Loss_D2: 0.4015 Loss_G: 4.2755\n",
            "[19/25][87/391] Loss_D1: 0.7077 Loss_D2: 0.4263 Loss_G: 5.2371\n",
            "[19/25][88/391] Loss_D1: 0.3292 Loss_D2: 0.4487 Loss_G: 6.1293\n",
            "[19/25][89/391] Loss_D1: 0.3456 Loss_D2: 0.3121 Loss_G: 5.0031\n",
            "[19/25][90/391] Loss_D1: 0.4220 Loss_D2: 0.3319 Loss_G: 5.7452\n",
            "[19/25][91/391] Loss_D1: 0.5655 Loss_D2: 0.4657 Loss_G: 4.3329\n",
            "[19/25][92/391] Loss_D1: 0.3317 Loss_D2: 0.4321 Loss_G: 5.9499\n",
            "[19/25][93/391] Loss_D1: 0.3106 Loss_D2: 0.4626 Loss_G: 4.9281\n",
            "[19/25][94/391] Loss_D1: 0.4540 Loss_D2: 0.4188 Loss_G: 4.5743\n",
            "[19/25][95/391] Loss_D1: 0.5241 Loss_D2: 0.6568 Loss_G: 5.3700\n",
            "[19/25][96/391] Loss_D1: 0.3833 Loss_D2: 0.4543 Loss_G: 4.8023\n",
            "[19/25][97/391] Loss_D1: 0.3000 Loss_D2: 0.4635 Loss_G: 6.1937\n",
            "[19/25][98/391] Loss_D1: 0.4603 Loss_D2: 0.4474 Loss_G: 3.5434\n",
            "[19/25][99/391] Loss_D1: 0.5072 Loss_D2: 0.5058 Loss_G: 6.1911\n",
            "[19/25][100/391] Loss_D1: 0.4311 Loss_D2: 0.4327 Loss_G: 4.9004\n",
            "saving the output\n",
            "[19/25][101/391] Loss_D1: 0.2619 Loss_D2: 0.3659 Loss_G: 5.6599\n",
            "[19/25][102/391] Loss_D1: 0.2509 Loss_D2: 0.3118 Loss_G: 5.5966\n",
            "[19/25][103/391] Loss_D1: 0.3012 Loss_D2: 0.4406 Loss_G: 5.5618\n",
            "[19/25][104/391] Loss_D1: 0.3844 Loss_D2: 0.5077 Loss_G: 4.6852\n",
            "[19/25][105/391] Loss_D1: 0.3588 Loss_D2: 0.4647 Loss_G: 5.6166\n",
            "[19/25][106/391] Loss_D1: 0.2728 Loss_D2: 0.4435 Loss_G: 5.3879\n",
            "[19/25][107/391] Loss_D1: 0.3902 Loss_D2: 0.4243 Loss_G: 5.9279\n",
            "[19/25][108/391] Loss_D1: 0.2764 Loss_D2: 0.2591 Loss_G: 5.8420\n",
            "[19/25][109/391] Loss_D1: 0.2855 Loss_D2: 0.4332 Loss_G: 4.4703\n",
            "[19/25][110/391] Loss_D1: 0.2586 Loss_D2: 0.5431 Loss_G: 7.5014\n",
            "[19/25][111/391] Loss_D1: 0.4499 Loss_D2: 0.6895 Loss_G: 2.6350\n",
            "[19/25][112/391] Loss_D1: 0.3897 Loss_D2: 1.0127 Loss_G: 8.3759\n",
            "[19/25][113/391] Loss_D1: 0.4287 Loss_D2: 1.2485 Loss_G: 2.8100\n",
            "[19/25][114/391] Loss_D1: 0.3175 Loss_D2: 4.6890 Loss_G: 10.6491\n",
            "[19/25][115/391] Loss_D1: 0.4719 Loss_D2: 2.9049 Loss_G: 1.8209\n",
            "[19/25][116/391] Loss_D1: 0.4356 Loss_D2: 2.7252 Loss_G: 9.9527\n",
            "[19/25][117/391] Loss_D1: 0.3142 Loss_D2: 2.7052 Loss_G: 3.1610\n",
            "[19/25][118/391] Loss_D1: 0.4294 Loss_D2: 2.6525 Loss_G: 7.5157\n",
            "[19/25][119/391] Loss_D1: 0.7247 Loss_D2: 2.1454 Loss_G: 4.7505\n",
            "[19/25][120/391] Loss_D1: 0.4545 Loss_D2: 1.0868 Loss_G: 5.0797\n",
            "[19/25][121/391] Loss_D1: 0.4959 Loss_D2: 0.6464 Loss_G: 6.9756\n",
            "[19/25][122/391] Loss_D1: 1.1806 Loss_D2: 0.7937 Loss_G: 2.2704\n",
            "[19/25][123/391] Loss_D1: 1.7210 Loss_D2: 0.7771 Loss_G: 8.9855\n",
            "[19/25][124/391] Loss_D1: 1.2431 Loss_D2: 0.5808 Loss_G: 2.5536\n",
            "[19/25][125/391] Loss_D1: 3.9307 Loss_D2: 0.7711 Loss_G: 12.3838\n",
            "[19/25][126/391] Loss_D1: 6.0223 Loss_D2: 0.7621 Loss_G: 2.6501\n",
            "[19/25][127/391] Loss_D1: 0.6335 Loss_D2: 1.1373 Loss_G: 11.1100\n",
            "[19/25][128/391] Loss_D1: 2.1904 Loss_D2: 1.8768 Loss_G: 0.2518\n",
            "[19/25][129/391] Loss_D1: 8.6886 Loss_D2: 2.0159 Loss_G: 7.2321\n",
            "[19/25][130/391] Loss_D1: 0.7375 Loss_D2: 0.9798 Loss_G: 7.7872\n",
            "[19/25][131/391] Loss_D1: 2.2849 Loss_D2: 0.7856 Loss_G: 1.2742\n",
            "[19/25][132/391] Loss_D1: 3.2083 Loss_D2: 0.8248 Loss_G: 10.1261\n",
            "[19/25][133/391] Loss_D1: 2.7603 Loss_D2: 0.7155 Loss_G: 2.4364\n",
            "[19/25][134/391] Loss_D1: 1.7316 Loss_D2: 0.6730 Loss_G: 6.6064\n",
            "[19/25][135/391] Loss_D1: 1.2589 Loss_D2: 0.8525 Loss_G: 3.4612\n",
            "[19/25][136/391] Loss_D1: 1.0468 Loss_D2: 0.6645 Loss_G: 4.8047\n",
            "[19/25][137/391] Loss_D1: 0.8194 Loss_D2: 0.4847 Loss_G: 5.6613\n",
            "[19/25][138/391] Loss_D1: 0.5523 Loss_D2: 0.5617 Loss_G: 4.4185\n",
            "[19/25][139/391] Loss_D1: 0.7672 Loss_D2: 0.8783 Loss_G: 5.7799\n",
            "[19/25][140/391] Loss_D1: 0.7764 Loss_D2: 0.9856 Loss_G: 5.1889\n",
            "[19/25][141/391] Loss_D1: 1.1524 Loss_D2: 0.8707 Loss_G: 4.1290\n",
            "[19/25][142/391] Loss_D1: 1.1603 Loss_D2: 0.8607 Loss_G: 5.6103\n",
            "[19/25][143/391] Loss_D1: 0.8293 Loss_D2: 0.7380 Loss_G: 5.1568\n",
            "[19/25][144/391] Loss_D1: 0.5924 Loss_D2: 0.6547 Loss_G: 3.9167\n",
            "[19/25][145/391] Loss_D1: 0.9463 Loss_D2: 0.5415 Loss_G: 5.4891\n",
            "[19/25][146/391] Loss_D1: 0.6920 Loss_D2: 0.4363 Loss_G: 5.1445\n",
            "[19/25][147/391] Loss_D1: 0.4804 Loss_D2: 0.5202 Loss_G: 5.5182\n",
            "[19/25][148/391] Loss_D1: 0.4890 Loss_D2: 0.5579 Loss_G: 5.6151\n",
            "[19/25][149/391] Loss_D1: 0.4064 Loss_D2: 0.2749 Loss_G: 5.5399\n",
            "[19/25][150/391] Loss_D1: 0.7683 Loss_D2: 0.3836 Loss_G: 5.2089\n",
            "[19/25][151/391] Loss_D1: 0.7078 Loss_D2: 0.5765 Loss_G: 3.4197\n",
            "[19/25][152/391] Loss_D1: 0.5855 Loss_D2: 0.5057 Loss_G: 6.5440\n",
            "[19/25][153/391] Loss_D1: 0.3333 Loss_D2: 0.3758 Loss_G: 6.5548\n",
            "[19/25][154/391] Loss_D1: 0.7848 Loss_D2: 0.6551 Loss_G: 3.1118\n",
            "[19/25][155/391] Loss_D1: 0.6370 Loss_D2: 0.5020 Loss_G: 6.3678\n",
            "[19/25][156/391] Loss_D1: 0.5058 Loss_D2: 0.5024 Loss_G: 5.0224\n",
            "[19/25][157/391] Loss_D1: 0.4623 Loss_D2: 0.4676 Loss_G: 4.7997\n",
            "[19/25][158/391] Loss_D1: 0.3603 Loss_D2: 0.6986 Loss_G: 5.3883\n",
            "[19/25][159/391] Loss_D1: 0.5083 Loss_D2: 0.4025 Loss_G: 5.1744\n",
            "[19/25][160/391] Loss_D1: 0.4705 Loss_D2: 0.6843 Loss_G: 3.9748\n",
            "[19/25][161/391] Loss_D1: 0.2282 Loss_D2: 0.6885 Loss_G: 7.0060\n",
            "[19/25][162/391] Loss_D1: 0.3076 Loss_D2: 0.5234 Loss_G: 5.6122\n",
            "[19/25][163/391] Loss_D1: 0.4557 Loss_D2: 0.4280 Loss_G: 4.7994\n",
            "[19/25][164/391] Loss_D1: 0.4492 Loss_D2: 0.4980 Loss_G: 4.7327\n",
            "[19/25][165/391] Loss_D1: 0.3711 Loss_D2: 0.4122 Loss_G: 5.5805\n",
            "[19/25][166/391] Loss_D1: 0.4722 Loss_D2: 0.5531 Loss_G: 4.3224\n",
            "[19/25][167/391] Loss_D1: 0.3197 Loss_D2: 0.5510 Loss_G: 6.4384\n",
            "[19/25][168/391] Loss_D1: 0.5856 Loss_D2: 0.7723 Loss_G: 3.1617\n",
            "[19/25][169/391] Loss_D1: 0.6820 Loss_D2: 0.6282 Loss_G: 5.9947\n",
            "[19/25][170/391] Loss_D1: 0.3878 Loss_D2: 0.6365 Loss_G: 5.5677\n",
            "[19/25][171/391] Loss_D1: 0.4821 Loss_D2: 0.4971 Loss_G: 3.5620\n",
            "[19/25][172/391] Loss_D1: 0.5380 Loss_D2: 0.5456 Loss_G: 5.8931\n",
            "[19/25][173/391] Loss_D1: 0.4139 Loss_D2: 0.5347 Loss_G: 4.4007\n",
            "[19/25][174/391] Loss_D1: 0.2369 Loss_D2: 0.3948 Loss_G: 6.0576\n",
            "[19/25][175/391] Loss_D1: 0.3962 Loss_D2: 0.4485 Loss_G: 4.9184\n",
            "[19/25][176/391] Loss_D1: 0.4931 Loss_D2: 0.5239 Loss_G: 5.1665\n",
            "[19/25][177/391] Loss_D1: 0.2992 Loss_D2: 0.5386 Loss_G: 4.7688\n",
            "[19/25][178/391] Loss_D1: 0.2350 Loss_D2: 0.4349 Loss_G: 5.7297\n",
            "[19/25][179/391] Loss_D1: 0.4032 Loss_D2: 0.4662 Loss_G: 4.9916\n",
            "[19/25][180/391] Loss_D1: 0.3650 Loss_D2: 0.5144 Loss_G: 4.4575\n",
            "[19/25][181/391] Loss_D1: 0.3864 Loss_D2: 0.4872 Loss_G: 6.3342\n",
            "[19/25][182/391] Loss_D1: 0.3645 Loss_D2: 0.4285 Loss_G: 4.9114\n",
            "[19/25][183/391] Loss_D1: 0.3717 Loss_D2: 0.4522 Loss_G: 4.0684\n",
            "[19/25][184/391] Loss_D1: 0.4215 Loss_D2: 0.3966 Loss_G: 6.6401\n",
            "[19/25][185/391] Loss_D1: 0.6221 Loss_D2: 0.6570 Loss_G: 3.3751\n",
            "[19/25][186/391] Loss_D1: 0.5266 Loss_D2: 0.5608 Loss_G: 5.5339\n",
            "[19/25][187/391] Loss_D1: 0.3029 Loss_D2: 0.4916 Loss_G: 6.7580\n",
            "[19/25][188/391] Loss_D1: 0.3066 Loss_D2: 0.6306 Loss_G: 4.3928\n",
            "[19/25][189/391] Loss_D1: 0.2724 Loss_D2: 0.3786 Loss_G: 5.6247\n",
            "[19/25][190/391] Loss_D1: 0.2717 Loss_D2: 0.4104 Loss_G: 5.7858\n",
            "[19/25][191/391] Loss_D1: 0.2944 Loss_D2: 0.4718 Loss_G: 5.0417\n",
            "[19/25][192/391] Loss_D1: 0.4720 Loss_D2: 0.3930 Loss_G: 4.4520\n",
            "[19/25][193/391] Loss_D1: 0.4618 Loss_D2: 0.4054 Loss_G: 6.7467\n",
            "[19/25][194/391] Loss_D1: 0.4493 Loss_D2: 0.5427 Loss_G: 3.6940\n",
            "[19/25][195/391] Loss_D1: 0.4024 Loss_D2: 0.5982 Loss_G: 5.7280\n",
            "[19/25][196/391] Loss_D1: 0.3783 Loss_D2: 0.5127 Loss_G: 4.2135\n",
            "[19/25][197/391] Loss_D1: 0.3003 Loss_D2: 0.5026 Loss_G: 6.0304\n",
            "[19/25][198/391] Loss_D1: 0.3013 Loss_D2: 0.6210 Loss_G: 4.8642\n",
            "[19/25][199/391] Loss_D1: 0.1721 Loss_D2: 0.5129 Loss_G: 6.6718\n",
            "[19/25][200/391] Loss_D1: 0.4274 Loss_D2: 0.7637 Loss_G: 3.6666\n",
            "saving the output\n",
            "[19/25][201/391] Loss_D1: 0.5101 Loss_D2: 0.6054 Loss_G: 4.9624\n",
            "[19/25][202/391] Loss_D1: 0.5173 Loss_D2: 0.4461 Loss_G: 6.2096\n",
            "[19/25][203/391] Loss_D1: 0.4719 Loss_D2: 0.5018 Loss_G: 4.1555\n",
            "[19/25][204/391] Loss_D1: 0.3112 Loss_D2: 0.5430 Loss_G: 5.3742\n",
            "[19/25][205/391] Loss_D1: 0.2919 Loss_D2: 0.4141 Loss_G: 5.8106\n",
            "[19/25][206/391] Loss_D1: 0.3966 Loss_D2: 0.4300 Loss_G: 4.3356\n",
            "[19/25][207/391] Loss_D1: 0.4298 Loss_D2: 0.4952 Loss_G: 4.8180\n",
            "[19/25][208/391] Loss_D1: 0.4953 Loss_D2: 0.5349 Loss_G: 5.6664\n",
            "[19/25][209/391] Loss_D1: 0.4299 Loss_D2: 0.4118 Loss_G: 5.8782\n",
            "[19/25][210/391] Loss_D1: 0.5027 Loss_D2: 0.4446 Loss_G: 3.9385\n",
            "[19/25][211/391] Loss_D1: 0.4636 Loss_D2: 0.3910 Loss_G: 5.6176\n",
            "[19/25][212/391] Loss_D1: 0.4102 Loss_D2: 0.5118 Loss_G: 5.2750\n",
            "[19/25][213/391] Loss_D1: 0.1969 Loss_D2: 0.4242 Loss_G: 6.1705\n",
            "[19/25][214/391] Loss_D1: 0.3845 Loss_D2: 0.5047 Loss_G: 5.3124\n",
            "[19/25][215/391] Loss_D1: 0.3006 Loss_D2: 0.3855 Loss_G: 5.4900\n",
            "[19/25][216/391] Loss_D1: 0.3053 Loss_D2: 0.3220 Loss_G: 5.1643\n",
            "[19/25][217/391] Loss_D1: 0.3694 Loss_D2: 0.4198 Loss_G: 5.1382\n",
            "[19/25][218/391] Loss_D1: 0.3700 Loss_D2: 0.4787 Loss_G: 6.3373\n",
            "[19/25][219/391] Loss_D1: 0.4578 Loss_D2: 0.4496 Loss_G: 4.3279\n",
            "[19/25][220/391] Loss_D1: 0.5449 Loss_D2: 0.5111 Loss_G: 4.8869\n",
            "[19/25][221/391] Loss_D1: 0.3223 Loss_D2: 0.5714 Loss_G: 6.4907\n",
            "[19/25][222/391] Loss_D1: 0.4989 Loss_D2: 0.5547 Loss_G: 3.1524\n",
            "[19/25][223/391] Loss_D1: 1.0113 Loss_D2: 0.5256 Loss_G: 6.1752\n",
            "[19/25][224/391] Loss_D1: 0.5107 Loss_D2: 0.4230 Loss_G: 4.8327\n",
            "[19/25][225/391] Loss_D1: 0.5596 Loss_D2: 0.5000 Loss_G: 4.7455\n",
            "[19/25][226/391] Loss_D1: 0.6996 Loss_D2: 0.5904 Loss_G: 3.8711\n",
            "[19/25][227/391] Loss_D1: 0.5491 Loss_D2: 0.3983 Loss_G: 5.4979\n",
            "[19/25][228/391] Loss_D1: 0.7187 Loss_D2: 0.5440 Loss_G: 4.7901\n",
            "[19/25][229/391] Loss_D1: 0.4792 Loss_D2: 0.6232 Loss_G: 4.9499\n",
            "[19/25][230/391] Loss_D1: 0.4881 Loss_D2: 0.6815 Loss_G: 5.7488\n",
            "[19/25][231/391] Loss_D1: 0.6606 Loss_D2: 0.5690 Loss_G: 4.8097\n",
            "[19/25][232/391] Loss_D1: 1.0761 Loss_D2: 0.6766 Loss_G: 4.5284\n",
            "[19/25][233/391] Loss_D1: 1.4676 Loss_D2: 0.6475 Loss_G: 6.4471\n",
            "[19/25][234/391] Loss_D1: 0.8451 Loss_D2: 0.4802 Loss_G: 3.1868\n",
            "[19/25][235/391] Loss_D1: 1.5714 Loss_D2: 0.4726 Loss_G: 9.0404\n",
            "[19/25][236/391] Loss_D1: 2.4379 Loss_D2: 0.6801 Loss_G: 1.6906\n",
            "[19/25][237/391] Loss_D1: 3.1354 Loss_D2: 0.6298 Loss_G: 10.0185\n",
            "[19/25][238/391] Loss_D1: 2.5316 Loss_D2: 0.6153 Loss_G: 2.8388\n",
            "[19/25][239/391] Loss_D1: 1.0223 Loss_D2: 0.6054 Loss_G: 6.8149\n",
            "[19/25][240/391] Loss_D1: 1.1205 Loss_D2: 0.5677 Loss_G: 2.6270\n",
            "[19/25][241/391] Loss_D1: 1.1751 Loss_D2: 0.5843 Loss_G: 8.4959\n",
            "[19/25][242/391] Loss_D1: 1.1948 Loss_D2: 0.4902 Loss_G: 3.0846\n",
            "[19/25][243/391] Loss_D1: 1.4148 Loss_D2: 0.4864 Loss_G: 7.9686\n",
            "[19/25][244/391] Loss_D1: 1.9394 Loss_D2: 0.4145 Loss_G: 2.7005\n",
            "[19/25][245/391] Loss_D1: 1.4594 Loss_D2: 0.5149 Loss_G: 7.8829\n",
            "[19/25][246/391] Loss_D1: 0.7138 Loss_D2: 0.4803 Loss_G: 4.5038\n",
            "[19/25][247/391] Loss_D1: 0.4841 Loss_D2: 0.5694 Loss_G: 4.9975\n",
            "[19/25][248/391] Loss_D1: 0.5374 Loss_D2: 0.4456 Loss_G: 5.6843\n",
            "[19/25][249/391] Loss_D1: 0.5815 Loss_D2: 0.4528 Loss_G: 4.2043\n",
            "[19/25][250/391] Loss_D1: 0.4914 Loss_D2: 0.6377 Loss_G: 6.0714\n",
            "[19/25][251/391] Loss_D1: 0.5171 Loss_D2: 0.4988 Loss_G: 4.8499\n",
            "[19/25][252/391] Loss_D1: 0.5995 Loss_D2: 0.4918 Loss_G: 4.9496\n",
            "[19/25][253/391] Loss_D1: 0.7373 Loss_D2: 0.4541 Loss_G: 4.5434\n",
            "[19/25][254/391] Loss_D1: 0.4837 Loss_D2: 0.3831 Loss_G: 5.3482\n",
            "[19/25][255/391] Loss_D1: 0.6592 Loss_D2: 0.5174 Loss_G: 4.5027\n",
            "[19/25][256/391] Loss_D1: 0.3426 Loss_D2: 0.5300 Loss_G: 6.0099\n",
            "[19/25][257/391] Loss_D1: 0.4371 Loss_D2: 0.4264 Loss_G: 4.8049\n",
            "[19/25][258/391] Loss_D1: 0.4776 Loss_D2: 0.3241 Loss_G: 5.5363\n",
            "[19/25][259/391] Loss_D1: 0.6963 Loss_D2: 0.6684 Loss_G: 3.5066\n",
            "[19/25][260/391] Loss_D1: 0.6596 Loss_D2: 0.3833 Loss_G: 6.3133\n",
            "[19/25][261/391] Loss_D1: 0.3191 Loss_D2: 0.4621 Loss_G: 5.7792\n",
            "[19/25][262/391] Loss_D1: 0.4112 Loss_D2: 0.4768 Loss_G: 3.6721\n",
            "[19/25][263/391] Loss_D1: 0.3860 Loss_D2: 0.7358 Loss_G: 7.1218\n",
            "[19/25][264/391] Loss_D1: 0.5298 Loss_D2: 0.6082 Loss_G: 3.9370\n",
            "[19/25][265/391] Loss_D1: 0.2775 Loss_D2: 0.4900 Loss_G: 6.6615\n",
            "[19/25][266/391] Loss_D1: 0.5818 Loss_D2: 0.3964 Loss_G: 4.7824\n",
            "[19/25][267/391] Loss_D1: 0.3507 Loss_D2: 0.3820 Loss_G: 5.1311\n",
            "[19/25][268/391] Loss_D1: 0.3959 Loss_D2: 0.4749 Loss_G: 5.6149\n",
            "[19/25][269/391] Loss_D1: 0.3433 Loss_D2: 0.4260 Loss_G: 4.8916\n",
            "[19/25][270/391] Loss_D1: 0.3843 Loss_D2: 0.4279 Loss_G: 4.9951\n",
            "[19/25][271/391] Loss_D1: 0.4113 Loss_D2: 0.4312 Loss_G: 5.8808\n",
            "[19/25][272/391] Loss_D1: 0.4141 Loss_D2: 0.4261 Loss_G: 5.3793\n",
            "[19/25][273/391] Loss_D1: 0.4919 Loss_D2: 0.5765 Loss_G: 3.6627\n",
            "[19/25][274/391] Loss_D1: 0.2868 Loss_D2: 0.5489 Loss_G: 7.0691\n",
            "[19/25][275/391] Loss_D1: 0.4127 Loss_D2: 0.5307 Loss_G: 4.1909\n",
            "[19/25][276/391] Loss_D1: 0.4381 Loss_D2: 0.6903 Loss_G: 5.2032\n",
            "[19/25][277/391] Loss_D1: 0.5360 Loss_D2: 0.4551 Loss_G: 5.4553\n",
            "[19/25][278/391] Loss_D1: 0.4998 Loss_D2: 0.4762 Loss_G: 4.0963\n",
            "[19/25][279/391] Loss_D1: 0.5245 Loss_D2: 0.3217 Loss_G: 6.5644\n",
            "[19/25][280/391] Loss_D1: 0.3450 Loss_D2: 0.3846 Loss_G: 5.6212\n",
            "[19/25][281/391] Loss_D1: 0.3849 Loss_D2: 0.4320 Loss_G: 5.2238\n",
            "[19/25][282/391] Loss_D1: 0.5872 Loss_D2: 0.5503 Loss_G: 5.0072\n",
            "[19/25][283/391] Loss_D1: 0.7079 Loss_D2: 0.5802 Loss_G: 5.0050\n",
            "[19/25][284/391] Loss_D1: 0.8654 Loss_D2: 0.2991 Loss_G: 7.0972\n",
            "[19/25][285/391] Loss_D1: 0.8894 Loss_D2: 0.4540 Loss_G: 2.7958\n",
            "[19/25][286/391] Loss_D1: 1.1969 Loss_D2: 0.4822 Loss_G: 8.5283\n",
            "[19/25][287/391] Loss_D1: 0.7577 Loss_D2: 0.4308 Loss_G: 4.1267\n",
            "[19/25][288/391] Loss_D1: 0.4811 Loss_D2: 0.5029 Loss_G: 5.0934\n",
            "[19/25][289/391] Loss_D1: 0.5035 Loss_D2: 0.3549 Loss_G: 7.2556\n",
            "[19/25][290/391] Loss_D1: 0.5923 Loss_D2: 0.5849 Loss_G: 3.1713\n",
            "[19/25][291/391] Loss_D1: 0.4366 Loss_D2: 0.8368 Loss_G: 7.1439\n",
            "[19/25][292/391] Loss_D1: 0.5274 Loss_D2: 0.5708 Loss_G: 4.9853\n",
            "[19/25][293/391] Loss_D1: 0.4565 Loss_D2: 0.6290 Loss_G: 6.9747\n",
            "[19/25][294/391] Loss_D1: 0.3144 Loss_D2: 0.5725 Loss_G: 3.4628\n",
            "[19/25][295/391] Loss_D1: 0.5558 Loss_D2: 0.8376 Loss_G: 6.6755\n",
            "[19/25][296/391] Loss_D1: 0.3113 Loss_D2: 0.9267 Loss_G: 4.2637\n",
            "[19/25][297/391] Loss_D1: 0.6784 Loss_D2: 1.1192 Loss_G: 6.1044\n",
            "[19/25][298/391] Loss_D1: 0.6428 Loss_D2: 0.9851 Loss_G: 4.9533\n",
            "[19/25][299/391] Loss_D1: 0.5592 Loss_D2: 0.7562 Loss_G: 6.0753\n",
            "[19/25][300/391] Loss_D1: 0.3481 Loss_D2: 0.5733 Loss_G: 4.7025\n",
            "saving the output\n",
            "[19/25][301/391] Loss_D1: 0.4508 Loss_D2: 0.3796 Loss_G: 5.2001\n",
            "[19/25][302/391] Loss_D1: 0.4139 Loss_D2: 0.6513 Loss_G: 6.6958\n",
            "[19/25][303/391] Loss_D1: 0.5242 Loss_D2: 0.9292 Loss_G: 2.8678\n",
            "[19/25][304/391] Loss_D1: 0.4978 Loss_D2: 0.9040 Loss_G: 6.9057\n",
            "[19/25][305/391] Loss_D1: 0.3868 Loss_D2: 0.8102 Loss_G: 4.2606\n",
            "[19/25][306/391] Loss_D1: 0.5147 Loss_D2: 0.5702 Loss_G: 6.7377\n",
            "[19/25][307/391] Loss_D1: 0.3754 Loss_D2: 0.5033 Loss_G: 4.7361\n",
            "[19/25][308/391] Loss_D1: 0.4080 Loss_D2: 0.4236 Loss_G: 5.5842\n",
            "[19/25][309/391] Loss_D1: 0.2944 Loss_D2: 0.5019 Loss_G: 5.7967\n",
            "[19/25][310/391] Loss_D1: 0.6781 Loss_D2: 0.5123 Loss_G: 3.2703\n",
            "[19/25][311/391] Loss_D1: 0.4939 Loss_D2: 0.5048 Loss_G: 7.0294\n",
            "[19/25][312/391] Loss_D1: 0.4854 Loss_D2: 0.3853 Loss_G: 5.0041\n",
            "[19/25][313/391] Loss_D1: 0.3962 Loss_D2: 0.4538 Loss_G: 4.9380\n",
            "[19/25][314/391] Loss_D1: 0.5919 Loss_D2: 0.6790 Loss_G: 5.5636\n",
            "[19/25][315/391] Loss_D1: 0.7137 Loss_D2: 0.4951 Loss_G: 6.1480\n",
            "[19/25][316/391] Loss_D1: 0.3898 Loss_D2: 0.5170 Loss_G: 4.7158\n",
            "[19/25][317/391] Loss_D1: 0.2171 Loss_D2: 0.4789 Loss_G: 6.4254\n",
            "[19/25][318/391] Loss_D1: 0.4847 Loss_D2: 0.4768 Loss_G: 6.2319\n",
            "[19/25][319/391] Loss_D1: 0.3406 Loss_D2: 0.4790 Loss_G: 4.8773\n",
            "[19/25][320/391] Loss_D1: 0.4978 Loss_D2: 0.5126 Loss_G: 4.4392\n",
            "[19/25][321/391] Loss_D1: 0.3664 Loss_D2: 0.4891 Loss_G: 6.3511\n",
            "[19/25][322/391] Loss_D1: 0.3575 Loss_D2: 0.5025 Loss_G: 5.4014\n",
            "[19/25][323/391] Loss_D1: 0.3956 Loss_D2: 0.3722 Loss_G: 4.8426\n",
            "[19/25][324/391] Loss_D1: 0.5961 Loss_D2: 0.3220 Loss_G: 6.3968\n",
            "[19/25][325/391] Loss_D1: 0.5376 Loss_D2: 0.4532 Loss_G: 4.8620\n",
            "[19/25][326/391] Loss_D1: 0.2877 Loss_D2: 0.4359 Loss_G: 6.1713\n",
            "[19/25][327/391] Loss_D1: 0.4512 Loss_D2: 0.4741 Loss_G: 4.4833\n",
            "[19/25][328/391] Loss_D1: 0.3761 Loss_D2: 0.5691 Loss_G: 4.7084\n",
            "[19/25][329/391] Loss_D1: 0.3576 Loss_D2: 0.6427 Loss_G: 9.6042\n",
            "[19/25][330/391] Loss_D1: 0.5943 Loss_D2: 1.1305 Loss_G: 2.7488\n",
            "[19/25][331/391] Loss_D1: 0.3588 Loss_D2: 1.5286 Loss_G: 8.2342\n",
            "[19/25][332/391] Loss_D1: 0.5292 Loss_D2: 1.2073 Loss_G: 3.5328\n",
            "[19/25][333/391] Loss_D1: 0.5232 Loss_D2: 0.7059 Loss_G: 7.7136\n",
            "[19/25][334/391] Loss_D1: 0.4293 Loss_D2: 0.6591 Loss_G: 4.6951\n",
            "[19/25][335/391] Loss_D1: 0.3937 Loss_D2: 0.7491 Loss_G: 6.1872\n",
            "[19/25][336/391] Loss_D1: 0.5426 Loss_D2: 0.7507 Loss_G: 4.1306\n",
            "[19/25][337/391] Loss_D1: 0.3748 Loss_D2: 0.4175 Loss_G: 6.3541\n",
            "[19/25][338/391] Loss_D1: 0.3786 Loss_D2: 0.3937 Loss_G: 6.4653\n",
            "[19/25][339/391] Loss_D1: 0.3789 Loss_D2: 0.5575 Loss_G: 3.9280\n",
            "[19/25][340/391] Loss_D1: 0.3487 Loss_D2: 0.7091 Loss_G: 6.3377\n",
            "[19/25][341/391] Loss_D1: 0.3317 Loss_D2: 0.5015 Loss_G: 5.1541\n",
            "[19/25][342/391] Loss_D1: 0.4480 Loss_D2: 0.6155 Loss_G: 3.4789\n",
            "[19/25][343/391] Loss_D1: 0.4954 Loss_D2: 0.5754 Loss_G: 7.4607\n",
            "[19/25][344/391] Loss_D1: 0.3702 Loss_D2: 0.5773 Loss_G: 4.5187\n",
            "[19/25][345/391] Loss_D1: 0.2585 Loss_D2: 0.4890 Loss_G: 6.6254\n",
            "[19/25][346/391] Loss_D1: 0.5188 Loss_D2: 0.4002 Loss_G: 4.8051\n",
            "[19/25][347/391] Loss_D1: 0.2879 Loss_D2: 0.5866 Loss_G: 5.0312\n",
            "[19/25][348/391] Loss_D1: 0.5454 Loss_D2: 0.4844 Loss_G: 3.8134\n",
            "[19/25][349/391] Loss_D1: 0.4156 Loss_D2: 0.4283 Loss_G: 6.4749\n",
            "[19/25][350/391] Loss_D1: 0.2753 Loss_D2: 0.5669 Loss_G: 4.6334\n",
            "[19/25][351/391] Loss_D1: 0.2889 Loss_D2: 0.5000 Loss_G: 6.2709\n",
            "[19/25][352/391] Loss_D1: 0.4093 Loss_D2: 0.5135 Loss_G: 4.3034\n",
            "[19/25][353/391] Loss_D1: 0.4790 Loss_D2: 0.4653 Loss_G: 5.5989\n",
            "[19/25][354/391] Loss_D1: 0.2376 Loss_D2: 0.4222 Loss_G: 6.1321\n",
            "[19/25][355/391] Loss_D1: 0.4241 Loss_D2: 0.4867 Loss_G: 4.4033\n",
            "[19/25][356/391] Loss_D1: 0.2646 Loss_D2: 0.3354 Loss_G: 5.6484\n",
            "[19/25][357/391] Loss_D1: 0.6110 Loss_D2: 0.4128 Loss_G: 4.7074\n",
            "[19/25][358/391] Loss_D1: 0.4128 Loss_D2: 0.3704 Loss_G: 5.1184\n",
            "[19/25][359/391] Loss_D1: 0.3773 Loss_D2: 0.4994 Loss_G: 6.1978\n",
            "[19/25][360/391] Loss_D1: 0.3171 Loss_D2: 0.4875 Loss_G: 4.3058\n",
            "[19/25][361/391] Loss_D1: 0.2761 Loss_D2: 0.7822 Loss_G: 7.7292\n",
            "[19/25][362/391] Loss_D1: 0.4390 Loss_D2: 1.0454 Loss_G: 2.8660\n",
            "[19/25][363/391] Loss_D1: 0.5282 Loss_D2: 0.8810 Loss_G: 7.6192\n",
            "[19/25][364/391] Loss_D1: 0.5755 Loss_D2: 0.5285 Loss_G: 4.0681\n",
            "[19/25][365/391] Loss_D1: 0.3353 Loss_D2: 0.4914 Loss_G: 4.6895\n",
            "[19/25][366/391] Loss_D1: 0.4472 Loss_D2: 0.5028 Loss_G: 6.4528\n",
            "[19/25][367/391] Loss_D1: 0.4267 Loss_D2: 0.5720 Loss_G: 4.6540\n",
            "[19/25][368/391] Loss_D1: 0.3698 Loss_D2: 0.3985 Loss_G: 4.5359\n",
            "[19/25][369/391] Loss_D1: 0.2612 Loss_D2: 0.3893 Loss_G: 6.3279\n",
            "[19/25][370/391] Loss_D1: 0.3112 Loss_D2: 0.6736 Loss_G: 4.7603\n",
            "[19/25][371/391] Loss_D1: 0.4716 Loss_D2: 0.6261 Loss_G: 5.0056\n",
            "[19/25][372/391] Loss_D1: 0.5996 Loss_D2: 0.2923 Loss_G: 7.2972\n",
            "[19/25][373/391] Loss_D1: 0.7218 Loss_D2: 0.4643 Loss_G: 3.8399\n",
            "[19/25][374/391] Loss_D1: 0.5683 Loss_D2: 0.6681 Loss_G: 4.0211\n",
            "[19/25][375/391] Loss_D1: 0.3351 Loss_D2: 1.3243 Loss_G: 8.1681\n",
            "[19/25][376/391] Loss_D1: 0.4099 Loss_D2: 1.1423 Loss_G: 2.5803\n",
            "[19/25][377/391] Loss_D1: 0.4315 Loss_D2: 4.8167 Loss_G: 9.7822\n",
            "[19/25][378/391] Loss_D1: 0.4393 Loss_D2: 3.6274 Loss_G: 4.2211\n",
            "[19/25][379/391] Loss_D1: 0.4281 Loss_D2: 1.4100 Loss_G: 6.7232\n",
            "[19/25][380/391] Loss_D1: 0.4655 Loss_D2: 1.0414 Loss_G: 4.1605\n",
            "[19/25][381/391] Loss_D1: 0.4389 Loss_D2: 0.7267 Loss_G: 5.1650\n",
            "[19/25][382/391] Loss_D1: 0.4958 Loss_D2: 0.7215 Loss_G: 4.4732\n",
            "[19/25][383/391] Loss_D1: 0.3784 Loss_D2: 0.8431 Loss_G: 5.5365\n",
            "[19/25][384/391] Loss_D1: 0.4931 Loss_D2: 0.8935 Loss_G: 4.9937\n",
            "[19/25][385/391] Loss_D1: 0.3593 Loss_D2: 1.1491 Loss_G: 3.2630\n",
            "[19/25][386/391] Loss_D1: 0.4429 Loss_D2: 2.7911 Loss_G: 9.3786\n",
            "[19/25][387/391] Loss_D1: 0.5271 Loss_D2: 2.5883 Loss_G: 3.0579\n",
            "[19/25][388/391] Loss_D1: 0.3010 Loss_D2: 2.2127 Loss_G: 10.2824\n",
            "[19/25][389/391] Loss_D1: 0.3151 Loss_D2: 2.3388 Loss_G: 2.6601\n",
            "[19/25][390/391] Loss_D1: 0.3555 Loss_D2: 1.9194 Loss_G: 11.0446\n",
            "[20/25][0/391] Loss_D1: 0.2329 Loss_D2: 2.6577 Loss_G: 3.5654\n",
            "saving the output\n",
            "[20/25][1/391] Loss_D1: 0.6115 Loss_D2: 2.8376 Loss_G: 6.2450\n",
            "[20/25][2/391] Loss_D1: 0.4240 Loss_D2: 1.6945 Loss_G: 4.0672\n",
            "[20/25][3/391] Loss_D1: 0.4752 Loss_D2: 1.0795 Loss_G: 5.8255\n",
            "[20/25][4/391] Loss_D1: 0.2274 Loss_D2: 0.6661 Loss_G: 7.0249\n",
            "[20/25][5/391] Loss_D1: 0.4923 Loss_D2: 0.9381 Loss_G: 3.4602\n",
            "[20/25][6/391] Loss_D1: 0.4162 Loss_D2: 0.9852 Loss_G: 6.3410\n",
            "[20/25][7/391] Loss_D1: 0.3908 Loss_D2: 0.5942 Loss_G: 5.9811\n",
            "[20/25][8/391] Loss_D1: 0.4875 Loss_D2: 0.8896 Loss_G: 3.2843\n",
            "[20/25][9/391] Loss_D1: 0.6866 Loss_D2: 0.8751 Loss_G: 7.5512\n",
            "[20/25][10/391] Loss_D1: 0.3092 Loss_D2: 0.6691 Loss_G: 6.0668\n",
            "[20/25][11/391] Loss_D1: 0.4271 Loss_D2: 0.6091 Loss_G: 3.7181\n",
            "[20/25][12/391] Loss_D1: 0.4200 Loss_D2: 0.6379 Loss_G: 5.7501\n",
            "[20/25][13/391] Loss_D1: 0.4256 Loss_D2: 0.5507 Loss_G: 6.2446\n",
            "[20/25][14/391] Loss_D1: 0.4581 Loss_D2: 0.6672 Loss_G: 3.4046\n",
            "[20/25][15/391] Loss_D1: 0.4449 Loss_D2: 0.7317 Loss_G: 7.8763\n",
            "[20/25][16/391] Loss_D1: 0.4264 Loss_D2: 0.5113 Loss_G: 5.1838\n",
            "[20/25][17/391] Loss_D1: 0.3365 Loss_D2: 0.5744 Loss_G: 4.2998\n",
            "[20/25][18/391] Loss_D1: 0.3473 Loss_D2: 0.6851 Loss_G: 5.9665\n",
            "[20/25][19/391] Loss_D1: 0.4533 Loss_D2: 0.4403 Loss_G: 5.3476\n",
            "[20/25][20/391] Loss_D1: 0.5111 Loss_D2: 0.5253 Loss_G: 4.7020\n",
            "[20/25][21/391] Loss_D1: 0.4086 Loss_D2: 0.4270 Loss_G: 5.5714\n",
            "[20/25][22/391] Loss_D1: 0.2369 Loss_D2: 0.4650 Loss_G: 6.8400\n",
            "[20/25][23/391] Loss_D1: 0.3861 Loss_D2: 0.4986 Loss_G: 4.1684\n",
            "[20/25][24/391] Loss_D1: 0.4451 Loss_D2: 0.5344 Loss_G: 6.3366\n",
            "[20/25][25/391] Loss_D1: 0.5423 Loss_D2: 0.4067 Loss_G: 4.4757\n",
            "[20/25][26/391] Loss_D1: 0.4114 Loss_D2: 0.5191 Loss_G: 5.5833\n",
            "[20/25][27/391] Loss_D1: 0.2621 Loss_D2: 0.5132 Loss_G: 5.4202\n",
            "[20/25][28/391] Loss_D1: 0.2282 Loss_D2: 0.5258 Loss_G: 5.3776\n",
            "[20/25][29/391] Loss_D1: 0.3151 Loss_D2: 0.6070 Loss_G: 4.4644\n",
            "[20/25][30/391] Loss_D1: 0.3925 Loss_D2: 0.5949 Loss_G: 5.4639\n",
            "[20/25][31/391] Loss_D1: 0.5593 Loss_D2: 0.5124 Loss_G: 5.4329\n",
            "[20/25][32/391] Loss_D1: 0.3534 Loss_D2: 0.3489 Loss_G: 4.2079\n",
            "[20/25][33/391] Loss_D1: 0.6605 Loss_D2: 0.6532 Loss_G: 5.9723\n",
            "[20/25][34/391] Loss_D1: 0.3990 Loss_D2: 0.4027 Loss_G: 5.8565\n",
            "[20/25][35/391] Loss_D1: 0.2860 Loss_D2: 0.5429 Loss_G: 4.5192\n",
            "[20/25][36/391] Loss_D1: 0.2951 Loss_D2: 0.4389 Loss_G: 5.2054\n",
            "[20/25][37/391] Loss_D1: 0.3300 Loss_D2: 0.4309 Loss_G: 6.6484\n",
            "[20/25][38/391] Loss_D1: 0.4182 Loss_D2: 0.4737 Loss_G: 4.2734\n",
            "[20/25][39/391] Loss_D1: 0.5187 Loss_D2: 0.3911 Loss_G: 6.1808\n",
            "[20/25][40/391] Loss_D1: 0.3625 Loss_D2: 0.4260 Loss_G: 5.9279\n",
            "[20/25][41/391] Loss_D1: 0.2655 Loss_D2: 0.4498 Loss_G: 4.7961\n",
            "[20/25][42/391] Loss_D1: 0.3069 Loss_D2: 0.4903 Loss_G: 5.8992\n",
            "[20/25][43/391] Loss_D1: 0.3714 Loss_D2: 0.3792 Loss_G: 4.8982\n",
            "[20/25][44/391] Loss_D1: 0.3349 Loss_D2: 0.4196 Loss_G: 5.7776\n",
            "[20/25][45/391] Loss_D1: 0.3124 Loss_D2: 0.4304 Loss_G: 4.7569\n",
            "[20/25][46/391] Loss_D1: 0.4443 Loss_D2: 0.4054 Loss_G: 4.5081\n",
            "[20/25][47/391] Loss_D1: 0.5224 Loss_D2: 0.4193 Loss_G: 6.2772\n",
            "[20/25][48/391] Loss_D1: 0.2962 Loss_D2: 0.4417 Loss_G: 5.0967\n",
            "[20/25][49/391] Loss_D1: 0.2898 Loss_D2: 0.3824 Loss_G: 5.2746\n",
            "[20/25][50/391] Loss_D1: 0.3530 Loss_D2: 0.4353 Loss_G: 5.9295\n",
            "[20/25][51/391] Loss_D1: 0.3173 Loss_D2: 0.4457 Loss_G: 4.9674\n",
            "[20/25][52/391] Loss_D1: 0.3465 Loss_D2: 0.3014 Loss_G: 5.2444\n",
            "[20/25][53/391] Loss_D1: 0.2605 Loss_D2: 0.4947 Loss_G: 5.3340\n",
            "[20/25][54/391] Loss_D1: 0.3504 Loss_D2: 0.4437 Loss_G: 4.6827\n",
            "[20/25][55/391] Loss_D1: 0.3003 Loss_D2: 0.4317 Loss_G: 5.7475\n",
            "[20/25][56/391] Loss_D1: 0.3082 Loss_D2: 0.3495 Loss_G: 5.2859\n",
            "[20/25][57/391] Loss_D1: 0.3162 Loss_D2: 0.3755 Loss_G: 5.1409\n",
            "[20/25][58/391] Loss_D1: 0.4120 Loss_D2: 0.6083 Loss_G: 3.4555\n",
            "[20/25][59/391] Loss_D1: 0.6388 Loss_D2: 0.6545 Loss_G: 8.1238\n",
            "[20/25][60/391] Loss_D1: 0.4487 Loss_D2: 0.5408 Loss_G: 4.2265\n",
            "[20/25][61/391] Loss_D1: 0.4186 Loss_D2: 0.4227 Loss_G: 5.0511\n",
            "[20/25][62/391] Loss_D1: 0.4397 Loss_D2: 0.4258 Loss_G: 6.0097\n",
            "[20/25][63/391] Loss_D1: 0.3259 Loss_D2: 0.6011 Loss_G: 4.1792\n",
            "[20/25][64/391] Loss_D1: 0.4184 Loss_D2: 0.6230 Loss_G: 5.8124\n",
            "[20/25][65/391] Loss_D1: 0.2435 Loss_D2: 0.4100 Loss_G: 5.5426\n",
            "[20/25][66/391] Loss_D1: 0.1924 Loss_D2: 0.2942 Loss_G: 6.2330\n",
            "[20/25][67/391] Loss_D1: 0.7177 Loss_D2: 0.3755 Loss_G: 3.3398\n",
            "[20/25][68/391] Loss_D1: 0.8520 Loss_D2: 0.3667 Loss_G: 6.8811\n",
            "[20/25][69/391] Loss_D1: 0.6706 Loss_D2: 0.4996 Loss_G: 1.9567\n",
            "[20/25][70/391] Loss_D1: 1.2595 Loss_D2: 0.5339 Loss_G: 11.2212\n",
            "[20/25][71/391] Loss_D1: 1.8968 Loss_D2: 0.3371 Loss_G: 4.7663\n",
            "[20/25][72/391] Loss_D1: 0.5342 Loss_D2: 0.4965 Loss_G: 4.1127\n",
            "[20/25][73/391] Loss_D1: 0.3744 Loss_D2: 0.7790 Loss_G: 7.4973\n",
            "[20/25][74/391] Loss_D1: 0.5813 Loss_D2: 0.7140 Loss_G: 3.8247\n",
            "[20/25][75/391] Loss_D1: 0.6371 Loss_D2: 0.4403 Loss_G: 6.3871\n",
            "[20/25][76/391] Loss_D1: 0.4846 Loss_D2: 0.6256 Loss_G: 4.4893\n",
            "[20/25][77/391] Loss_D1: 0.8163 Loss_D2: 0.5780 Loss_G: 8.8029\n",
            "[20/25][78/391] Loss_D1: 1.2942 Loss_D2: 0.6280 Loss_G: 2.1947\n",
            "[20/25][79/391] Loss_D1: 1.0855 Loss_D2: 0.8957 Loss_G: 8.7740\n",
            "[20/25][80/391] Loss_D1: 0.5292 Loss_D2: 0.7363 Loss_G: 4.5483\n",
            "[20/25][81/391] Loss_D1: 0.5694 Loss_D2: 0.5571 Loss_G: 5.0755\n",
            "[20/25][82/391] Loss_D1: 0.6470 Loss_D2: 0.3952 Loss_G: 6.3943\n",
            "[20/25][83/391] Loss_D1: 0.6245 Loss_D2: 0.4779 Loss_G: 3.5980\n",
            "[20/25][84/391] Loss_D1: 0.4753 Loss_D2: 0.5633 Loss_G: 6.7832\n",
            "[20/25][85/391] Loss_D1: 0.5141 Loss_D2: 0.5189 Loss_G: 4.0509\n",
            "[20/25][86/391] Loss_D1: 0.5605 Loss_D2: 0.4875 Loss_G: 6.1837\n",
            "[20/25][87/391] Loss_D1: 0.2622 Loss_D2: 0.4998 Loss_G: 6.4753\n",
            "[20/25][88/391] Loss_D1: 0.6368 Loss_D2: 0.3813 Loss_G: 4.3804\n",
            "[20/25][89/391] Loss_D1: 0.4990 Loss_D2: 0.4803 Loss_G: 4.7366\n",
            "[20/25][90/391] Loss_D1: 0.3301 Loss_D2: 0.3198 Loss_G: 6.5428\n",
            "[20/25][91/391] Loss_D1: 0.4488 Loss_D2: 0.3483 Loss_G: 5.6987\n",
            "[20/25][92/391] Loss_D1: 0.6186 Loss_D2: 0.5180 Loss_G: 3.3500\n",
            "[20/25][93/391] Loss_D1: 0.7601 Loss_D2: 0.4423 Loss_G: 7.1138\n",
            "[20/25][94/391] Loss_D1: 0.2219 Loss_D2: 0.4750 Loss_G: 6.3655\n",
            "[20/25][95/391] Loss_D1: 0.9021 Loss_D2: 0.3803 Loss_G: 3.0563\n",
            "[20/25][96/391] Loss_D1: 1.8054 Loss_D2: 0.3386 Loss_G: 8.1570\n",
            "[20/25][97/391] Loss_D1: 0.8749 Loss_D2: 0.4877 Loss_G: 2.4873\n",
            "[20/25][98/391] Loss_D1: 2.3752 Loss_D2: 0.3478 Loss_G: 10.9111\n",
            "[20/25][99/391] Loss_D1: 3.6559 Loss_D2: 0.4370 Loss_G: 3.0651\n",
            "[20/25][100/391] Loss_D1: 1.6016 Loss_D2: 0.3775 Loss_G: 9.0630\n",
            "saving the output\n",
            "[20/25][101/391] Loss_D1: 3.5372 Loss_D2: 0.5159 Loss_G: 2.2132\n",
            "[20/25][102/391] Loss_D1: 4.6840 Loss_D2: 0.3871 Loss_G: 7.0017\n",
            "[20/25][103/391] Loss_D1: 1.6548 Loss_D2: 0.4998 Loss_G: 3.7589\n",
            "[20/25][104/391] Loss_D1: 1.0827 Loss_D2: 0.4709 Loss_G: 4.8648\n",
            "[20/25][105/391] Loss_D1: 1.6457 Loss_D2: 0.4414 Loss_G: 6.3411\n",
            "[20/25][106/391] Loss_D1: 1.6184 Loss_D2: 0.4457 Loss_G: 3.5918\n",
            "[20/25][107/391] Loss_D1: 1.0595 Loss_D2: 0.5781 Loss_G: 6.5459\n",
            "[20/25][108/391] Loss_D1: 1.3345 Loss_D2: 0.6816 Loss_G: 4.1556\n",
            "[20/25][109/391] Loss_D1: 1.4040 Loss_D2: 0.4077 Loss_G: 7.3843\n",
            "[20/25][110/391] Loss_D1: 0.7625 Loss_D2: 0.4027 Loss_G: 4.3427\n",
            "[20/25][111/391] Loss_D1: 0.9653 Loss_D2: 0.6411 Loss_G: 4.6856\n",
            "[20/25][112/391] Loss_D1: 0.9128 Loss_D2: 0.5679 Loss_G: 5.6595\n",
            "[20/25][113/391] Loss_D1: 0.8917 Loss_D2: 0.8828 Loss_G: 5.7039\n",
            "[20/25][114/391] Loss_D1: 0.4429 Loss_D2: 0.7025 Loss_G: 3.6617\n",
            "[20/25][115/391] Loss_D1: 0.5201 Loss_D2: 0.6069 Loss_G: 6.6420\n",
            "[20/25][116/391] Loss_D1: 0.4312 Loss_D2: 0.7249 Loss_G: 3.7687\n",
            "[20/25][117/391] Loss_D1: 0.5115 Loss_D2: 0.7415 Loss_G: 6.3050\n",
            "[20/25][118/391] Loss_D1: 0.5068 Loss_D2: 0.4902 Loss_G: 5.7409\n",
            "[20/25][119/391] Loss_D1: 0.7746 Loss_D2: 0.6060 Loss_G: 2.7182\n",
            "[20/25][120/391] Loss_D1: 0.6104 Loss_D2: 0.8118 Loss_G: 8.1734\n",
            "[20/25][121/391] Loss_D1: 0.4154 Loss_D2: 0.5637 Loss_G: 5.1953\n",
            "[20/25][122/391] Loss_D1: 0.7602 Loss_D2: 0.4921 Loss_G: 3.8952\n",
            "[20/25][123/391] Loss_D1: 0.6397 Loss_D2: 0.4085 Loss_G: 7.1604\n",
            "[20/25][124/391] Loss_D1: 0.5851 Loss_D2: 0.7235 Loss_G: 3.2042\n",
            "[20/25][125/391] Loss_D1: 0.3981 Loss_D2: 0.7801 Loss_G: 8.0324\n",
            "[20/25][126/391] Loss_D1: 0.7626 Loss_D2: 0.6453 Loss_G: 2.7450\n",
            "[20/25][127/391] Loss_D1: 0.8566 Loss_D2: 0.7713 Loss_G: 9.2400\n",
            "[20/25][128/391] Loss_D1: 0.8749 Loss_D2: 0.9404 Loss_G: 1.7497\n",
            "[20/25][129/391] Loss_D1: 0.9401 Loss_D2: 1.0589 Loss_G: 9.4530\n",
            "[20/25][130/391] Loss_D1: 0.7828 Loss_D2: 0.7989 Loss_G: 4.5531\n",
            "[20/25][131/391] Loss_D1: 0.5975 Loss_D2: 0.5838 Loss_G: 4.7813\n",
            "[20/25][132/391] Loss_D1: 0.4113 Loss_D2: 0.6186 Loss_G: 7.1474\n",
            "[20/25][133/391] Loss_D1: 0.5352 Loss_D2: 0.8511 Loss_G: 2.9452\n",
            "[20/25][134/391] Loss_D1: 1.1026 Loss_D2: 0.9240 Loss_G: 7.4371\n",
            "[20/25][135/391] Loss_D1: 0.5549 Loss_D2: 0.6644 Loss_G: 4.7236\n",
            "[20/25][136/391] Loss_D1: 0.5643 Loss_D2: 0.6024 Loss_G: 4.7690\n",
            "[20/25][137/391] Loss_D1: 0.5508 Loss_D2: 0.4736 Loss_G: 5.6258\n",
            "[20/25][138/391] Loss_D1: 0.6095 Loss_D2: 0.5756 Loss_G: 4.4575\n",
            "[20/25][139/391] Loss_D1: 0.5718 Loss_D2: 0.5908 Loss_G: 5.7496\n",
            "[20/25][140/391] Loss_D1: 0.6751 Loss_D2: 0.4254 Loss_G: 4.9602\n",
            "[20/25][141/391] Loss_D1: 0.6748 Loss_D2: 0.4180 Loss_G: 6.7863\n",
            "[20/25][142/391] Loss_D1: 0.3852 Loss_D2: 0.2927 Loss_G: 5.6624\n",
            "[20/25][143/391] Loss_D1: 0.4453 Loss_D2: 0.3217 Loss_G: 5.2716\n",
            "[20/25][144/391] Loss_D1: 0.4539 Loss_D2: 0.4626 Loss_G: 6.4663\n",
            "[20/25][145/391] Loss_D1: 0.6501 Loss_D2: 0.5064 Loss_G: 3.9675\n",
            "[20/25][146/391] Loss_D1: 0.4263 Loss_D2: 0.4681 Loss_G: 5.6217\n",
            "[20/25][147/391] Loss_D1: 0.3188 Loss_D2: 0.5563 Loss_G: 5.4091\n",
            "[20/25][148/391] Loss_D1: 0.4025 Loss_D2: 0.4387 Loss_G: 5.1064\n",
            "[20/25][149/391] Loss_D1: 0.4772 Loss_D2: 0.5030 Loss_G: 5.0517\n",
            "[20/25][150/391] Loss_D1: 0.4257 Loss_D2: 0.4547 Loss_G: 5.9707\n",
            "[20/25][151/391] Loss_D1: 0.3882 Loss_D2: 0.5139 Loss_G: 4.3595\n",
            "[20/25][152/391] Loss_D1: 0.4136 Loss_D2: 0.5017 Loss_G: 6.0763\n",
            "[20/25][153/391] Loss_D1: 0.5422 Loss_D2: 0.5922 Loss_G: 3.9946\n",
            "[20/25][154/391] Loss_D1: 0.3618 Loss_D2: 0.4958 Loss_G: 5.2406\n",
            "[20/25][155/391] Loss_D1: 0.4381 Loss_D2: 0.3639 Loss_G: 6.5819\n",
            "[20/25][156/391] Loss_D1: 0.4014 Loss_D2: 0.6262 Loss_G: 4.1564\n",
            "[20/25][157/391] Loss_D1: 0.4875 Loss_D2: 0.8033 Loss_G: 5.9410\n",
            "[20/25][158/391] Loss_D1: 0.3655 Loss_D2: 0.4226 Loss_G: 5.4308\n",
            "[20/25][159/391] Loss_D1: 0.3520 Loss_D2: 0.4784 Loss_G: 4.9881\n",
            "[20/25][160/391] Loss_D1: 0.3551 Loss_D2: 0.4123 Loss_G: 5.8510\n",
            "[20/25][161/391] Loss_D1: 0.3390 Loss_D2: 0.4062 Loss_G: 6.0938\n",
            "[20/25][162/391] Loss_D1: 0.3404 Loss_D2: 0.4869 Loss_G: 4.1141\n",
            "[20/25][163/391] Loss_D1: 0.3375 Loss_D2: 0.5576 Loss_G: 6.3711\n",
            "[20/25][164/391] Loss_D1: 0.4195 Loss_D2: 0.5599 Loss_G: 3.9942\n",
            "[20/25][165/391] Loss_D1: 0.5057 Loss_D2: 0.6757 Loss_G: 7.7952\n",
            "[20/25][166/391] Loss_D1: 0.3422 Loss_D2: 0.5880 Loss_G: 5.3258\n",
            "[20/25][167/391] Loss_D1: 0.4455 Loss_D2: 0.5170 Loss_G: 4.5866\n",
            "[20/25][168/391] Loss_D1: 0.4174 Loss_D2: 0.3878 Loss_G: 5.9855\n",
            "[20/25][169/391] Loss_D1: 0.4955 Loss_D2: 0.6637 Loss_G: 3.7233\n",
            "[20/25][170/391] Loss_D1: 0.4425 Loss_D2: 0.6057 Loss_G: 6.2130\n",
            "[20/25][171/391] Loss_D1: 0.2777 Loss_D2: 0.3770 Loss_G: 6.2732\n",
            "[20/25][172/391] Loss_D1: 0.5292 Loss_D2: 0.4572 Loss_G: 3.7367\n",
            "[20/25][173/391] Loss_D1: 0.3862 Loss_D2: 0.4689 Loss_G: 5.7563\n",
            "[20/25][174/391] Loss_D1: 0.5478 Loss_D2: 0.4566 Loss_G: 3.9577\n",
            "[20/25][175/391] Loss_D1: 0.5523 Loss_D2: 0.4769 Loss_G: 6.2907\n",
            "[20/25][176/391] Loss_D1: 0.3860 Loss_D2: 0.4868 Loss_G: 5.9883\n",
            "[20/25][177/391] Loss_D1: 0.3670 Loss_D2: 0.5954 Loss_G: 3.8722\n",
            "[20/25][178/391] Loss_D1: 0.4655 Loss_D2: 0.4139 Loss_G: 5.9498\n",
            "[20/25][179/391] Loss_D1: 0.1993 Loss_D2: 0.5299 Loss_G: 5.4030\n",
            "[20/25][180/391] Loss_D1: 0.3435 Loss_D2: 0.4474 Loss_G: 5.6886\n",
            "[20/25][181/391] Loss_D1: 0.4078 Loss_D2: 0.4560 Loss_G: 4.0445\n",
            "[20/25][182/391] Loss_D1: 0.3616 Loss_D2: 0.6749 Loss_G: 6.3507\n",
            "[20/25][183/391] Loss_D1: 0.4138 Loss_D2: 0.4667 Loss_G: 4.6748\n",
            "[20/25][184/391] Loss_D1: 0.3460 Loss_D2: 0.5292 Loss_G: 5.5076\n",
            "[20/25][185/391] Loss_D1: 0.3844 Loss_D2: 0.5587 Loss_G: 5.4191\n",
            "[20/25][186/391] Loss_D1: 0.4060 Loss_D2: 0.4485 Loss_G: 4.9894\n",
            "[20/25][187/391] Loss_D1: 0.3720 Loss_D2: 0.4518 Loss_G: 4.4061\n",
            "[20/25][188/391] Loss_D1: 0.3897 Loss_D2: 0.4550 Loss_G: 5.2530\n",
            "[20/25][189/391] Loss_D1: 0.3054 Loss_D2: 0.4022 Loss_G: 5.7277\n",
            "[20/25][190/391] Loss_D1: 0.3598 Loss_D2: 0.3354 Loss_G: 5.7529\n",
            "[20/25][191/391] Loss_D1: 0.3951 Loss_D2: 0.4837 Loss_G: 4.4870\n",
            "[20/25][192/391] Loss_D1: 0.3583 Loss_D2: 0.4157 Loss_G: 5.2590\n",
            "[20/25][193/391] Loss_D1: 0.5605 Loss_D2: 0.4571 Loss_G: 6.5190\n",
            "[20/25][194/391] Loss_D1: 0.3732 Loss_D2: 0.7202 Loss_G: 3.6187\n",
            "[20/25][195/391] Loss_D1: 0.2614 Loss_D2: 0.5570 Loss_G: 5.8949\n",
            "[20/25][196/391] Loss_D1: 0.3172 Loss_D2: 0.2585 Loss_G: 6.1449\n",
            "[20/25][197/391] Loss_D1: 0.2811 Loss_D2: 0.5554 Loss_G: 4.4611\n",
            "[20/25][198/391] Loss_D1: 0.3975 Loss_D2: 0.5736 Loss_G: 5.6148\n",
            "[20/25][199/391] Loss_D1: 0.3256 Loss_D2: 0.5230 Loss_G: 4.4032\n",
            "[20/25][200/391] Loss_D1: 0.3237 Loss_D2: 0.5155 Loss_G: 6.5096\n",
            "saving the output\n",
            "[20/25][201/391] Loss_D1: 0.5965 Loss_D2: 0.3991 Loss_G: 4.9244\n",
            "[20/25][202/391] Loss_D1: 0.3363 Loss_D2: 0.4863 Loss_G: 3.6545\n",
            "[20/25][203/391] Loss_D1: 0.4041 Loss_D2: 0.6182 Loss_G: 7.3804\n",
            "[20/25][204/391] Loss_D1: 0.4587 Loss_D2: 0.5109 Loss_G: 4.5939\n",
            "[20/25][205/391] Loss_D1: 0.3181 Loss_D2: 0.4613 Loss_G: 5.0856\n",
            "[20/25][206/391] Loss_D1: 0.2520 Loss_D2: 0.4862 Loss_G: 6.0671\n",
            "[20/25][207/391] Loss_D1: 0.2386 Loss_D2: 0.4263 Loss_G: 5.3152\n",
            "[20/25][208/391] Loss_D1: 0.3271 Loss_D2: 0.4465 Loss_G: 3.7650\n",
            "[20/25][209/391] Loss_D1: 0.4852 Loss_D2: 0.5417 Loss_G: 7.8622\n",
            "[20/25][210/391] Loss_D1: 0.3748 Loss_D2: 0.6675 Loss_G: 3.9862\n",
            "[20/25][211/391] Loss_D1: 0.3386 Loss_D2: 0.4827 Loss_G: 5.2453\n",
            "[20/25][212/391] Loss_D1: 0.6946 Loss_D2: 0.5474 Loss_G: 5.5364\n",
            "[20/25][213/391] Loss_D1: 0.9736 Loss_D2: 0.6001 Loss_G: 3.5687\n",
            "[20/25][214/391] Loss_D1: 1.5442 Loss_D2: 0.5387 Loss_G: 7.9934\n",
            "[20/25][215/391] Loss_D1: 2.0011 Loss_D2: 0.4721 Loss_G: 3.3829\n",
            "[20/25][216/391] Loss_D1: 1.7842 Loss_D2: 0.4011 Loss_G: 8.6808\n",
            "[20/25][217/391] Loss_D1: 1.0161 Loss_D2: 0.3757 Loss_G: 3.2645\n",
            "[20/25][218/391] Loss_D1: 1.2111 Loss_D2: 0.5602 Loss_G: 6.3187\n",
            "[20/25][219/391] Loss_D1: 0.7091 Loss_D2: 0.7431 Loss_G: 6.4253\n",
            "[20/25][220/391] Loss_D1: 0.5697 Loss_D2: 0.9450 Loss_G: 3.9658\n",
            "[20/25][221/391] Loss_D1: 0.5221 Loss_D2: 0.7149 Loss_G: 6.4281\n",
            "[20/25][222/391] Loss_D1: 0.7891 Loss_D2: 0.6091 Loss_G: 5.7679\n",
            "[20/25][223/391] Loss_D1: 1.4434 Loss_D2: 0.6811 Loss_G: 3.6632\n",
            "[20/25][224/391] Loss_D1: 2.0238 Loss_D2: 0.6216 Loss_G: 7.3002\n",
            "[20/25][225/391] Loss_D1: 1.2892 Loss_D2: 0.6049 Loss_G: 2.5588\n",
            "[20/25][226/391] Loss_D1: 0.8850 Loss_D2: 0.7478 Loss_G: 8.0356\n",
            "[20/25][227/391] Loss_D1: 0.4623 Loss_D2: 0.5779 Loss_G: 4.7066\n",
            "[20/25][228/391] Loss_D1: 0.7976 Loss_D2: 0.6599 Loss_G: 5.5353\n",
            "[20/25][229/391] Loss_D1: 0.9626 Loss_D2: 0.8095 Loss_G: 5.4347\n",
            "[20/25][230/391] Loss_D1: 0.5699 Loss_D2: 0.6387 Loss_G: 5.9326\n",
            "[20/25][231/391] Loss_D1: 0.6789 Loss_D2: 0.5326 Loss_G: 4.3030\n",
            "[20/25][232/391] Loss_D1: 0.5667 Loss_D2: 0.5280 Loss_G: 5.7485\n",
            "[20/25][233/391] Loss_D1: 0.4998 Loss_D2: 0.4794 Loss_G: 5.3767\n",
            "[20/25][234/391] Loss_D1: 0.4480 Loss_D2: 0.5328 Loss_G: 4.8336\n",
            "[20/25][235/391] Loss_D1: 0.3875 Loss_D2: 0.4359 Loss_G: 5.9819\n",
            "[20/25][236/391] Loss_D1: 0.7894 Loss_D2: 0.6000 Loss_G: 4.9853\n",
            "[20/25][237/391] Loss_D1: 0.7210 Loss_D2: 0.4839 Loss_G: 5.2764\n",
            "[20/25][238/391] Loss_D1: 0.6228 Loss_D2: 0.5690 Loss_G: 5.1605\n",
            "[20/25][239/391] Loss_D1: 0.4904 Loss_D2: 0.3545 Loss_G: 6.9349\n",
            "[20/25][240/391] Loss_D1: 0.5694 Loss_D2: 0.2893 Loss_G: 4.9122\n",
            "[20/25][241/391] Loss_D1: 0.3699 Loss_D2: 0.3778 Loss_G: 5.5129\n",
            "[20/25][242/391] Loss_D1: 0.4642 Loss_D2: 0.4141 Loss_G: 5.4520\n",
            "[20/25][243/391] Loss_D1: 0.4626 Loss_D2: 0.4188 Loss_G: 5.1567\n",
            "[20/25][244/391] Loss_D1: 0.5992 Loss_D2: 0.5167 Loss_G: 5.7435\n",
            "[20/25][245/391] Loss_D1: 0.2785 Loss_D2: 0.3828 Loss_G: 6.2094\n",
            "[20/25][246/391] Loss_D1: 0.4489 Loss_D2: 0.3928 Loss_G: 4.2882\n",
            "[20/25][247/391] Loss_D1: 0.7577 Loss_D2: 0.3276 Loss_G: 6.6382\n",
            "[20/25][248/391] Loss_D1: 0.4987 Loss_D2: 0.5406 Loss_G: 7.1650\n",
            "[20/25][249/391] Loss_D1: 0.4468 Loss_D2: 0.4721 Loss_G: 3.5149\n",
            "[20/25][250/391] Loss_D1: 0.7569 Loss_D2: 0.4811 Loss_G: 6.5322\n",
            "[20/25][251/391] Loss_D1: 0.4087 Loss_D2: 0.4643 Loss_G: 6.0559\n",
            "[20/25][252/391] Loss_D1: 0.2906 Loss_D2: 0.3495 Loss_G: 5.1180\n",
            "[20/25][253/391] Loss_D1: 0.4622 Loss_D2: 0.5554 Loss_G: 5.9234\n",
            "[20/25][254/391] Loss_D1: 0.5434 Loss_D2: 0.4590 Loss_G: 3.7887\n",
            "[20/25][255/391] Loss_D1: 0.5093 Loss_D2: 0.3407 Loss_G: 6.0731\n",
            "[20/25][256/391] Loss_D1: 0.3794 Loss_D2: 0.5045 Loss_G: 6.2835\n",
            "[20/25][257/391] Loss_D1: 0.5241 Loss_D2: 0.5976 Loss_G: 3.0134\n",
            "[20/25][258/391] Loss_D1: 0.4257 Loss_D2: 0.4624 Loss_G: 6.7307\n",
            "[20/25][259/391] Loss_D1: 0.4527 Loss_D2: 0.3455 Loss_G: 5.5637\n",
            "[20/25][260/391] Loss_D1: 0.4977 Loss_D2: 0.4026 Loss_G: 4.1550\n",
            "[20/25][261/391] Loss_D1: 0.3035 Loss_D2: 0.6888 Loss_G: 7.2912\n",
            "[20/25][262/391] Loss_D1: 0.3245 Loss_D2: 0.7854 Loss_G: 3.4144\n",
            "[20/25][263/391] Loss_D1: 0.2890 Loss_D2: 0.7901 Loss_G: 8.2322\n",
            "[20/25][264/391] Loss_D1: 0.4217 Loss_D2: 1.0720 Loss_G: 3.7266\n",
            "[20/25][265/391] Loss_D1: 0.3720 Loss_D2: 0.9385 Loss_G: 6.0951\n",
            "[20/25][266/391] Loss_D1: 0.6319 Loss_D2: 0.6651 Loss_G: 5.1264\n",
            "[20/25][267/391] Loss_D1: 0.4697 Loss_D2: 0.4688 Loss_G: 5.4464\n",
            "[20/25][268/391] Loss_D1: 0.3392 Loss_D2: 0.5191 Loss_G: 4.7609\n",
            "[20/25][269/391] Loss_D1: 0.5670 Loss_D2: 0.5665 Loss_G: 5.7779\n",
            "[20/25][270/391] Loss_D1: 0.3520 Loss_D2: 0.4784 Loss_G: 5.4619\n",
            "[20/25][271/391] Loss_D1: 0.4547 Loss_D2: 0.4542 Loss_G: 4.4170\n",
            "[20/25][272/391] Loss_D1: 0.4761 Loss_D2: 0.4218 Loss_G: 5.0398\n",
            "[20/25][273/391] Loss_D1: 0.4494 Loss_D2: 0.5827 Loss_G: 5.8855\n",
            "[20/25][274/391] Loss_D1: 0.2802 Loss_D2: 0.4002 Loss_G: 5.5465\n",
            "[20/25][275/391] Loss_D1: 0.6507 Loss_D2: 0.4349 Loss_G: 4.2002\n",
            "[20/25][276/391] Loss_D1: 0.5135 Loss_D2: 0.5621 Loss_G: 5.2421\n",
            "[20/25][277/391] Loss_D1: 0.3365 Loss_D2: 0.3778 Loss_G: 6.5647\n",
            "[20/25][278/391] Loss_D1: 0.3591 Loss_D2: 0.6695 Loss_G: 6.0639\n",
            "[20/25][279/391] Loss_D1: 0.5451 Loss_D2: 0.5364 Loss_G: 2.8280\n",
            "[20/25][280/391] Loss_D1: 0.2880 Loss_D2: 0.7029 Loss_G: 6.9195\n",
            "[20/25][281/391] Loss_D1: 0.5449 Loss_D2: 0.8790 Loss_G: 4.3874\n",
            "[20/25][282/391] Loss_D1: 0.5558 Loss_D2: 0.6339 Loss_G: 5.5907\n",
            "[20/25][283/391] Loss_D1: 0.4159 Loss_D2: 0.3877 Loss_G: 5.6153\n",
            "[20/25][284/391] Loss_D1: 0.2927 Loss_D2: 0.4808 Loss_G: 5.3855\n",
            "[20/25][285/391] Loss_D1: 0.3900 Loss_D2: 0.4994 Loss_G: 5.8555\n",
            "[20/25][286/391] Loss_D1: 0.4718 Loss_D2: 0.4582 Loss_G: 4.5775\n",
            "[20/25][287/391] Loss_D1: 0.2200 Loss_D2: 0.5218 Loss_G: 6.4289\n",
            "[20/25][288/391] Loss_D1: 0.3645 Loss_D2: 0.4298 Loss_G: 5.0556\n",
            "[20/25][289/391] Loss_D1: 0.3291 Loss_D2: 0.4580 Loss_G: 4.6567\n",
            "[20/25][290/391] Loss_D1: 0.3441 Loss_D2: 0.3108 Loss_G: 5.6701\n",
            "[20/25][291/391] Loss_D1: 0.4839 Loss_D2: 0.4397 Loss_G: 5.2227\n",
            "[20/25][292/391] Loss_D1: 0.3898 Loss_D2: 0.4002 Loss_G: 5.3981\n",
            "[20/25][293/391] Loss_D1: 0.4333 Loss_D2: 0.3750 Loss_G: 4.7877\n",
            "[20/25][294/391] Loss_D1: 0.4738 Loss_D2: 0.4435 Loss_G: 5.1325\n",
            "[20/25][295/391] Loss_D1: 0.2822 Loss_D2: 0.4807 Loss_G: 5.8285\n",
            "[20/25][296/391] Loss_D1: 0.3917 Loss_D2: 0.3325 Loss_G: 5.5746\n",
            "[20/25][297/391] Loss_D1: 0.3125 Loss_D2: 0.4840 Loss_G: 5.5134\n",
            "[20/25][298/391] Loss_D1: 0.8188 Loss_D2: 0.6109 Loss_G: 4.7221\n",
            "[20/25][299/391] Loss_D1: 1.3157 Loss_D2: 0.6951 Loss_G: 5.4615\n",
            "[20/25][300/391] Loss_D1: 1.2501 Loss_D2: 0.4977 Loss_G: 3.5660\n",
            "saving the output\n",
            "[20/25][301/391] Loss_D1: 3.2635 Loss_D2: 0.5008 Loss_G: 9.7108\n",
            "[20/25][302/391] Loss_D1: 2.8027 Loss_D2: 0.3806 Loss_G: 3.5233\n",
            "[20/25][303/391] Loss_D1: 1.6505 Loss_D2: 0.6577 Loss_G: 6.1873\n",
            "[20/25][304/391] Loss_D1: 1.3958 Loss_D2: 0.6113 Loss_G: 3.7965\n",
            "[20/25][305/391] Loss_D1: 1.3974 Loss_D2: 0.3733 Loss_G: 7.6601\n",
            "[20/25][306/391] Loss_D1: 0.8609 Loss_D2: 0.4305 Loss_G: 4.5121\n",
            "[20/25][307/391] Loss_D1: 0.5549 Loss_D2: 0.6662 Loss_G: 3.4251\n",
            "[20/25][308/391] Loss_D1: 0.7362 Loss_D2: 0.6230 Loss_G: 7.4278\n",
            "[20/25][309/391] Loss_D1: 0.5606 Loss_D2: 0.4283 Loss_G: 5.7814\n",
            "[20/25][310/391] Loss_D1: 0.6845 Loss_D2: 0.3532 Loss_G: 3.6752\n",
            "[20/25][311/391] Loss_D1: 0.6421 Loss_D2: 0.4718 Loss_G: 6.2663\n",
            "[20/25][312/391] Loss_D1: 0.3241 Loss_D2: 0.3969 Loss_G: 6.0251\n",
            "[20/25][313/391] Loss_D1: 0.2767 Loss_D2: 0.3345 Loss_G: 6.3897\n",
            "[20/25][314/391] Loss_D1: 0.4603 Loss_D2: 0.5015 Loss_G: 4.2105\n",
            "[20/25][315/391] Loss_D1: 0.6903 Loss_D2: 0.4976 Loss_G: 4.4595\n",
            "[20/25][316/391] Loss_D1: 0.3922 Loss_D2: 0.4596 Loss_G: 6.7076\n",
            "[20/25][317/391] Loss_D1: 0.5141 Loss_D2: 0.4381 Loss_G: 4.9038\n",
            "[20/25][318/391] Loss_D1: 0.6260 Loss_D2: 0.3527 Loss_G: 3.7726\n",
            "[20/25][319/391] Loss_D1: 0.5734 Loss_D2: 0.4783 Loss_G: 6.6469\n",
            "[20/25][320/391] Loss_D1: 0.5367 Loss_D2: 0.7379 Loss_G: 3.9267\n",
            "[20/25][321/391] Loss_D1: 0.2718 Loss_D2: 0.8300 Loss_G: 7.9057\n",
            "[20/25][322/391] Loss_D1: 0.4329 Loss_D2: 0.4319 Loss_G: 4.8988\n",
            "[20/25][323/391] Loss_D1: 0.4525 Loss_D2: 0.4433 Loss_G: 5.6487\n",
            "[20/25][324/391] Loss_D1: 0.4316 Loss_D2: 0.5411 Loss_G: 5.0199\n",
            "[20/25][325/391] Loss_D1: 0.3032 Loss_D2: 0.5143 Loss_G: 4.4352\n",
            "[20/25][326/391] Loss_D1: 0.5277 Loss_D2: 0.4562 Loss_G: 6.9055\n",
            "[20/25][327/391] Loss_D1: 0.6782 Loss_D2: 0.4393 Loss_G: 4.4909\n",
            "[20/25][328/391] Loss_D1: 0.4761 Loss_D2: 0.5746 Loss_G: 4.7629\n",
            "[20/25][329/391] Loss_D1: 0.3881 Loss_D2: 0.5851 Loss_G: 6.5206\n",
            "[20/25][330/391] Loss_D1: 0.3940 Loss_D2: 0.4901 Loss_G: 3.9746\n",
            "[20/25][331/391] Loss_D1: 0.4633 Loss_D2: 0.5900 Loss_G: 7.7593\n",
            "[20/25][332/391] Loss_D1: 0.2807 Loss_D2: 1.2793 Loss_G: 3.4841\n",
            "[20/25][333/391] Loss_D1: 0.5840 Loss_D2: 1.8148 Loss_G: 6.4859\n",
            "[20/25][334/391] Loss_D1: 0.5061 Loss_D2: 0.5403 Loss_G: 4.1971\n",
            "[20/25][335/391] Loss_D1: 0.2796 Loss_D2: 1.4920 Loss_G: 12.4615\n",
            "[20/25][336/391] Loss_D1: 0.8246 Loss_D2: 3.4268 Loss_G: 1.6604\n",
            "[20/25][337/391] Loss_D1: 0.9171 Loss_D2: 2.1715 Loss_G: 10.3141\n",
            "[20/25][338/391] Loss_D1: 0.4899 Loss_D2: 2.5307 Loss_G: 2.6866\n",
            "[20/25][339/391] Loss_D1: 0.7300 Loss_D2: 2.2557 Loss_G: 6.5154\n",
            "[20/25][340/391] Loss_D1: 0.5059 Loss_D2: 1.5134 Loss_G: 4.3431\n",
            "[20/25][341/391] Loss_D1: 0.5201 Loss_D2: 1.4353 Loss_G: 6.5474\n",
            "[20/25][342/391] Loss_D1: 0.4684 Loss_D2: 1.6032 Loss_G: 2.6787\n",
            "[20/25][343/391] Loss_D1: 0.5725 Loss_D2: 1.6882 Loss_G: 8.5183\n",
            "[20/25][344/391] Loss_D1: 0.4195 Loss_D2: 1.1088 Loss_G: 4.5335\n",
            "[20/25][345/391] Loss_D1: 0.7272 Loss_D2: 0.8585 Loss_G: 2.8056\n",
            "[20/25][346/391] Loss_D1: 0.8862 Loss_D2: 0.7537 Loss_G: 8.2350\n",
            "[20/25][347/391] Loss_D1: 0.7107 Loss_D2: 1.1673 Loss_G: 2.8606\n",
            "[20/25][348/391] Loss_D1: 0.6435 Loss_D2: 1.1282 Loss_G: 7.2152\n",
            "[20/25][349/391] Loss_D1: 0.7100 Loss_D2: 0.7473 Loss_G: 3.4341\n",
            "[20/25][350/391] Loss_D1: 0.5766 Loss_D2: 0.8037 Loss_G: 6.4420\n",
            "[20/25][351/391] Loss_D1: 0.7926 Loss_D2: 0.9267 Loss_G: 2.5276\n",
            "[20/25][352/391] Loss_D1: 0.5286 Loss_D2: 1.2687 Loss_G: 8.9982\n",
            "[20/25][353/391] Loss_D1: 0.2966 Loss_D2: 1.4685 Loss_G: 4.7796\n",
            "[20/25][354/391] Loss_D1: 0.3884 Loss_D2: 0.7976 Loss_G: 4.6899\n",
            "[20/25][355/391] Loss_D1: 0.5385 Loss_D2: 0.5896 Loss_G: 6.2974\n",
            "[20/25][356/391] Loss_D1: 0.2948 Loss_D2: 0.4871 Loss_G: 6.0100\n",
            "[20/25][357/391] Loss_D1: 0.3636 Loss_D2: 0.6183 Loss_G: 4.6211\n",
            "[20/25][358/391] Loss_D1: 0.3841 Loss_D2: 0.4934 Loss_G: 5.4667\n",
            "[20/25][359/391] Loss_D1: 0.5125 Loss_D2: 0.5685 Loss_G: 5.9653\n",
            "[20/25][360/391] Loss_D1: 0.4026 Loss_D2: 0.7698 Loss_G: 4.8085\n",
            "[20/25][361/391] Loss_D1: 0.3521 Loss_D2: 0.6218 Loss_G: 5.5762\n",
            "[20/25][362/391] Loss_D1: 0.3893 Loss_D2: 0.5952 Loss_G: 4.8042\n",
            "[20/25][363/391] Loss_D1: 0.2058 Loss_D2: 0.6202 Loss_G: 6.7183\n",
            "[20/25][364/391] Loss_D1: 0.4902 Loss_D2: 0.3930 Loss_G: 4.8649\n",
            "[20/25][365/391] Loss_D1: 0.4432 Loss_D2: 0.5653 Loss_G: 4.8829\n",
            "[20/25][366/391] Loss_D1: 0.5302 Loss_D2: 0.4179 Loss_G: 5.6886\n",
            "[20/25][367/391] Loss_D1: 0.5103 Loss_D2: 0.8520 Loss_G: 6.0715\n",
            "[20/25][368/391] Loss_D1: 0.5601 Loss_D2: 1.0225 Loss_G: 4.4450\n",
            "[20/25][369/391] Loss_D1: 0.4194 Loss_D2: 0.8255 Loss_G: 5.5787\n",
            "[20/25][370/391] Loss_D1: 0.3785 Loss_D2: 0.5462 Loss_G: 5.5627\n",
            "[20/25][371/391] Loss_D1: 0.5217 Loss_D2: 0.8268 Loss_G: 3.5093\n",
            "[20/25][372/391] Loss_D1: 0.4760 Loss_D2: 1.4929 Loss_G: 6.4153\n",
            "[20/25][373/391] Loss_D1: 0.8813 Loss_D2: 0.9878 Loss_G: 5.9777\n",
            "[20/25][374/391] Loss_D1: 0.8665 Loss_D2: 0.6092 Loss_G: 3.0146\n",
            "[20/25][375/391] Loss_D1: 1.0519 Loss_D2: 0.6216 Loss_G: 7.9558\n",
            "[20/25][376/391] Loss_D1: 1.1312 Loss_D2: 0.6618 Loss_G: 2.1594\n",
            "[20/25][377/391] Loss_D1: 2.8942 Loss_D2: 0.6563 Loss_G: 10.1143\n",
            "[20/25][378/391] Loss_D1: 2.5150 Loss_D2: 0.7143 Loss_G: 2.1929\n",
            "[20/25][379/391] Loss_D1: 1.9209 Loss_D2: 0.5153 Loss_G: 7.9187\n",
            "[20/25][380/391] Loss_D1: 2.6992 Loss_D2: 0.5935 Loss_G: 2.8698\n",
            "[20/25][381/391] Loss_D1: 2.7694 Loss_D2: 0.5484 Loss_G: 10.7788\n",
            "[20/25][382/391] Loss_D1: 1.9905 Loss_D2: 0.4657 Loss_G: 2.8522\n",
            "[20/25][383/391] Loss_D1: 1.3704 Loss_D2: 0.5974 Loss_G: 8.1298\n",
            "[20/25][384/391] Loss_D1: 1.6137 Loss_D2: 0.4474 Loss_G: 2.8052\n",
            "[20/25][385/391] Loss_D1: 1.3716 Loss_D2: 0.6168 Loss_G: 7.3332\n",
            "[20/25][386/391] Loss_D1: 0.7615 Loss_D2: 0.5967 Loss_G: 5.2882\n",
            "[20/25][387/391] Loss_D1: 0.7221 Loss_D2: 0.5559 Loss_G: 4.5162\n",
            "[20/25][388/391] Loss_D1: 0.6145 Loss_D2: 0.7965 Loss_G: 4.8820\n",
            "[20/25][389/391] Loss_D1: 0.6655 Loss_D2: 0.4998 Loss_G: 5.1561\n",
            "[20/25][390/391] Loss_D1: 1.0061 Loss_D2: 0.5552 Loss_G: 4.0409\n",
            "[21/25][0/391] Loss_D1: 1.4095 Loss_D2: 0.4777 Loss_G: 6.8969\n",
            "saving the output\n",
            "[21/25][1/391] Loss_D1: 1.0264 Loss_D2: 0.4654 Loss_G: 6.8296\n",
            "[21/25][2/391] Loss_D1: 0.6351 Loss_D2: 0.4044 Loss_G: 4.6969\n",
            "[21/25][3/391] Loss_D1: 0.5019 Loss_D2: 0.3285 Loss_G: 5.8568\n",
            "[21/25][4/391] Loss_D1: 0.5014 Loss_D2: 0.3799 Loss_G: 5.4753\n",
            "[21/25][5/391] Loss_D1: 0.5207 Loss_D2: 0.4877 Loss_G: 5.0154\n",
            "[21/25][6/391] Loss_D1: 0.7039 Loss_D2: 0.4481 Loss_G: 5.3351\n",
            "[21/25][7/391] Loss_D1: 0.6066 Loss_D2: 0.3006 Loss_G: 5.2484\n",
            "[21/25][8/391] Loss_D1: 0.8285 Loss_D2: 0.5941 Loss_G: 6.4361\n",
            "[21/25][9/391] Loss_D1: 0.8048 Loss_D2: 0.4495 Loss_G: 4.9944\n",
            "[21/25][10/391] Loss_D1: 0.5228 Loss_D2: 0.3750 Loss_G: 5.3075\n",
            "[21/25][11/391] Loss_D1: 0.5719 Loss_D2: 0.5129 Loss_G: 5.0779\n",
            "[21/25][12/391] Loss_D1: 0.3988 Loss_D2: 0.6110 Loss_G: 6.5723\n",
            "[21/25][13/391] Loss_D1: 0.6983 Loss_D2: 0.4756 Loss_G: 4.2079\n",
            "[21/25][14/391] Loss_D1: 0.6809 Loss_D2: 0.5156 Loss_G: 5.1812\n",
            "[21/25][15/391] Loss_D1: 0.7835 Loss_D2: 0.5390 Loss_G: 5.4547\n",
            "[21/25][16/391] Loss_D1: 0.4926 Loss_D2: 0.5396 Loss_G: 5.1481\n",
            "[21/25][17/391] Loss_D1: 0.3292 Loss_D2: 0.3806 Loss_G: 5.8555\n",
            "[21/25][18/391] Loss_D1: 0.5571 Loss_D2: 0.3917 Loss_G: 4.9778\n",
            "[21/25][19/391] Loss_D1: 0.4953 Loss_D2: 0.3580 Loss_G: 6.3326\n",
            "[21/25][20/391] Loss_D1: 0.4585 Loss_D2: 0.3967 Loss_G: 4.9304\n",
            "[21/25][21/391] Loss_D1: 0.4839 Loss_D2: 0.4231 Loss_G: 5.5712\n",
            "[21/25][22/391] Loss_D1: 0.5256 Loss_D2: 0.3459 Loss_G: 4.9550\n",
            "[21/25][23/391] Loss_D1: 0.4806 Loss_D2: 0.3850 Loss_G: 5.2489\n",
            "[21/25][24/391] Loss_D1: 0.5457 Loss_D2: 0.3913 Loss_G: 6.1983\n",
            "[21/25][25/391] Loss_D1: 0.4790 Loss_D2: 0.2786 Loss_G: 5.6993\n",
            "[21/25][26/391] Loss_D1: 0.3387 Loss_D2: 0.4881 Loss_G: 4.7580\n",
            "[21/25][27/391] Loss_D1: 0.6774 Loss_D2: 0.4580 Loss_G: 4.6221\n",
            "[21/25][28/391] Loss_D1: 0.3690 Loss_D2: 0.3634 Loss_G: 6.3170\n",
            "[21/25][29/391] Loss_D1: 0.5280 Loss_D2: 0.4162 Loss_G: 4.9474\n",
            "[21/25][30/391] Loss_D1: 0.3330 Loss_D2: 0.3035 Loss_G: 6.6480\n",
            "[21/25][31/391] Loss_D1: 0.5686 Loss_D2: 0.3585 Loss_G: 4.8692\n",
            "[21/25][32/391] Loss_D1: 0.4364 Loss_D2: 0.3219 Loss_G: 5.6149\n",
            "[21/25][33/391] Loss_D1: 0.3819 Loss_D2: 0.3190 Loss_G: 5.8943\n",
            "[21/25][34/391] Loss_D1: 0.4191 Loss_D2: 0.3797 Loss_G: 5.1991\n",
            "[21/25][35/391] Loss_D1: 0.2723 Loss_D2: 0.3691 Loss_G: 5.6279\n",
            "[21/25][36/391] Loss_D1: 0.4713 Loss_D2: 0.3741 Loss_G: 5.1700\n",
            "[21/25][37/391] Loss_D1: 0.4636 Loss_D2: 0.4008 Loss_G: 5.3681\n",
            "[21/25][38/391] Loss_D1: 0.3800 Loss_D2: 0.5420 Loss_G: 4.7518\n",
            "[21/25][39/391] Loss_D1: 0.4176 Loss_D2: 0.4703 Loss_G: 3.9914\n",
            "[21/25][40/391] Loss_D1: 0.3692 Loss_D2: 0.4523 Loss_G: 6.3764\n",
            "[21/25][41/391] Loss_D1: 0.3705 Loss_D2: 0.2791 Loss_G: 5.7940\n",
            "[21/25][42/391] Loss_D1: 0.3407 Loss_D2: 0.3714 Loss_G: 4.9462\n",
            "[21/25][43/391] Loss_D1: 0.3131 Loss_D2: 0.7252 Loss_G: 7.0095\n",
            "[21/25][44/391] Loss_D1: 0.3549 Loss_D2: 0.5428 Loss_G: 3.9684\n",
            "[21/25][45/391] Loss_D1: 0.5318 Loss_D2: 0.5918 Loss_G: 4.3566\n",
            "[21/25][46/391] Loss_D1: 0.5703 Loss_D2: 0.5111 Loss_G: 6.6643\n",
            "[21/25][47/391] Loss_D1: 0.4529 Loss_D2: 0.4237 Loss_G: 4.5369\n",
            "[21/25][48/391] Loss_D1: 0.4292 Loss_D2: 0.5126 Loss_G: 6.3020\n",
            "[21/25][49/391] Loss_D1: 0.3337 Loss_D2: 0.5660 Loss_G: 5.6174\n",
            "[21/25][50/391] Loss_D1: 0.3499 Loss_D2: 0.4321 Loss_G: 5.1867\n",
            "[21/25][51/391] Loss_D1: 0.4993 Loss_D2: 0.4175 Loss_G: 5.4392\n",
            "[21/25][52/391] Loss_D1: 0.3719 Loss_D2: 0.5791 Loss_G: 5.4081\n",
            "[21/25][53/391] Loss_D1: 0.3157 Loss_D2: 0.5746 Loss_G: 3.9220\n",
            "[21/25][54/391] Loss_D1: 0.4315 Loss_D2: 0.5441 Loss_G: 5.3314\n",
            "[21/25][55/391] Loss_D1: 0.3287 Loss_D2: 0.3496 Loss_G: 7.3292\n",
            "[21/25][56/391] Loss_D1: 0.5073 Loss_D2: 0.6804 Loss_G: 3.5116\n",
            "[21/25][57/391] Loss_D1: 0.4895 Loss_D2: 0.4465 Loss_G: 4.7477\n",
            "[21/25][58/391] Loss_D1: 0.3568 Loss_D2: 0.2764 Loss_G: 6.6518\n",
            "[21/25][59/391] Loss_D1: 0.3719 Loss_D2: 0.5537 Loss_G: 5.4259\n",
            "[21/25][60/391] Loss_D1: 0.7337 Loss_D2: 0.4535 Loss_G: 4.0695\n",
            "[21/25][61/391] Loss_D1: 0.4562 Loss_D2: 0.5110 Loss_G: 5.0966\n",
            "[21/25][62/391] Loss_D1: 0.3658 Loss_D2: 0.4898 Loss_G: 6.5612\n",
            "[21/25][63/391] Loss_D1: 0.7642 Loss_D2: 0.4365 Loss_G: 3.7413\n",
            "[21/25][64/391] Loss_D1: 0.6448 Loss_D2: 0.4514 Loss_G: 5.9903\n",
            "[21/25][65/391] Loss_D1: 0.2802 Loss_D2: 0.4379 Loss_G: 5.3498\n",
            "[21/25][66/391] Loss_D1: 0.5645 Loss_D2: 0.2730 Loss_G: 4.4236\n",
            "[21/25][67/391] Loss_D1: 0.7926 Loss_D2: 0.4111 Loss_G: 6.6780\n",
            "[21/25][68/391] Loss_D1: 0.7507 Loss_D2: 0.5388 Loss_G: 4.5554\n",
            "[21/25][69/391] Loss_D1: 1.0587 Loss_D2: 0.5406 Loss_G: 6.2159\n",
            "[21/25][70/391] Loss_D1: 1.1696 Loss_D2: 0.3716 Loss_G: 3.7102\n",
            "[21/25][71/391] Loss_D1: 1.0225 Loss_D2: 0.3686 Loss_G: 6.6128\n",
            "[21/25][72/391] Loss_D1: 0.4791 Loss_D2: 0.4297 Loss_G: 5.2527\n",
            "[21/25][73/391] Loss_D1: 0.4177 Loss_D2: 0.4410 Loss_G: 4.2659\n",
            "[21/25][74/391] Loss_D1: 0.4665 Loss_D2: 0.5087 Loss_G: 5.6638\n",
            "[21/25][75/391] Loss_D1: 0.4910 Loss_D2: 0.3974 Loss_G: 5.9068\n",
            "[21/25][76/391] Loss_D1: 0.4713 Loss_D2: 0.5061 Loss_G: 4.1963\n",
            "[21/25][77/391] Loss_D1: 0.3582 Loss_D2: 0.4758 Loss_G: 6.0879\n",
            "[21/25][78/391] Loss_D1: 0.2331 Loss_D2: 0.3761 Loss_G: 5.6360\n",
            "[21/25][79/391] Loss_D1: 0.3273 Loss_D2: 0.4449 Loss_G: 5.5765\n",
            "[21/25][80/391] Loss_D1: 0.5541 Loss_D2: 0.4614 Loss_G: 3.4037\n",
            "[21/25][81/391] Loss_D1: 0.4760 Loss_D2: 0.4842 Loss_G: 6.4997\n",
            "[21/25][82/391] Loss_D1: 0.3848 Loss_D2: 0.4103 Loss_G: 5.7284\n",
            "[21/25][83/391] Loss_D1: 0.3603 Loss_D2: 0.3973 Loss_G: 4.6663\n",
            "[21/25][84/391] Loss_D1: 0.3796 Loss_D2: 0.2924 Loss_G: 6.5695\n",
            "[21/25][85/391] Loss_D1: 0.3415 Loss_D2: 0.3465 Loss_G: 5.7395\n",
            "[21/25][86/391] Loss_D1: 0.4073 Loss_D2: 0.3954 Loss_G: 5.0899\n",
            "[21/25][87/391] Loss_D1: 0.3789 Loss_D2: 0.2722 Loss_G: 5.1321\n",
            "[21/25][88/391] Loss_D1: 0.8855 Loss_D2: 0.5390 Loss_G: 6.6317\n",
            "[21/25][89/391] Loss_D1: 1.1451 Loss_D2: 0.5632 Loss_G: 4.1146\n",
            "[21/25][90/391] Loss_D1: 0.8633 Loss_D2: 0.3504 Loss_G: 7.3516\n",
            "[21/25][91/391] Loss_D1: 0.6445 Loss_D2: 0.4137 Loss_G: 4.5123\n",
            "[21/25][92/391] Loss_D1: 0.4087 Loss_D2: 0.4905 Loss_G: 6.2768\n",
            "[21/25][93/391] Loss_D1: 0.4473 Loss_D2: 0.3818 Loss_G: 5.0649\n",
            "[21/25][94/391] Loss_D1: 0.5131 Loss_D2: 0.3957 Loss_G: 5.1385\n",
            "[21/25][95/391] Loss_D1: 0.2983 Loss_D2: 0.4291 Loss_G: 5.9261\n",
            "[21/25][96/391] Loss_D1: 0.3909 Loss_D2: 0.2507 Loss_G: 5.3889\n",
            "[21/25][97/391] Loss_D1: 0.4336 Loss_D2: 0.5505 Loss_G: 4.6912\n",
            "[21/25][98/391] Loss_D1: 0.7381 Loss_D2: 0.5331 Loss_G: 4.5845\n",
            "[21/25][99/391] Loss_D1: 1.0380 Loss_D2: 0.4466 Loss_G: 8.2245\n",
            "[21/25][100/391] Loss_D1: 1.0597 Loss_D2: 0.4252 Loss_G: 4.8768\n",
            "saving the output\n",
            "[21/25][101/391] Loss_D1: 0.4305 Loss_D2: 0.4215 Loss_G: 4.6034\n",
            "[21/25][102/391] Loss_D1: 0.3210 Loss_D2: 0.5026 Loss_G: 5.5988\n",
            "[21/25][103/391] Loss_D1: 0.4607 Loss_D2: 0.6417 Loss_G: 7.8141\n",
            "[21/25][104/391] Loss_D1: 0.5249 Loss_D2: 0.8185 Loss_G: 3.2750\n",
            "[21/25][105/391] Loss_D1: 0.5598 Loss_D2: 0.7820 Loss_G: 5.8658\n",
            "[21/25][106/391] Loss_D1: 0.3322 Loss_D2: 0.4351 Loss_G: 5.1589\n",
            "[21/25][107/391] Loss_D1: 0.4410 Loss_D2: 0.3472 Loss_G: 6.0946\n",
            "[21/25][108/391] Loss_D1: 0.4730 Loss_D2: 0.4888 Loss_G: 5.1981\n",
            "[21/25][109/391] Loss_D1: 0.4678 Loss_D2: 0.2131 Loss_G: 5.8783\n",
            "[21/25][110/391] Loss_D1: 0.5762 Loss_D2: 0.4562 Loss_G: 5.3453\n",
            "[21/25][111/391] Loss_D1: 0.4626 Loss_D2: 0.3115 Loss_G: 5.9325\n",
            "[21/25][112/391] Loss_D1: 0.3603 Loss_D2: 0.4288 Loss_G: 5.4321\n",
            "[21/25][113/391] Loss_D1: 0.2826 Loss_D2: 0.4553 Loss_G: 5.1818\n",
            "[21/25][114/391] Loss_D1: 0.5770 Loss_D2: 0.4694 Loss_G: 5.2070\n",
            "[21/25][115/391] Loss_D1: 0.4369 Loss_D2: 0.3814 Loss_G: 6.1307\n",
            "[21/25][116/391] Loss_D1: 0.5661 Loss_D2: 0.4038 Loss_G: 3.6928\n",
            "[21/25][117/391] Loss_D1: 0.4717 Loss_D2: 0.5704 Loss_G: 7.4682\n",
            "[21/25][118/391] Loss_D1: 0.3831 Loss_D2: 0.4851 Loss_G: 4.4896\n",
            "[21/25][119/391] Loss_D1: 0.4518 Loss_D2: 0.5579 Loss_G: 6.0194\n",
            "[21/25][120/391] Loss_D1: 0.4825 Loss_D2: 0.7685 Loss_G: 2.7035\n",
            "[21/25][121/391] Loss_D1: 0.6883 Loss_D2: 0.9959 Loss_G: 8.8203\n",
            "[21/25][122/391] Loss_D1: 0.4330 Loss_D2: 0.8435 Loss_G: 2.9820\n",
            "[21/25][123/391] Loss_D1: 0.3714 Loss_D2: 1.4925 Loss_G: 8.7722\n",
            "[21/25][124/391] Loss_D1: 0.4965 Loss_D2: 1.3536 Loss_G: 2.4265\n",
            "[21/25][125/391] Loss_D1: 0.4552 Loss_D2: 1.2346 Loss_G: 7.2509\n",
            "[21/25][126/391] Loss_D1: 0.3459 Loss_D2: 0.8624 Loss_G: 4.8711\n",
            "[21/25][127/391] Loss_D1: 0.4743 Loss_D2: 0.6219 Loss_G: 4.0200\n",
            "[21/25][128/391] Loss_D1: 0.4212 Loss_D2: 0.6785 Loss_G: 6.4754\n",
            "[21/25][129/391] Loss_D1: 0.4388 Loss_D2: 0.9222 Loss_G: 3.1216\n",
            "[21/25][130/391] Loss_D1: 0.5222 Loss_D2: 1.0980 Loss_G: 7.4016\n",
            "[21/25][131/391] Loss_D1: 0.5549 Loss_D2: 1.1659 Loss_G: 3.4574\n",
            "[21/25][132/391] Loss_D1: 0.3071 Loss_D2: 1.0557 Loss_G: 7.2071\n",
            "[21/25][133/391] Loss_D1: 0.3105 Loss_D2: 1.0107 Loss_G: 4.8954\n",
            "[21/25][134/391] Loss_D1: 0.4665 Loss_D2: 0.5743 Loss_G: 4.5840\n",
            "[21/25][135/391] Loss_D1: 0.6621 Loss_D2: 0.4842 Loss_G: 7.3609\n",
            "[21/25][136/391] Loss_D1: 0.3549 Loss_D2: 0.5662 Loss_G: 4.9218\n",
            "[21/25][137/391] Loss_D1: 0.3580 Loss_D2: 0.4953 Loss_G: 5.5282\n",
            "[21/25][138/391] Loss_D1: 0.5349 Loss_D2: 0.5204 Loss_G: 4.7826\n",
            "[21/25][139/391] Loss_D1: 0.3541 Loss_D2: 0.6173 Loss_G: 6.7789\n",
            "[21/25][140/391] Loss_D1: 0.2739 Loss_D2: 0.5955 Loss_G: 4.7892\n",
            "[21/25][141/391] Loss_D1: 0.3382 Loss_D2: 0.4953 Loss_G: 5.1175\n",
            "[21/25][142/391] Loss_D1: 0.5055 Loss_D2: 0.5196 Loss_G: 3.6096\n",
            "[21/25][143/391] Loss_D1: 0.6971 Loss_D2: 0.7202 Loss_G: 8.0537\n",
            "[21/25][144/391] Loss_D1: 0.6906 Loss_D2: 0.6048 Loss_G: 3.5596\n",
            "[21/25][145/391] Loss_D1: 0.4937 Loss_D2: 0.6508 Loss_G: 6.0397\n",
            "[21/25][146/391] Loss_D1: 0.4076 Loss_D2: 0.4779 Loss_G: 4.8367\n",
            "[21/25][147/391] Loss_D1: 0.3587 Loss_D2: 0.6439 Loss_G: 6.4785\n",
            "[21/25][148/391] Loss_D1: 0.3117 Loss_D2: 0.6294 Loss_G: 4.3002\n",
            "[21/25][149/391] Loss_D1: 0.2817 Loss_D2: 0.5919 Loss_G: 6.9221\n",
            "[21/25][150/391] Loss_D1: 0.3475 Loss_D2: 0.6125 Loss_G: 4.8279\n",
            "[21/25][151/391] Loss_D1: 0.2174 Loss_D2: 0.3980 Loss_G: 6.7003\n",
            "[21/25][152/391] Loss_D1: 0.3939 Loss_D2: 0.5672 Loss_G: 4.5055\n",
            "[21/25][153/391] Loss_D1: 0.3493 Loss_D2: 0.3245 Loss_G: 5.5506\n",
            "[21/25][154/391] Loss_D1: 0.4888 Loss_D2: 0.4529 Loss_G: 5.6552\n",
            "[21/25][155/391] Loss_D1: 0.2731 Loss_D2: 0.6727 Loss_G: 5.3286\n",
            "[21/25][156/391] Loss_D1: 0.3815 Loss_D2: 0.3649 Loss_G: 4.8955\n",
            "[21/25][157/391] Loss_D1: 0.4636 Loss_D2: 0.3474 Loss_G: 5.5560\n",
            "[21/25][158/391] Loss_D1: 0.3906 Loss_D2: 0.3885 Loss_G: 5.4999\n",
            "[21/25][159/391] Loss_D1: 0.4472 Loss_D2: 0.4679 Loss_G: 5.2050\n",
            "[21/25][160/391] Loss_D1: 0.3430 Loss_D2: 0.3953 Loss_G: 4.7858\n",
            "[21/25][161/391] Loss_D1: 0.3577 Loss_D2: 0.4019 Loss_G: 6.6823\n",
            "[21/25][162/391] Loss_D1: 0.4980 Loss_D2: 0.5469 Loss_G: 3.1221\n",
            "[21/25][163/391] Loss_D1: 0.7615 Loss_D2: 0.7011 Loss_G: 8.6883\n",
            "[21/25][164/391] Loss_D1: 0.4911 Loss_D2: 1.1166 Loss_G: 1.6118\n",
            "[21/25][165/391] Loss_D1: 0.7215 Loss_D2: 1.8897 Loss_G: 10.5400\n",
            "[21/25][166/391] Loss_D1: 1.2599 Loss_D2: 1.3075 Loss_G: 2.0461\n",
            "[21/25][167/391] Loss_D1: 0.8646 Loss_D2: 0.7710 Loss_G: 8.6248\n",
            "[21/25][168/391] Loss_D1: 0.5751 Loss_D2: 0.6404 Loss_G: 3.3223\n",
            "[21/25][169/391] Loss_D1: 0.6868 Loss_D2: 0.5281 Loss_G: 7.1926\n",
            "[21/25][170/391] Loss_D1: 1.3959 Loss_D2: 0.6891 Loss_G: 3.7083\n",
            "[21/25][171/391] Loss_D1: 1.4247 Loss_D2: 0.5916 Loss_G: 7.6209\n",
            "[21/25][172/391] Loss_D1: 0.7480 Loss_D2: 0.4633 Loss_G: 3.5729\n",
            "[21/25][173/391] Loss_D1: 0.5005 Loss_D2: 0.5166 Loss_G: 7.2205\n",
            "[21/25][174/391] Loss_D1: 0.5223 Loss_D2: 0.4574 Loss_G: 5.3104\n",
            "[21/25][175/391] Loss_D1: 0.4384 Loss_D2: 0.6589 Loss_G: 4.5214\n",
            "[21/25][176/391] Loss_D1: 0.3904 Loss_D2: 0.5110 Loss_G: 6.8986\n",
            "[21/25][177/391] Loss_D1: 0.7188 Loss_D2: 0.5933 Loss_G: 3.0646\n",
            "[21/25][178/391] Loss_D1: 0.7379 Loss_D2: 0.6638 Loss_G: 7.6580\n",
            "[21/25][179/391] Loss_D1: 0.4013 Loss_D2: 0.4573 Loss_G: 5.7903\n",
            "[21/25][180/391] Loss_D1: 0.2980 Loss_D2: 0.4039 Loss_G: 4.7294\n",
            "[21/25][181/391] Loss_D1: 0.4685 Loss_D2: 0.4011 Loss_G: 5.7457\n",
            "[21/25][182/391] Loss_D1: 0.3779 Loss_D2: 0.4970 Loss_G: 6.2553\n",
            "[21/25][183/391] Loss_D1: 0.3730 Loss_D2: 0.6438 Loss_G: 4.0040\n",
            "[21/25][184/391] Loss_D1: 0.4156 Loss_D2: 0.8095 Loss_G: 7.4186\n",
            "[21/25][185/391] Loss_D1: 0.4669 Loss_D2: 0.5776 Loss_G: 4.2572\n",
            "[21/25][186/391] Loss_D1: 0.6094 Loss_D2: 0.4919 Loss_G: 6.5446\n",
            "[21/25][187/391] Loss_D1: 0.4753 Loss_D2: 0.5103 Loss_G: 4.5662\n",
            "[21/25][188/391] Loss_D1: 0.4696 Loss_D2: 0.3933 Loss_G: 6.3727\n",
            "[21/25][189/391] Loss_D1: 0.4986 Loss_D2: 0.4943 Loss_G: 5.5186\n",
            "[21/25][190/391] Loss_D1: 0.5164 Loss_D2: 0.6474 Loss_G: 5.0783\n",
            "[21/25][191/391] Loss_D1: 0.5155 Loss_D2: 0.5534 Loss_G: 6.2074\n",
            "[21/25][192/391] Loss_D1: 0.3741 Loss_D2: 0.5759 Loss_G: 4.6991\n",
            "[21/25][193/391] Loss_D1: 0.3798 Loss_D2: 0.5100 Loss_G: 5.8476\n",
            "[21/25][194/391] Loss_D1: 0.4965 Loss_D2: 0.3061 Loss_G: 5.4065\n",
            "[21/25][195/391] Loss_D1: 0.5506 Loss_D2: 0.3619 Loss_G: 5.6178\n",
            "[21/25][196/391] Loss_D1: 0.4883 Loss_D2: 0.3446 Loss_G: 5.8013\n",
            "[21/25][197/391] Loss_D1: 0.2700 Loss_D2: 0.4610 Loss_G: 4.5329\n",
            "[21/25][198/391] Loss_D1: 0.3998 Loss_D2: 0.5618 Loss_G: 6.3417\n",
            "[21/25][199/391] Loss_D1: 0.2622 Loss_D2: 0.3278 Loss_G: 6.4793\n",
            "[21/25][200/391] Loss_D1: 0.3802 Loss_D2: 0.4924 Loss_G: 4.2838\n",
            "saving the output\n",
            "[21/25][201/391] Loss_D1: 0.4047 Loss_D2: 0.4446 Loss_G: 6.7514\n",
            "[21/25][202/391] Loss_D1: 0.3112 Loss_D2: 0.4136 Loss_G: 5.6351\n",
            "[21/25][203/391] Loss_D1: 0.4128 Loss_D2: 0.4850 Loss_G: 4.9472\n",
            "[21/25][204/391] Loss_D1: 0.5233 Loss_D2: 0.2302 Loss_G: 6.7915\n",
            "[21/25][205/391] Loss_D1: 0.2769 Loss_D2: 0.3042 Loss_G: 5.9189\n",
            "[21/25][206/391] Loss_D1: 0.3277 Loss_D2: 0.4755 Loss_G: 3.7375\n",
            "[21/25][207/391] Loss_D1: 0.5764 Loss_D2: 0.4270 Loss_G: 7.7906\n",
            "[21/25][208/391] Loss_D1: 0.5807 Loss_D2: 0.6913 Loss_G: 3.7182\n",
            "[21/25][209/391] Loss_D1: 0.4859 Loss_D2: 0.8205 Loss_G: 5.6638\n",
            "[21/25][210/391] Loss_D1: 0.6815 Loss_D2: 0.6200 Loss_G: 5.6962\n",
            "[21/25][211/391] Loss_D1: 0.4389 Loss_D2: 0.3476 Loss_G: 4.5536\n",
            "[21/25][212/391] Loss_D1: 0.6398 Loss_D2: 0.5136 Loss_G: 6.9764\n",
            "[21/25][213/391] Loss_D1: 0.7961 Loss_D2: 0.3605 Loss_G: 3.9955\n",
            "[21/25][214/391] Loss_D1: 0.6493 Loss_D2: 0.5043 Loss_G: 7.2094\n",
            "[21/25][215/391] Loss_D1: 0.6552 Loss_D2: 0.5013 Loss_G: 4.4430\n",
            "[21/25][216/391] Loss_D1: 0.4658 Loss_D2: 0.4268 Loss_G: 5.5481\n",
            "[21/25][217/391] Loss_D1: 0.4218 Loss_D2: 0.3914 Loss_G: 6.0419\n",
            "[21/25][218/391] Loss_D1: 0.4051 Loss_D2: 0.3745 Loss_G: 4.6188\n",
            "[21/25][219/391] Loss_D1: 0.5327 Loss_D2: 0.5187 Loss_G: 7.6349\n",
            "[21/25][220/391] Loss_D1: 0.3144 Loss_D2: 0.6699 Loss_G: 4.0416\n",
            "[21/25][221/391] Loss_D1: 0.2726 Loss_D2: 0.6498 Loss_G: 6.1933\n",
            "[21/25][222/391] Loss_D1: 0.4129 Loss_D2: 0.3864 Loss_G: 5.2257\n",
            "[21/25][223/391] Loss_D1: 0.2756 Loss_D2: 0.5459 Loss_G: 5.5786\n",
            "[21/25][224/391] Loss_D1: 0.2618 Loss_D2: 0.4527 Loss_G: 5.8084\n",
            "[21/25][225/391] Loss_D1: 0.3977 Loss_D2: 0.3782 Loss_G: 4.5465\n",
            "[21/25][226/391] Loss_D1: 0.4312 Loss_D2: 0.5673 Loss_G: 5.3121\n",
            "[21/25][227/391] Loss_D1: 0.5216 Loss_D2: 0.6083 Loss_G: 5.9062\n",
            "[21/25][228/391] Loss_D1: 0.6092 Loss_D2: 0.3894 Loss_G: 5.2091\n",
            "[21/25][229/391] Loss_D1: 0.4430 Loss_D2: 0.7134 Loss_G: 3.1912\n",
            "[21/25][230/391] Loss_D1: 0.6680 Loss_D2: 0.6657 Loss_G: 8.5747\n",
            "[21/25][231/391] Loss_D1: 0.5261 Loss_D2: 0.7665 Loss_G: 3.9618\n",
            "[21/25][232/391] Loss_D1: 0.3098 Loss_D2: 0.5275 Loss_G: 5.3927\n",
            "[21/25][233/391] Loss_D1: 0.4424 Loss_D2: 0.5217 Loss_G: 5.6441\n",
            "[21/25][234/391] Loss_D1: 0.3168 Loss_D2: 0.5744 Loss_G: 8.2123\n",
            "[21/25][235/391] Loss_D1: 0.9505 Loss_D2: 0.4915 Loss_G: 2.9407\n",
            "[21/25][236/391] Loss_D1: 0.9581 Loss_D2: 0.5439 Loss_G: 6.4007\n",
            "[21/25][237/391] Loss_D1: 0.8182 Loss_D2: 0.5263 Loss_G: 3.3045\n",
            "[21/25][238/391] Loss_D1: 1.8951 Loss_D2: 0.5163 Loss_G: 9.0654\n",
            "[21/25][239/391] Loss_D1: 1.1986 Loss_D2: 0.5403 Loss_G: 3.3402\n",
            "[21/25][240/391] Loss_D1: 0.7964 Loss_D2: 0.5897 Loss_G: 6.9149\n",
            "[21/25][241/391] Loss_D1: 0.9359 Loss_D2: 0.5880 Loss_G: 2.7604\n",
            "[21/25][242/391] Loss_D1: 0.8406 Loss_D2: 0.7440 Loss_G: 7.8199\n",
            "[21/25][243/391] Loss_D1: 0.4875 Loss_D2: 0.5578 Loss_G: 5.0354\n",
            "[21/25][244/391] Loss_D1: 0.6586 Loss_D2: 0.4488 Loss_G: 3.6166\n",
            "[21/25][245/391] Loss_D1: 0.7842 Loss_D2: 0.4922 Loss_G: 8.2279\n",
            "[21/25][246/391] Loss_D1: 0.4006 Loss_D2: 0.5734 Loss_G: 4.7416\n",
            "[21/25][247/391] Loss_D1: 0.3932 Loss_D2: 0.5915 Loss_G: 5.6052\n",
            "[21/25][248/391] Loss_D1: 0.8745 Loss_D2: 0.3952 Loss_G: 7.1247\n",
            "[21/25][249/391] Loss_D1: 0.7497 Loss_D2: 0.3778 Loss_G: 3.3536\n",
            "[21/25][250/391] Loss_D1: 1.2433 Loss_D2: 0.4017 Loss_G: 8.7092\n",
            "[21/25][251/391] Loss_D1: 1.5110 Loss_D2: 0.3282 Loss_G: 4.0418\n",
            "[21/25][252/391] Loss_D1: 0.4828 Loss_D2: 0.4433 Loss_G: 5.8986\n",
            "[21/25][253/391] Loss_D1: 0.5326 Loss_D2: 0.3729 Loss_G: 7.3458\n",
            "[21/25][254/391] Loss_D1: 1.2784 Loss_D2: 0.4366 Loss_G: 2.8739\n",
            "[21/25][255/391] Loss_D1: 2.8925 Loss_D2: 0.5061 Loss_G: 7.2758\n",
            "[21/25][256/391] Loss_D1: 1.3213 Loss_D2: 0.4702 Loss_G: 5.2017\n",
            "[21/25][257/391] Loss_D1: 1.2112 Loss_D2: 0.6322 Loss_G: 6.1138\n",
            "[21/25][258/391] Loss_D1: 0.8298 Loss_D2: 0.5049 Loss_G: 4.5087\n",
            "[21/25][259/391] Loss_D1: 0.9334 Loss_D2: 0.5729 Loss_G: 3.8962\n",
            "[21/25][260/391] Loss_D1: 1.0271 Loss_D2: 0.5299 Loss_G: 7.4250\n",
            "[21/25][261/391] Loss_D1: 0.6220 Loss_D2: 0.4599 Loss_G: 4.4907\n",
            "[21/25][262/391] Loss_D1: 0.5427 Loss_D2: 0.4633 Loss_G: 6.6097\n",
            "[21/25][263/391] Loss_D1: 0.7311 Loss_D2: 0.4726 Loss_G: 3.6553\n",
            "[21/25][264/391] Loss_D1: 0.8270 Loss_D2: 0.4870 Loss_G: 8.1990\n",
            "[21/25][265/391] Loss_D1: 0.8604 Loss_D2: 0.4144 Loss_G: 4.7255\n",
            "[21/25][266/391] Loss_D1: 0.4470 Loss_D2: 0.3554 Loss_G: 5.6825\n",
            "[21/25][267/391] Loss_D1: 0.6613 Loss_D2: 0.4873 Loss_G: 4.6850\n",
            "[21/25][268/391] Loss_D1: 0.3286 Loss_D2: 0.4290 Loss_G: 6.7300\n",
            "[21/25][269/391] Loss_D1: 0.5353 Loss_D2: 0.3940 Loss_G: 5.6901\n",
            "[21/25][270/391] Loss_D1: 0.3673 Loss_D2: 0.4612 Loss_G: 4.5271\n",
            "[21/25][271/391] Loss_D1: 0.3700 Loss_D2: 0.6146 Loss_G: 6.3611\n",
            "[21/25][272/391] Loss_D1: 0.5466 Loss_D2: 0.3947 Loss_G: 5.7242\n",
            "[21/25][273/391] Loss_D1: 0.4850 Loss_D2: 0.5343 Loss_G: 4.1622\n",
            "[21/25][274/391] Loss_D1: 0.4853 Loss_D2: 0.9086 Loss_G: 6.3500\n",
            "[21/25][275/391] Loss_D1: 0.4127 Loss_D2: 0.3879 Loss_G: 5.0224\n",
            "[21/25][276/391] Loss_D1: 0.5671 Loss_D2: 0.5959 Loss_G: 4.5490\n",
            "[21/25][277/391] Loss_D1: 0.4985 Loss_D2: 0.4825 Loss_G: 5.8610\n",
            "[21/25][278/391] Loss_D1: 0.3362 Loss_D2: 0.3938 Loss_G: 5.3117\n",
            "[21/25][279/391] Loss_D1: 0.5121 Loss_D2: 0.5196 Loss_G: 5.6619\n",
            "[21/25][280/391] Loss_D1: 0.3733 Loss_D2: 0.6142 Loss_G: 3.4259\n",
            "[21/25][281/391] Loss_D1: 0.4774 Loss_D2: 0.5698 Loss_G: 5.8877\n",
            "[21/25][282/391] Loss_D1: 0.4508 Loss_D2: 0.6164 Loss_G: 5.2314\n",
            "[21/25][283/391] Loss_D1: 0.5511 Loss_D2: 0.2952 Loss_G: 5.0688\n",
            "[21/25][284/391] Loss_D1: 0.4398 Loss_D2: 0.4746 Loss_G: 4.9633\n",
            "[21/25][285/391] Loss_D1: 0.3192 Loss_D2: 0.4695 Loss_G: 7.2556\n",
            "[21/25][286/391] Loss_D1: 0.3049 Loss_D2: 1.1069 Loss_G: 2.9966\n",
            "[21/25][287/391] Loss_D1: 0.3224 Loss_D2: 1.8920 Loss_G: 8.4190\n",
            "[21/25][288/391] Loss_D1: 0.3476 Loss_D2: 0.5010 Loss_G: 3.9779\n",
            "[21/25][289/391] Loss_D1: 0.4057 Loss_D2: 1.9580 Loss_G: 12.3801\n",
            "[21/25][290/391] Loss_D1: 0.2571 Loss_D2: 4.0488 Loss_G: 2.5073\n",
            "[21/25][291/391] Loss_D1: 0.4924 Loss_D2: 4.6143 Loss_G: 11.0747\n",
            "[21/25][292/391] Loss_D1: 0.2433 Loss_D2: 2.7291 Loss_G: 3.5577\n",
            "[21/25][293/391] Loss_D1: 0.2358 Loss_D2: 2.9468 Loss_G: 11.0709\n",
            "[21/25][294/391] Loss_D1: 0.5403 Loss_D2: 4.3782 Loss_G: 2.8159\n",
            "[21/25][295/391] Loss_D1: 0.5045 Loss_D2: 1.4552 Loss_G: 6.1072\n",
            "[21/25][296/391] Loss_D1: 0.4483 Loss_D2: 0.9555 Loss_G: 5.0085\n",
            "[21/25][297/391] Loss_D1: 0.5801 Loss_D2: 1.6490 Loss_G: 5.8990\n",
            "[21/25][298/391] Loss_D1: 0.6320 Loss_D2: 1.9291 Loss_G: 2.5767\n",
            "[21/25][299/391] Loss_D1: 0.4859 Loss_D2: 2.8242 Loss_G: 8.5847\n",
            "[21/25][300/391] Loss_D1: 0.5689 Loss_D2: 1.3004 Loss_G: 4.3148\n",
            "saving the output\n",
            "[21/25][301/391] Loss_D1: 0.4366 Loss_D2: 0.7996 Loss_G: 4.7230\n",
            "[21/25][302/391] Loss_D1: 0.2705 Loss_D2: 0.8904 Loss_G: 6.1832\n",
            "[21/25][303/391] Loss_D1: 0.5229 Loss_D2: 1.0279 Loss_G: 3.0815\n",
            "[21/25][304/391] Loss_D1: 0.6656 Loss_D2: 1.2252 Loss_G: 9.1204\n",
            "[21/25][305/391] Loss_D1: 0.4700 Loss_D2: 1.1066 Loss_G: 4.4344\n",
            "[21/25][306/391] Loss_D1: 0.2749 Loss_D2: 0.6418 Loss_G: 4.9613\n",
            "[21/25][307/391] Loss_D1: 0.3908 Loss_D2: 0.6917 Loss_G: 5.2397\n",
            "[21/25][308/391] Loss_D1: 0.4664 Loss_D2: 0.6020 Loss_G: 5.7365\n",
            "[21/25][309/391] Loss_D1: 0.5289 Loss_D2: 0.7642 Loss_G: 3.8734\n",
            "[21/25][310/391] Loss_D1: 0.4046 Loss_D2: 0.6814 Loss_G: 5.6658\n",
            "[21/25][311/391] Loss_D1: 0.3330 Loss_D2: 0.6965 Loss_G: 5.0891\n",
            "[21/25][312/391] Loss_D1: 0.5392 Loss_D2: 0.5428 Loss_G: 4.4718\n",
            "[21/25][313/391] Loss_D1: 0.3875 Loss_D2: 0.6032 Loss_G: 6.3918\n",
            "[21/25][314/391] Loss_D1: 0.3001 Loss_D2: 0.8622 Loss_G: 4.2446\n",
            "[21/25][315/391] Loss_D1: 0.3600 Loss_D2: 0.6787 Loss_G: 5.7304\n",
            "[21/25][316/391] Loss_D1: 0.4032 Loss_D2: 0.5715 Loss_G: 5.4390\n",
            "[21/25][317/391] Loss_D1: 0.2679 Loss_D2: 0.6951 Loss_G: 5.2911\n",
            "[21/25][318/391] Loss_D1: 0.2232 Loss_D2: 0.6479 Loss_G: 6.4824\n",
            "[21/25][319/391] Loss_D1: 0.2974 Loss_D2: 0.5982 Loss_G: 5.2573\n",
            "[21/25][320/391] Loss_D1: 0.2696 Loss_D2: 0.6784 Loss_G: 4.8546\n",
            "[21/25][321/391] Loss_D1: 0.4094 Loss_D2: 0.8560 Loss_G: 6.8898\n",
            "[21/25][322/391] Loss_D1: 0.3043 Loss_D2: 0.7357 Loss_G: 4.5175\n",
            "[21/25][323/391] Loss_D1: 0.4681 Loss_D2: 0.7335 Loss_G: 4.2697\n",
            "[21/25][324/391] Loss_D1: 0.3816 Loss_D2: 0.4782 Loss_G: 5.9520\n",
            "[21/25][325/391] Loss_D1: 0.3476 Loss_D2: 0.4834 Loss_G: 5.5707\n",
            "[21/25][326/391] Loss_D1: 0.4392 Loss_D2: 0.4572 Loss_G: 4.6633\n",
            "[21/25][327/391] Loss_D1: 0.3888 Loss_D2: 0.5908 Loss_G: 5.2542\n",
            "[21/25][328/391] Loss_D1: 0.5434 Loss_D2: 0.7738 Loss_G: 6.4940\n",
            "[21/25][329/391] Loss_D1: 0.4213 Loss_D2: 1.0191 Loss_G: 4.5122\n",
            "[21/25][330/391] Loss_D1: 0.3419 Loss_D2: 1.0278 Loss_G: 6.4103\n",
            "[21/25][331/391] Loss_D1: 0.3365 Loss_D2: 0.4079 Loss_G: 5.4153\n",
            "[21/25][332/391] Loss_D1: 0.4834 Loss_D2: 0.5292 Loss_G: 3.8985\n",
            "[21/25][333/391] Loss_D1: 0.4364 Loss_D2: 0.5478 Loss_G: 7.1182\n",
            "[21/25][334/391] Loss_D1: 0.3972 Loss_D2: 0.3920 Loss_G: 5.3632\n",
            "[21/25][335/391] Loss_D1: 0.3759 Loss_D2: 0.4202 Loss_G: 4.3768\n",
            "[21/25][336/391] Loss_D1: 0.3362 Loss_D2: 0.6161 Loss_G: 6.2984\n",
            "[21/25][337/391] Loss_D1: 0.3326 Loss_D2: 0.4438 Loss_G: 6.0795\n",
            "[21/25][338/391] Loss_D1: 0.6083 Loss_D2: 0.6349 Loss_G: 2.2542\n",
            "[21/25][339/391] Loss_D1: 0.5937 Loss_D2: 1.0906 Loss_G: 8.3122\n",
            "[21/25][340/391] Loss_D1: 0.5213 Loss_D2: 0.9273 Loss_G: 2.7118\n",
            "[21/25][341/391] Loss_D1: 0.7123 Loss_D2: 0.7879 Loss_G: 7.6016\n",
            "[21/25][342/391] Loss_D1: 0.4227 Loss_D2: 0.6414 Loss_G: 4.5175\n",
            "[21/25][343/391] Loss_D1: 0.3099 Loss_D2: 0.6025 Loss_G: 4.6160\n",
            "[21/25][344/391] Loss_D1: 0.5964 Loss_D2: 0.5289 Loss_G: 6.5764\n",
            "[21/25][345/391] Loss_D1: 0.4502 Loss_D2: 0.5632 Loss_G: 5.1990\n",
            "[21/25][346/391] Loss_D1: 0.3703 Loss_D2: 0.5520 Loss_G: 3.2730\n",
            "[21/25][347/391] Loss_D1: 0.6407 Loss_D2: 0.8094 Loss_G: 9.2640\n",
            "[21/25][348/391] Loss_D1: 0.8671 Loss_D2: 0.5306 Loss_G: 3.7912\n",
            "[21/25][349/391] Loss_D1: 0.9314 Loss_D2: 0.4234 Loss_G: 6.6604\n",
            "[21/25][350/391] Loss_D1: 1.2056 Loss_D2: 0.4818 Loss_G: 2.8081\n",
            "[21/25][351/391] Loss_D1: 1.5953 Loss_D2: 0.5491 Loss_G: 9.0882\n",
            "[21/25][352/391] Loss_D1: 0.8606 Loss_D2: 0.7041 Loss_G: 2.7741\n",
            "[21/25][353/391] Loss_D1: 1.0640 Loss_D2: 0.3845 Loss_G: 8.2359\n",
            "[21/25][354/391] Loss_D1: 1.0176 Loss_D2: 0.4691 Loss_G: 4.7863\n",
            "[21/25][355/391] Loss_D1: 0.6272 Loss_D2: 0.6661 Loss_G: 3.7000\n",
            "[21/25][356/391] Loss_D1: 0.9165 Loss_D2: 0.7231 Loss_G: 9.1964\n",
            "[21/25][357/391] Loss_D1: 1.0347 Loss_D2: 0.6115 Loss_G: 2.8737\n",
            "[21/25][358/391] Loss_D1: 0.7819 Loss_D2: 0.5918 Loss_G: 6.4951\n",
            "[21/25][359/391] Loss_D1: 0.3814 Loss_D2: 0.4435 Loss_G: 5.8255\n",
            "[21/25][360/391] Loss_D1: 0.4869 Loss_D2: 0.3914 Loss_G: 4.5269\n",
            "[21/25][361/391] Loss_D1: 0.5668 Loss_D2: 0.3761 Loss_G: 4.7965\n",
            "[21/25][362/391] Loss_D1: 0.5150 Loss_D2: 0.5704 Loss_G: 5.4489\n",
            "[21/25][363/391] Loss_D1: 0.4458 Loss_D2: 0.4164 Loss_G: 5.2077\n",
            "[21/25][364/391] Loss_D1: 0.4819 Loss_D2: 0.3837 Loss_G: 5.7958\n",
            "[21/25][365/391] Loss_D1: 0.2752 Loss_D2: 0.4527 Loss_G: 5.8643\n",
            "[21/25][366/391] Loss_D1: 0.4747 Loss_D2: 0.5402 Loss_G: 4.4682\n",
            "[21/25][367/391] Loss_D1: 0.3460 Loss_D2: 0.4114 Loss_G: 6.0711\n",
            "[21/25][368/391] Loss_D1: 0.4436 Loss_D2: 0.4365 Loss_G: 4.6410\n",
            "[21/25][369/391] Loss_D1: 0.3845 Loss_D2: 0.3598 Loss_G: 5.6317\n",
            "[21/25][370/391] Loss_D1: 0.3544 Loss_D2: 0.6250 Loss_G: 4.5353\n",
            "[21/25][371/391] Loss_D1: 0.3062 Loss_D2: 0.4955 Loss_G: 6.3222\n",
            "[21/25][372/391] Loss_D1: 0.2279 Loss_D2: 0.3545 Loss_G: 6.0022\n",
            "[21/25][373/391] Loss_D1: 0.3720 Loss_D2: 0.6503 Loss_G: 3.3534\n",
            "[21/25][374/391] Loss_D1: 0.3696 Loss_D2: 0.9280 Loss_G: 7.3562\n",
            "[21/25][375/391] Loss_D1: 0.4146 Loss_D2: 0.6479 Loss_G: 4.2300\n",
            "[21/25][376/391] Loss_D1: 0.3761 Loss_D2: 0.3686 Loss_G: 5.8360\n",
            "[21/25][377/391] Loss_D1: 0.3041 Loss_D2: 0.4696 Loss_G: 5.6782\n",
            "[21/25][378/391] Loss_D1: 0.3541 Loss_D2: 0.3752 Loss_G: 4.6179\n",
            "[21/25][379/391] Loss_D1: 0.4002 Loss_D2: 0.5331 Loss_G: 6.6653\n",
            "[21/25][380/391] Loss_D1: 0.5576 Loss_D2: 0.5053 Loss_G: 4.1364\n",
            "[21/25][381/391] Loss_D1: 0.5631 Loss_D2: 0.3887 Loss_G: 5.4617\n",
            "[21/25][382/391] Loss_D1: 0.3090 Loss_D2: 0.4731 Loss_G: 6.0076\n",
            "[21/25][383/391] Loss_D1: 0.3581 Loss_D2: 0.4214 Loss_G: 4.5367\n",
            "[21/25][384/391] Loss_D1: 0.4493 Loss_D2: 0.4425 Loss_G: 5.2016\n",
            "[21/25][385/391] Loss_D1: 0.4918 Loss_D2: 0.4789 Loss_G: 5.3515\n",
            "[21/25][386/391] Loss_D1: 0.4826 Loss_D2: 0.4427 Loss_G: 4.2274\n",
            "[21/25][387/391] Loss_D1: 0.4120 Loss_D2: 0.4522 Loss_G: 5.9465\n",
            "[21/25][388/391] Loss_D1: 0.4174 Loss_D2: 0.3459 Loss_G: 5.0655\n",
            "[21/25][389/391] Loss_D1: 0.4241 Loss_D2: 0.5120 Loss_G: 6.2299\n",
            "[21/25][390/391] Loss_D1: 0.3982 Loss_D2: 0.4787 Loss_G: 4.8707\n",
            "[22/25][0/391] Loss_D1: 0.3601 Loss_D2: 0.5061 Loss_G: 4.0559\n",
            "saving the output\n",
            "[22/25][1/391] Loss_D1: 0.3880 Loss_D2: 0.5493 Loss_G: 7.2348\n",
            "[22/25][2/391] Loss_D1: 0.2503 Loss_D2: 0.5396 Loss_G: 5.7574\n",
            "[22/25][3/391] Loss_D1: 0.3582 Loss_D2: 0.3769 Loss_G: 4.8278\n",
            "[22/25][4/391] Loss_D1: 0.3991 Loss_D2: 0.4126 Loss_G: 6.0825\n",
            "[22/25][5/391] Loss_D1: 0.3746 Loss_D2: 0.4031 Loss_G: 5.0604\n",
            "[22/25][6/391] Loss_D1: 0.3793 Loss_D2: 0.4520 Loss_G: 4.3930\n",
            "[22/25][7/391] Loss_D1: 0.4328 Loss_D2: 0.3979 Loss_G: 6.7481\n",
            "[22/25][8/391] Loss_D1: 0.2756 Loss_D2: 0.6039 Loss_G: 4.9478\n",
            "[22/25][9/391] Loss_D1: 0.2355 Loss_D2: 0.5367 Loss_G: 6.2996\n",
            "[22/25][10/391] Loss_D1: 0.4264 Loss_D2: 0.4567 Loss_G: 4.6417\n",
            "[22/25][11/391] Loss_D1: 0.5829 Loss_D2: 0.4084 Loss_G: 3.4355\n",
            "[22/25][12/391] Loss_D1: 0.5698 Loss_D2: 0.3596 Loss_G: 6.5817\n",
            "[22/25][13/391] Loss_D1: 0.4900 Loss_D2: 0.4113 Loss_G: 5.1776\n",
            "[22/25][14/391] Loss_D1: 0.3442 Loss_D2: 0.5435 Loss_G: 5.4187\n",
            "[22/25][15/391] Loss_D1: 0.4248 Loss_D2: 0.3437 Loss_G: 4.9773\n",
            "[22/25][16/391] Loss_D1: 0.3850 Loss_D2: 0.3881 Loss_G: 4.9412\n",
            "[22/25][17/391] Loss_D1: 0.7677 Loss_D2: 0.4848 Loss_G: 6.2007\n",
            "[22/25][18/391] Loss_D1: 0.2738 Loss_D2: 0.3827 Loss_G: 6.5468\n",
            "[22/25][19/391] Loss_D1: 0.4916 Loss_D2: 0.4704 Loss_G: 4.1744\n",
            "[22/25][20/391] Loss_D1: 0.3215 Loss_D2: 0.3372 Loss_G: 5.1747\n",
            "[22/25][21/391] Loss_D1: 0.3585 Loss_D2: 0.3931 Loss_G: 5.9413\n",
            "[22/25][22/391] Loss_D1: 0.3127 Loss_D2: 0.4464 Loss_G: 5.2987\n",
            "[22/25][23/391] Loss_D1: 0.3191 Loss_D2: 0.4215 Loss_G: 4.7923\n",
            "[22/25][24/391] Loss_D1: 0.3040 Loss_D2: 0.4039 Loss_G: 6.1800\n",
            "[22/25][25/391] Loss_D1: 0.4298 Loss_D2: 0.3610 Loss_G: 4.8806\n",
            "[22/25][26/391] Loss_D1: 0.2683 Loss_D2: 0.2943 Loss_G: 5.8248\n",
            "[22/25][27/391] Loss_D1: 0.2096 Loss_D2: 0.5352 Loss_G: 5.0933\n",
            "[22/25][28/391] Loss_D1: 0.4926 Loss_D2: 0.5410 Loss_G: 4.8870\n",
            "[22/25][29/391] Loss_D1: 0.4252 Loss_D2: 0.4408 Loss_G: 6.0913\n",
            "[22/25][30/391] Loss_D1: 0.4291 Loss_D2: 1.1091 Loss_G: 3.9499\n",
            "[22/25][31/391] Loss_D1: 0.6716 Loss_D2: 1.3723 Loss_G: 6.1519\n",
            "[22/25][32/391] Loss_D1: 0.5612 Loss_D2: 0.4308 Loss_G: 6.4249\n",
            "[22/25][33/391] Loss_D1: 0.6576 Loss_D2: 0.4335 Loss_G: 2.4785\n",
            "[22/25][34/391] Loss_D1: 0.5100 Loss_D2: 1.1681 Loss_G: 9.9438\n",
            "[22/25][35/391] Loss_D1: 0.6555 Loss_D2: 1.8845 Loss_G: 2.0496\n",
            "[22/25][36/391] Loss_D1: 0.6762 Loss_D2: 1.4371 Loss_G: 8.0319\n",
            "[22/25][37/391] Loss_D1: 0.5868 Loss_D2: 0.7539 Loss_G: 4.3527\n",
            "[22/25][38/391] Loss_D1: 0.3593 Loss_D2: 0.3634 Loss_G: 5.3367\n",
            "[22/25][39/391] Loss_D1: 0.5055 Loss_D2: 0.4256 Loss_G: 5.1453\n",
            "[22/25][40/391] Loss_D1: 0.5529 Loss_D2: 0.4195 Loss_G: 4.7633\n",
            "[22/25][41/391] Loss_D1: 0.4777 Loss_D2: 0.4134 Loss_G: 6.6036\n",
            "[22/25][42/391] Loss_D1: 0.6536 Loss_D2: 0.4811 Loss_G: 4.6569\n",
            "[22/25][43/391] Loss_D1: 0.3964 Loss_D2: 0.6284 Loss_G: 5.1010\n",
            "[22/25][44/391] Loss_D1: 0.3843 Loss_D2: 0.5758 Loss_G: 5.4034\n",
            "[22/25][45/391] Loss_D1: 0.4958 Loss_D2: 0.5311 Loss_G: 5.4835\n",
            "[22/25][46/391] Loss_D1: 0.3562 Loss_D2: 0.7409 Loss_G: 3.8213\n",
            "[22/25][47/391] Loss_D1: 0.3824 Loss_D2: 0.8850 Loss_G: 8.8404\n",
            "[22/25][48/391] Loss_D1: 0.4750 Loss_D2: 0.8409 Loss_G: 3.3899\n",
            "[22/25][49/391] Loss_D1: 0.5252 Loss_D2: 0.7138 Loss_G: 6.9992\n",
            "[22/25][50/391] Loss_D1: 0.5215 Loss_D2: 0.3445 Loss_G: 5.3641\n",
            "[22/25][51/391] Loss_D1: 0.4290 Loss_D2: 0.4399 Loss_G: 5.2680\n",
            "[22/25][52/391] Loss_D1: 0.4234 Loss_D2: 0.5404 Loss_G: 4.9555\n",
            "[22/25][53/391] Loss_D1: 0.3289 Loss_D2: 0.5510 Loss_G: 4.6848\n",
            "[22/25][54/391] Loss_D1: 0.3070 Loss_D2: 0.4869 Loss_G: 6.7726\n",
            "[22/25][55/391] Loss_D1: 0.4168 Loss_D2: 0.4840 Loss_G: 4.7910\n",
            "[22/25][56/391] Loss_D1: 0.3756 Loss_D2: 0.4572 Loss_G: 5.4255\n",
            "[22/25][57/391] Loss_D1: 0.3828 Loss_D2: 0.5757 Loss_G: 6.5829\n",
            "[22/25][58/391] Loss_D1: 0.3293 Loss_D2: 0.9189 Loss_G: 5.0831\n",
            "[22/25][59/391] Loss_D1: 0.7007 Loss_D2: 0.5837 Loss_G: 4.2853\n",
            "[22/25][60/391] Loss_D1: 1.0120 Loss_D2: 0.3052 Loss_G: 8.8953\n",
            "[22/25][61/391] Loss_D1: 0.7886 Loss_D2: 0.5077 Loss_G: 2.5196\n",
            "[22/25][62/391] Loss_D1: 1.4398 Loss_D2: 0.4011 Loss_G: 10.3414\n",
            "[22/25][63/391] Loss_D1: 1.5357 Loss_D2: 0.3367 Loss_G: 4.3029\n",
            "[22/25][64/391] Loss_D1: 2.4210 Loss_D2: 0.4141 Loss_G: 9.4422\n",
            "[22/25][65/391] Loss_D1: 2.6709 Loss_D2: 0.4605 Loss_G: 1.8027\n",
            "[22/25][66/391] Loss_D1: 1.8239 Loss_D2: 0.6847 Loss_G: 8.9793\n",
            "[22/25][67/391] Loss_D1: 1.0603 Loss_D2: 0.4581 Loss_G: 4.2118\n",
            "[22/25][68/391] Loss_D1: 0.7847 Loss_D2: 0.4828 Loss_G: 4.6444\n",
            "[22/25][69/391] Loss_D1: 0.5986 Loss_D2: 0.5132 Loss_G: 8.2240\n",
            "[22/25][70/391] Loss_D1: 1.3434 Loss_D2: 0.4886 Loss_G: 2.6047\n",
            "[22/25][71/391] Loss_D1: 2.3399 Loss_D2: 0.7340 Loss_G: 9.0942\n",
            "[22/25][72/391] Loss_D1: 1.5557 Loss_D2: 0.5670 Loss_G: 2.6632\n",
            "[22/25][73/391] Loss_D1: 1.5459 Loss_D2: 0.6087 Loss_G: 7.9309\n",
            "[22/25][74/391] Loss_D1: 0.9621 Loss_D2: 0.5082 Loss_G: 4.0504\n",
            "[22/25][75/391] Loss_D1: 0.7231 Loss_D2: 0.5504 Loss_G: 5.7374\n",
            "[22/25][76/391] Loss_D1: 0.7152 Loss_D2: 0.4488 Loss_G: 5.0199\n",
            "[22/25][77/391] Loss_D1: 0.4745 Loss_D2: 0.3612 Loss_G: 6.7096\n",
            "[22/25][78/391] Loss_D1: 0.3669 Loss_D2: 0.3799 Loss_G: 5.3693\n",
            "[22/25][79/391] Loss_D1: 0.3533 Loss_D2: 0.4490 Loss_G: 5.2280\n",
            "[22/25][80/391] Loss_D1: 0.5012 Loss_D2: 0.3927 Loss_G: 5.2495\n",
            "[22/25][81/391] Loss_D1: 0.3202 Loss_D2: 0.3462 Loss_G: 6.5684\n",
            "[22/25][82/391] Loss_D1: 0.5871 Loss_D2: 0.3340 Loss_G: 4.5967\n",
            "[22/25][83/391] Loss_D1: 0.4636 Loss_D2: 0.4297 Loss_G: 5.6346\n",
            "[22/25][84/391] Loss_D1: 0.4292 Loss_D2: 0.3968 Loss_G: 6.2998\n",
            "[22/25][85/391] Loss_D1: 0.6044 Loss_D2: 0.4222 Loss_G: 4.5246\n",
            "[22/25][86/391] Loss_D1: 0.4814 Loss_D2: 0.5038 Loss_G: 5.5575\n",
            "[22/25][87/391] Loss_D1: 0.4171 Loss_D2: 0.4073 Loss_G: 5.5307\n",
            "[22/25][88/391] Loss_D1: 0.5009 Loss_D2: 0.5193 Loss_G: 4.2375\n",
            "[22/25][89/391] Loss_D1: 0.4576 Loss_D2: 0.4946 Loss_G: 6.3789\n",
            "[22/25][90/391] Loss_D1: 0.4469 Loss_D2: 0.4007 Loss_G: 5.0851\n",
            "[22/25][91/391] Loss_D1: 0.5561 Loss_D2: 0.3500 Loss_G: 5.5521\n",
            "[22/25][92/391] Loss_D1: 0.5681 Loss_D2: 0.3521 Loss_G: 7.3729\n",
            "[22/25][93/391] Loss_D1: 1.1369 Loss_D2: 0.3539 Loss_G: 2.7311\n",
            "[22/25][94/391] Loss_D1: 1.2363 Loss_D2: 0.5366 Loss_G: 8.5759\n",
            "[22/25][95/391] Loss_D1: 0.8549 Loss_D2: 0.4406 Loss_G: 2.5534\n",
            "[22/25][96/391] Loss_D1: 1.1275 Loss_D2: 0.6334 Loss_G: 10.2878\n",
            "[22/25][97/391] Loss_D1: 1.4618 Loss_D2: 0.7418 Loss_G: 2.6520\n",
            "[22/25][98/391] Loss_D1: 0.5608 Loss_D2: 0.9505 Loss_G: 7.7487\n",
            "[22/25][99/391] Loss_D1: 0.4340 Loss_D2: 0.5259 Loss_G: 5.5281\n",
            "[22/25][100/391] Loss_D1: 0.4981 Loss_D2: 0.4693 Loss_G: 4.6485\n",
            "saving the output\n",
            "[22/25][101/391] Loss_D1: 0.6391 Loss_D2: 0.5631 Loss_G: 5.9334\n",
            "[22/25][102/391] Loss_D1: 0.5563 Loss_D2: 0.3993 Loss_G: 6.0745\n",
            "[22/25][103/391] Loss_D1: 0.6162 Loss_D2: 0.3873 Loss_G: 5.2721\n",
            "[22/25][104/391] Loss_D1: 0.7106 Loss_D2: 0.4525 Loss_G: 5.7617\n",
            "[22/25][105/391] Loss_D1: 0.7246 Loss_D2: 0.5151 Loss_G: 4.1934\n",
            "[22/25][106/391] Loss_D1: 0.5074 Loss_D2: 0.3570 Loss_G: 6.8352\n",
            "[22/25][107/391] Loss_D1: 0.2547 Loss_D2: 0.6885 Loss_G: 4.3093\n",
            "[22/25][108/391] Loss_D1: 0.3022 Loss_D2: 1.5267 Loss_G: 7.2182\n",
            "[22/25][109/391] Loss_D1: 0.4288 Loss_D2: 0.7256 Loss_G: 4.2368\n",
            "[22/25][110/391] Loss_D1: 0.3470 Loss_D2: 0.6886 Loss_G: 5.9916\n",
            "[22/25][111/391] Loss_D1: 0.5412 Loss_D2: 0.4950 Loss_G: 5.0091\n",
            "[22/25][112/391] Loss_D1: 0.5905 Loss_D2: 0.5375 Loss_G: 4.0571\n",
            "[22/25][113/391] Loss_D1: 0.5835 Loss_D2: 0.7510 Loss_G: 8.3169\n",
            "[22/25][114/391] Loss_D1: 0.7913 Loss_D2: 1.2358 Loss_G: 2.2920\n",
            "[22/25][115/391] Loss_D1: 0.5316 Loss_D2: 1.1713 Loss_G: 7.3033\n",
            "[22/25][116/391] Loss_D1: 0.4138 Loss_D2: 0.7319 Loss_G: 6.0234\n",
            "[22/25][117/391] Loss_D1: 0.3975 Loss_D2: 0.5659 Loss_G: 4.5583\n",
            "[22/25][118/391] Loss_D1: 0.4101 Loss_D2: 0.5285 Loss_G: 5.4249\n",
            "[22/25][119/391] Loss_D1: 0.4339 Loss_D2: 0.7929 Loss_G: 3.9828\n",
            "[22/25][120/391] Loss_D1: 0.3917 Loss_D2: 0.6955 Loss_G: 7.4394\n",
            "[22/25][121/391] Loss_D1: 0.1791 Loss_D2: 0.5947 Loss_G: 5.7415\n",
            "[22/25][122/391] Loss_D1: 0.4209 Loss_D2: 0.5498 Loss_G: 5.4067\n",
            "[22/25][123/391] Loss_D1: 0.5022 Loss_D2: 0.3704 Loss_G: 6.3614\n",
            "[22/25][124/391] Loss_D1: 0.3989 Loss_D2: 0.4034 Loss_G: 4.4278\n",
            "[22/25][125/391] Loss_D1: 0.3032 Loss_D2: 0.4699 Loss_G: 5.5951\n",
            "[22/25][126/391] Loss_D1: 0.8066 Loss_D2: 0.4280 Loss_G: 4.9409\n",
            "[22/25][127/391] Loss_D1: 0.5099 Loss_D2: 0.6582 Loss_G: 5.8683\n",
            "[22/25][128/391] Loss_D1: 0.5917 Loss_D2: 0.3975 Loss_G: 5.1055\n",
            "[22/25][129/391] Loss_D1: 0.4635 Loss_D2: 0.4893 Loss_G: 5.0728\n",
            "[22/25][130/391] Loss_D1: 0.3757 Loss_D2: 0.4532 Loss_G: 7.7529\n",
            "[22/25][131/391] Loss_D1: 0.6351 Loss_D2: 0.5216 Loss_G: 3.9382\n",
            "[22/25][132/391] Loss_D1: 0.4407 Loss_D2: 0.5215 Loss_G: 5.9126\n",
            "[22/25][133/391] Loss_D1: 0.4979 Loss_D2: 0.7686 Loss_G: 3.9888\n",
            "[22/25][134/391] Loss_D1: 0.6107 Loss_D2: 0.7931 Loss_G: 5.8646\n",
            "[22/25][135/391] Loss_D1: 0.3337 Loss_D2: 0.4144 Loss_G: 4.4338\n",
            "[22/25][136/391] Loss_D1: 0.4440 Loss_D2: 0.4764 Loss_G: 5.8182\n",
            "[22/25][137/391] Loss_D1: 0.6212 Loss_D2: 0.6939 Loss_G: 6.0437\n",
            "[22/25][138/391] Loss_D1: 0.4218 Loss_D2: 1.0096 Loss_G: 3.5162\n",
            "[22/25][139/391] Loss_D1: 0.3486 Loss_D2: 1.2362 Loss_G: 7.7721\n",
            "[22/25][140/391] Loss_D1: 0.3179 Loss_D2: 0.9093 Loss_G: 4.5704\n",
            "[22/25][141/391] Loss_D1: 0.6289 Loss_D2: 0.7634 Loss_G: 4.4336\n",
            "[22/25][142/391] Loss_D1: 0.3421 Loss_D2: 0.3896 Loss_G: 5.8868\n",
            "[22/25][143/391] Loss_D1: 0.4477 Loss_D2: 0.7953 Loss_G: 3.0473\n",
            "[22/25][144/391] Loss_D1: 0.4614 Loss_D2: 0.9707 Loss_G: 7.6556\n",
            "[22/25][145/391] Loss_D1: 0.2772 Loss_D2: 0.7199 Loss_G: 5.1587\n",
            "[22/25][146/391] Loss_D1: 0.3364 Loss_D2: 0.4547 Loss_G: 5.1229\n",
            "[22/25][147/391] Loss_D1: 0.4543 Loss_D2: 0.5958 Loss_G: 4.2162\n",
            "[22/25][148/391] Loss_D1: 0.6211 Loss_D2: 0.4529 Loss_G: 7.9176\n",
            "[22/25][149/391] Loss_D1: 0.3105 Loss_D2: 0.8454 Loss_G: 4.1597\n",
            "[22/25][150/391] Loss_D1: 0.4133 Loss_D2: 1.3740 Loss_G: 6.1472\n",
            "[22/25][151/391] Loss_D1: 0.6707 Loss_D2: 0.4707 Loss_G: 7.5218\n",
            "[22/25][152/391] Loss_D1: 0.7686 Loss_D2: 0.6429 Loss_G: 2.1751\n",
            "[22/25][153/391] Loss_D1: 0.7546 Loss_D2: 1.0800 Loss_G: 9.5415\n",
            "[22/25][154/391] Loss_D1: 0.7908 Loss_D2: 1.3355 Loss_G: 1.8306\n",
            "[22/25][155/391] Loss_D1: 1.4211 Loss_D2: 1.0579 Loss_G: 10.6695\n",
            "[22/25][156/391] Loss_D1: 1.2407 Loss_D2: 0.7299 Loss_G: 2.7988\n",
            "[22/25][157/391] Loss_D1: 1.0268 Loss_D2: 0.6969 Loss_G: 7.5640\n",
            "[22/25][158/391] Loss_D1: 0.9373 Loss_D2: 0.4336 Loss_G: 4.1381\n",
            "[22/25][159/391] Loss_D1: 1.4334 Loss_D2: 0.5765 Loss_G: 6.3789\n",
            "[22/25][160/391] Loss_D1: 0.8280 Loss_D2: 0.8916 Loss_G: 7.0882\n",
            "[22/25][161/391] Loss_D1: 0.6859 Loss_D2: 1.3943 Loss_G: 1.9495\n",
            "[22/25][162/391] Loss_D1: 1.0710 Loss_D2: 0.8027 Loss_G: 8.4265\n",
            "[22/25][163/391] Loss_D1: 0.8966 Loss_D2: 0.4786 Loss_G: 5.2275\n",
            "[22/25][164/391] Loss_D1: 0.4965 Loss_D2: 0.3095 Loss_G: 6.2253\n",
            "[22/25][165/391] Loss_D1: 0.4306 Loss_D2: 0.4014 Loss_G: 4.9796\n",
            "[22/25][166/391] Loss_D1: 0.6678 Loss_D2: 0.6170 Loss_G: 4.2211\n",
            "[22/25][167/391] Loss_D1: 0.6786 Loss_D2: 0.4685 Loss_G: 7.7748\n",
            "[22/25][168/391] Loss_D1: 0.7872 Loss_D2: 0.4493 Loss_G: 4.8194\n",
            "[22/25][169/391] Loss_D1: 0.8471 Loss_D2: 0.6494 Loss_G: 4.9832\n",
            "[22/25][170/391] Loss_D1: 0.6972 Loss_D2: 1.1260 Loss_G: 7.5089\n",
            "[22/25][171/391] Loss_D1: 0.6385 Loss_D2: 1.2863 Loss_G: 5.0507\n",
            "[22/25][172/391] Loss_D1: 0.5509 Loss_D2: 0.7827 Loss_G: 5.2508\n",
            "[22/25][173/391] Loss_D1: 0.5304 Loss_D2: 0.4773 Loss_G: 6.6999\n",
            "[22/25][174/391] Loss_D1: 0.3453 Loss_D2: 0.5348 Loss_G: 4.8718\n",
            "[22/25][175/391] Loss_D1: 0.4470 Loss_D2: 0.6216 Loss_G: 4.9549\n",
            "[22/25][176/391] Loss_D1: 0.3788 Loss_D2: 0.5167 Loss_G: 5.4825\n",
            "[22/25][177/391] Loss_D1: 0.4766 Loss_D2: 0.4764 Loss_G: 5.5950\n",
            "[22/25][178/391] Loss_D1: 0.4964 Loss_D2: 0.4864 Loss_G: 5.3063\n",
            "[22/25][179/391] Loss_D1: 0.4432 Loss_D2: 0.6341 Loss_G: 4.2497\n",
            "[22/25][180/391] Loss_D1: 0.5221 Loss_D2: 0.5033 Loss_G: 6.7722\n",
            "[22/25][181/391] Loss_D1: 0.4341 Loss_D2: 0.4602 Loss_G: 5.1249\n",
            "[22/25][182/391] Loss_D1: 0.3241 Loss_D2: 0.3186 Loss_G: 5.4974\n",
            "[22/25][183/391] Loss_D1: 0.4102 Loss_D2: 0.3875 Loss_G: 4.8108\n",
            "[22/25][184/391] Loss_D1: 0.3334 Loss_D2: 0.4570 Loss_G: 6.8823\n",
            "[22/25][185/391] Loss_D1: 0.3869 Loss_D2: 0.3439 Loss_G: 5.2952\n",
            "[22/25][186/391] Loss_D1: 0.3269 Loss_D2: 0.4870 Loss_G: 4.5304\n",
            "[22/25][187/391] Loss_D1: 0.3842 Loss_D2: 0.5966 Loss_G: 6.8001\n",
            "[22/25][188/391] Loss_D1: 0.3969 Loss_D2: 0.8314 Loss_G: 3.3589\n",
            "[22/25][189/391] Loss_D1: 0.4520 Loss_D2: 0.8351 Loss_G: 7.1935\n",
            "[22/25][190/391] Loss_D1: 0.2788 Loss_D2: 0.6260 Loss_G: 5.0288\n",
            "[22/25][191/391] Loss_D1: 0.6511 Loss_D2: 0.3893 Loss_G: 3.8365\n",
            "[22/25][192/391] Loss_D1: 0.6399 Loss_D2: 0.4458 Loss_G: 7.0559\n",
            "[22/25][193/391] Loss_D1: 0.3671 Loss_D2: 0.5824 Loss_G: 4.7581\n",
            "[22/25][194/391] Loss_D1: 0.4594 Loss_D2: 0.4044 Loss_G: 4.8919\n",
            "[22/25][195/391] Loss_D1: 0.3733 Loss_D2: 0.3519 Loss_G: 6.1821\n",
            "[22/25][196/391] Loss_D1: 0.3436 Loss_D2: 0.4926 Loss_G: 5.4990\n",
            "[22/25][197/391] Loss_D1: 0.3214 Loss_D2: 0.2995 Loss_G: 5.3167\n",
            "[22/25][198/391] Loss_D1: 0.4019 Loss_D2: 0.5446 Loss_G: 5.0357\n",
            "[22/25][199/391] Loss_D1: 0.3346 Loss_D2: 0.3349 Loss_G: 6.0003\n",
            "[22/25][200/391] Loss_D1: 0.3914 Loss_D2: 0.4183 Loss_G: 5.7140\n",
            "saving the output\n",
            "[22/25][201/391] Loss_D1: 0.3676 Loss_D2: 0.4487 Loss_G: 4.5108\n",
            "[22/25][202/391] Loss_D1: 0.3912 Loss_D2: 0.5454 Loss_G: 4.3898\n",
            "[22/25][203/391] Loss_D1: 0.4918 Loss_D2: 0.4585 Loss_G: 5.8867\n",
            "[22/25][204/391] Loss_D1: 0.3751 Loss_D2: 0.4129 Loss_G: 5.3612\n",
            "[22/25][205/391] Loss_D1: 0.3896 Loss_D2: 0.3255 Loss_G: 5.3626\n",
            "[22/25][206/391] Loss_D1: 0.3056 Loss_D2: 0.3671 Loss_G: 5.5145\n",
            "[22/25][207/391] Loss_D1: 0.5336 Loss_D2: 0.3673 Loss_G: 4.6654\n",
            "[22/25][208/391] Loss_D1: 0.5509 Loss_D2: 0.3864 Loss_G: 6.9910\n",
            "[22/25][209/391] Loss_D1: 0.4655 Loss_D2: 0.3644 Loss_G: 4.8429\n",
            "[22/25][210/391] Loss_D1: 0.4262 Loss_D2: 0.4333 Loss_G: 4.7053\n",
            "[22/25][211/391] Loss_D1: 0.4195 Loss_D2: 0.4269 Loss_G: 6.2737\n",
            "[22/25][212/391] Loss_D1: 0.5373 Loss_D2: 0.3445 Loss_G: 4.0640\n",
            "[22/25][213/391] Loss_D1: 0.4763 Loss_D2: 0.3475 Loss_G: 6.4058\n",
            "[22/25][214/391] Loss_D1: 0.3659 Loss_D2: 0.3905 Loss_G: 5.6384\n",
            "[22/25][215/391] Loss_D1: 0.4287 Loss_D2: 0.4173 Loss_G: 4.5236\n",
            "[22/25][216/391] Loss_D1: 0.3713 Loss_D2: 0.3735 Loss_G: 7.2494\n",
            "[22/25][217/391] Loss_D1: 0.3333 Loss_D2: 0.4103 Loss_G: 4.6742\n",
            "[22/25][218/391] Loss_D1: 0.6273 Loss_D2: 0.4371 Loss_G: 4.1563\n",
            "[22/25][219/391] Loss_D1: 1.5893 Loss_D2: 0.4894 Loss_G: 7.0844\n",
            "[22/25][220/391] Loss_D1: 0.4689 Loss_D2: 0.4649 Loss_G: 3.0166\n",
            "[22/25][221/391] Loss_D1: 1.0594 Loss_D2: 0.4693 Loss_G: 8.0305\n",
            "[22/25][222/391] Loss_D1: 2.1870 Loss_D2: 0.4625 Loss_G: 4.0262\n",
            "[22/25][223/391] Loss_D1: 1.9508 Loss_D2: 0.5513 Loss_G: 9.7150\n",
            "[22/25][224/391] Loss_D1: 2.0885 Loss_D2: 0.4868 Loss_G: 3.4660\n",
            "[22/25][225/391] Loss_D1: 0.8364 Loss_D2: 0.4118 Loss_G: 5.9154\n",
            "[22/25][226/391] Loss_D1: 0.3413 Loss_D2: 0.3128 Loss_G: 6.8499\n",
            "[22/25][227/391] Loss_D1: 0.7410 Loss_D2: 0.3540 Loss_G: 4.0055\n",
            "[22/25][228/391] Loss_D1: 1.1307 Loss_D2: 0.6423 Loss_G: 7.3999\n",
            "[22/25][229/391] Loss_D1: 0.9450 Loss_D2: 0.6168 Loss_G: 5.7972\n",
            "[22/25][230/391] Loss_D1: 0.5183 Loss_D2: 0.8864 Loss_G: 3.2802\n",
            "[22/25][231/391] Loss_D1: 0.5607 Loss_D2: 0.8071 Loss_G: 8.3321\n",
            "[22/25][232/391] Loss_D1: 0.9146 Loss_D2: 0.5424 Loss_G: 3.4706\n",
            "[22/25][233/391] Loss_D1: 1.0233 Loss_D2: 0.4380 Loss_G: 7.0208\n",
            "[22/25][234/391] Loss_D1: 0.6640 Loss_D2: 0.4833 Loss_G: 5.3445\n",
            "[22/25][235/391] Loss_D1: 0.3688 Loss_D2: 0.4305 Loss_G: 5.5518\n",
            "[22/25][236/391] Loss_D1: 0.4000 Loss_D2: 0.5336 Loss_G: 4.2247\n",
            "[22/25][237/391] Loss_D1: 0.3468 Loss_D2: 0.4217 Loss_G: 6.3125\n",
            "[22/25][238/391] Loss_D1: 0.6784 Loss_D2: 0.3857 Loss_G: 4.4741\n",
            "[22/25][239/391] Loss_D1: 0.5747 Loss_D2: 0.3946 Loss_G: 6.7920\n",
            "[22/25][240/391] Loss_D1: 0.4705 Loss_D2: 0.5401 Loss_G: 4.4243\n",
            "[22/25][241/391] Loss_D1: 0.5646 Loss_D2: 0.4523 Loss_G: 4.6258\n",
            "[22/25][242/391] Loss_D1: 0.6580 Loss_D2: 0.5007 Loss_G: 6.3338\n",
            "[22/25][243/391] Loss_D1: 0.7213 Loss_D2: 0.4366 Loss_G: 3.6242\n",
            "[22/25][244/391] Loss_D1: 0.5313 Loss_D2: 0.6433 Loss_G: 6.3636\n",
            "[22/25][245/391] Loss_D1: 0.3702 Loss_D2: 0.4990 Loss_G: 6.0810\n",
            "[22/25][246/391] Loss_D1: 0.4864 Loss_D2: 0.3321 Loss_G: 4.9502\n",
            "[22/25][247/391] Loss_D1: 0.3461 Loss_D2: 0.3468 Loss_G: 5.9361\n",
            "[22/25][248/391] Loss_D1: 0.3673 Loss_D2: 0.4779 Loss_G: 5.4180\n",
            "[22/25][249/391] Loss_D1: 0.3872 Loss_D2: 0.3253 Loss_G: 5.5103\n",
            "[22/25][250/391] Loss_D1: 0.4582 Loss_D2: 0.3653 Loss_G: 5.1942\n",
            "[22/25][251/391] Loss_D1: 0.2862 Loss_D2: 0.3904 Loss_G: 5.2323\n",
            "[22/25][252/391] Loss_D1: 0.3224 Loss_D2: 0.4323 Loss_G: 6.8708\n",
            "[22/25][253/391] Loss_D1: 0.2961 Loss_D2: 0.3417 Loss_G: 6.0770\n",
            "[22/25][254/391] Loss_D1: 0.5421 Loss_D2: 0.5192 Loss_G: 3.6238\n",
            "[22/25][255/391] Loss_D1: 0.5113 Loss_D2: 0.4592 Loss_G: 6.0796\n",
            "[22/25][256/391] Loss_D1: 0.4659 Loss_D2: 0.3534 Loss_G: 5.1819\n",
            "[22/25][257/391] Loss_D1: 0.5351 Loss_D2: 0.4355 Loss_G: 5.3256\n",
            "[22/25][258/391] Loss_D1: 0.4023 Loss_D2: 0.3565 Loss_G: 5.3975\n",
            "[22/25][259/391] Loss_D1: 0.4374 Loss_D2: 0.4156 Loss_G: 4.9056\n",
            "[22/25][260/391] Loss_D1: 0.5561 Loss_D2: 0.4392 Loss_G: 4.6182\n",
            "[22/25][261/391] Loss_D1: 0.3826 Loss_D2: 0.4880 Loss_G: 6.8645\n",
            "[22/25][262/391] Loss_D1: 0.6228 Loss_D2: 0.8470 Loss_G: 2.6573\n",
            "[22/25][263/391] Loss_D1: 0.9384 Loss_D2: 0.9188 Loss_G: 8.9895\n",
            "[22/25][264/391] Loss_D1: 0.5243 Loss_D2: 0.4182 Loss_G: 4.2624\n",
            "[22/25][265/391] Loss_D1: 0.5619 Loss_D2: 0.6282 Loss_G: 8.3128\n",
            "[22/25][266/391] Loss_D1: 0.4252 Loss_D2: 1.2827 Loss_G: 4.6546\n",
            "[22/25][267/391] Loss_D1: 0.5002 Loss_D2: 0.8007 Loss_G: 5.3878\n",
            "[22/25][268/391] Loss_D1: 0.4824 Loss_D2: 0.3702 Loss_G: 6.2437\n",
            "[22/25][269/391] Loss_D1: 0.3371 Loss_D2: 0.5986 Loss_G: 4.0543\n",
            "[22/25][270/391] Loss_D1: 0.7233 Loss_D2: 0.9551 Loss_G: 6.1781\n",
            "[22/25][271/391] Loss_D1: 1.0315 Loss_D2: 0.8663 Loss_G: 5.8845\n",
            "[22/25][272/391] Loss_D1: 1.3490 Loss_D2: 0.5774 Loss_G: 3.4172\n",
            "[22/25][273/391] Loss_D1: 2.1893 Loss_D2: 0.5624 Loss_G: 8.4129\n",
            "[22/25][274/391] Loss_D1: 1.3040 Loss_D2: 0.4123 Loss_G: 2.4467\n",
            "[22/25][275/391] Loss_D1: 3.1006 Loss_D2: 0.4296 Loss_G: 11.4226\n",
            "[22/25][276/391] Loss_D1: 2.7867 Loss_D2: 0.6146 Loss_G: 2.2995\n",
            "[22/25][277/391] Loss_D1: 1.1550 Loss_D2: 0.6039 Loss_G: 4.8969\n",
            "[22/25][278/391] Loss_D1: 0.9986 Loss_D2: 0.4058 Loss_G: 7.4957\n",
            "[22/25][279/391] Loss_D1: 1.4913 Loss_D2: 0.5980 Loss_G: 2.3899\n",
            "[22/25][280/391] Loss_D1: 1.2967 Loss_D2: 0.6787 Loss_G: 8.8944\n",
            "[22/25][281/391] Loss_D1: 0.8485 Loss_D2: 0.6855 Loss_G: 3.5736\n",
            "[22/25][282/391] Loss_D1: 0.8444 Loss_D2: 0.5018 Loss_G: 6.2235\n",
            "[22/25][283/391] Loss_D1: 0.6627 Loss_D2: 0.4246 Loss_G: 6.1681\n",
            "[22/25][284/391] Loss_D1: 0.3415 Loss_D2: 0.4520 Loss_G: 4.6049\n",
            "[22/25][285/391] Loss_D1: 0.6870 Loss_D2: 0.4652 Loss_G: 6.7065\n",
            "[22/25][286/391] Loss_D1: 0.8458 Loss_D2: 0.3994 Loss_G: 4.9366\n",
            "[22/25][287/391] Loss_D1: 0.6608 Loss_D2: 0.5216 Loss_G: 5.1585\n",
            "[22/25][288/391] Loss_D1: 0.4765 Loss_D2: 0.4215 Loss_G: 5.3627\n",
            "[22/25][289/391] Loss_D1: 0.4558 Loss_D2: 0.4149 Loss_G: 4.6514\n",
            "[22/25][290/391] Loss_D1: 0.5804 Loss_D2: 0.4628 Loss_G: 4.2100\n",
            "[22/25][291/391] Loss_D1: 0.7798 Loss_D2: 0.5472 Loss_G: 8.0310\n",
            "[22/25][292/391] Loss_D1: 0.8193 Loss_D2: 0.4890 Loss_G: 4.0573\n",
            "[22/25][293/391] Loss_D1: 0.6162 Loss_D2: 0.3407 Loss_G: 6.1237\n",
            "[22/25][294/391] Loss_D1: 0.4184 Loss_D2: 0.3090 Loss_G: 6.4343\n",
            "[22/25][295/391] Loss_D1: 0.5992 Loss_D2: 0.3993 Loss_G: 3.5745\n",
            "[22/25][296/391] Loss_D1: 0.6571 Loss_D2: 0.3146 Loss_G: 6.8238\n",
            "[22/25][297/391] Loss_D1: 0.5270 Loss_D2: 0.3806 Loss_G: 6.0550\n",
            "[22/25][298/391] Loss_D1: 0.4447 Loss_D2: 0.8637 Loss_G: 4.0382\n",
            "[22/25][299/391] Loss_D1: 0.6730 Loss_D2: 0.9420 Loss_G: 5.8610\n",
            "[22/25][300/391] Loss_D1: 0.8010 Loss_D2: 0.5463 Loss_G: 6.7180\n",
            "saving the output\n",
            "[22/25][301/391] Loss_D1: 0.6750 Loss_D2: 0.3769 Loss_G: 3.7745\n",
            "[22/25][302/391] Loss_D1: 0.4453 Loss_D2: 0.7675 Loss_G: 7.5445\n",
            "[22/25][303/391] Loss_D1: 0.5952 Loss_D2: 0.7547 Loss_G: 3.0306\n",
            "[22/25][304/391] Loss_D1: 0.3853 Loss_D2: 1.0006 Loss_G: 9.0421\n",
            "[22/25][305/391] Loss_D1: 0.3935 Loss_D2: 1.1568 Loss_G: 4.5517\n",
            "[22/25][306/391] Loss_D1: 0.6500 Loss_D2: 0.5043 Loss_G: 4.5108\n",
            "[22/25][307/391] Loss_D1: 0.6772 Loss_D2: 0.3755 Loss_G: 6.8008\n",
            "[22/25][308/391] Loss_D1: 0.3796 Loss_D2: 0.7973 Loss_G: 4.4261\n",
            "[22/25][309/391] Loss_D1: 0.3903 Loss_D2: 0.6829 Loss_G: 6.4661\n",
            "[22/25][310/391] Loss_D1: 0.3845 Loss_D2: 0.4375 Loss_G: 5.3087\n",
            "[22/25][311/391] Loss_D1: 0.2945 Loss_D2: 0.4117 Loss_G: 5.7510\n",
            "[22/25][312/391] Loss_D1: 0.4463 Loss_D2: 0.4880 Loss_G: 5.9035\n",
            "[22/25][313/391] Loss_D1: 0.3923 Loss_D2: 0.5000 Loss_G: 5.1723\n",
            "[22/25][314/391] Loss_D1: 0.2765 Loss_D2: 0.5444 Loss_G: 5.3827\n",
            "[22/25][315/391] Loss_D1: 0.3695 Loss_D2: 0.4476 Loss_G: 5.1380\n",
            "[22/25][316/391] Loss_D1: 0.4452 Loss_D2: 0.4324 Loss_G: 4.8679\n",
            "[22/25][317/391] Loss_D1: 0.3088 Loss_D2: 0.4024 Loss_G: 5.9486\n",
            "[22/25][318/391] Loss_D1: 0.3383 Loss_D2: 0.2431 Loss_G: 6.1010\n",
            "[22/25][319/391] Loss_D1: 0.5174 Loss_D2: 0.4621 Loss_G: 5.4167\n",
            "[22/25][320/391] Loss_D1: 0.5908 Loss_D2: 0.3929 Loss_G: 5.0588\n",
            "[22/25][321/391] Loss_D1: 0.3007 Loss_D2: 0.4372 Loss_G: 5.1539\n",
            "[22/25][322/391] Loss_D1: 0.3896 Loss_D2: 0.3452 Loss_G: 5.8785\n",
            "[22/25][323/391] Loss_D1: 0.4841 Loss_D2: 0.4369 Loss_G: 4.2432\n",
            "[22/25][324/391] Loss_D1: 0.6931 Loss_D2: 0.5149 Loss_G: 6.1231\n",
            "[22/25][325/391] Loss_D1: 0.5134 Loss_D2: 0.3658 Loss_G: 5.4814\n",
            "[22/25][326/391] Loss_D1: 0.3096 Loss_D2: 0.4290 Loss_G: 4.8426\n",
            "[22/25][327/391] Loss_D1: 0.3724 Loss_D2: 0.3766 Loss_G: 5.4782\n",
            "[22/25][328/391] Loss_D1: 0.2586 Loss_D2: 0.4201 Loss_G: 7.4043\n",
            "[22/25][329/391] Loss_D1: 1.0630 Loss_D2: 0.5571 Loss_G: 2.5915\n",
            "[22/25][330/391] Loss_D1: 1.2517 Loss_D2: 0.4122 Loss_G: 6.8811\n",
            "[22/25][331/391] Loss_D1: 0.3735 Loss_D2: 0.4728 Loss_G: 4.6354\n",
            "[22/25][332/391] Loss_D1: 0.5078 Loss_D2: 0.5623 Loss_G: 5.4560\n",
            "[22/25][333/391] Loss_D1: 0.6104 Loss_D2: 0.4844 Loss_G: 4.1905\n",
            "[22/25][334/391] Loss_D1: 0.6401 Loss_D2: 0.4339 Loss_G: 7.5122\n",
            "[22/25][335/391] Loss_D1: 0.3735 Loss_D2: 0.3909 Loss_G: 5.6270\n",
            "[22/25][336/391] Loss_D1: 0.6339 Loss_D2: 0.5049 Loss_G: 3.2913\n",
            "[22/25][337/391] Loss_D1: 0.6579 Loss_D2: 0.3762 Loss_G: 6.9064\n",
            "[22/25][338/391] Loss_D1: 0.6205 Loss_D2: 0.6310 Loss_G: 4.8589\n",
            "[22/25][339/391] Loss_D1: 0.4818 Loss_D2: 0.4513 Loss_G: 4.2295\n",
            "[22/25][340/391] Loss_D1: 0.4607 Loss_D2: 0.4169 Loss_G: 5.5169\n",
            "[22/25][341/391] Loss_D1: 0.2943 Loss_D2: 0.4389 Loss_G: 5.6845\n",
            "[22/25][342/391] Loss_D1: 0.1848 Loss_D2: 0.4392 Loss_G: 6.0373\n",
            "[22/25][343/391] Loss_D1: 0.6446 Loss_D2: 0.4262 Loss_G: 4.5925\n",
            "[22/25][344/391] Loss_D1: 0.6207 Loss_D2: 0.4515 Loss_G: 5.9099\n",
            "[22/25][345/391] Loss_D1: 0.4201 Loss_D2: 0.5889 Loss_G: 4.9125\n",
            "[22/25][346/391] Loss_D1: 0.2846 Loss_D2: 0.5610 Loss_G: 6.4173\n",
            "[22/25][347/391] Loss_D1: 0.3258 Loss_D2: 0.4767 Loss_G: 5.2553\n",
            "[22/25][348/391] Loss_D1: 0.3500 Loss_D2: 0.4043 Loss_G: 5.3691\n",
            "[22/25][349/391] Loss_D1: 0.6610 Loss_D2: 0.3542 Loss_G: 3.9508\n",
            "[22/25][350/391] Loss_D1: 0.4828 Loss_D2: 0.5504 Loss_G: 6.7487\n",
            "[22/25][351/391] Loss_D1: 0.4637 Loss_D2: 0.5086 Loss_G: 4.3811\n",
            "[22/25][352/391] Loss_D1: 0.3743 Loss_D2: 0.4337 Loss_G: 5.8282\n",
            "[22/25][353/391] Loss_D1: 0.3319 Loss_D2: 0.5075 Loss_G: 4.3156\n",
            "[22/25][354/391] Loss_D1: 0.3216 Loss_D2: 0.6687 Loss_G: 7.4657\n",
            "[22/25][355/391] Loss_D1: 0.2908 Loss_D2: 0.4390 Loss_G: 5.9657\n",
            "[22/25][356/391] Loss_D1: 0.5341 Loss_D2: 0.3574 Loss_G: 3.5378\n",
            "[22/25][357/391] Loss_D1: 0.4831 Loss_D2: 0.3796 Loss_G: 7.4838\n",
            "[22/25][358/391] Loss_D1: 0.2731 Loss_D2: 0.5487 Loss_G: 5.1780\n",
            "[22/25][359/391] Loss_D1: 0.4534 Loss_D2: 0.5440 Loss_G: 5.1455\n",
            "[22/25][360/391] Loss_D1: 0.3437 Loss_D2: 0.4945 Loss_G: 5.3587\n",
            "[22/25][361/391] Loss_D1: 0.3462 Loss_D2: 0.3611 Loss_G: 6.6983\n",
            "[22/25][362/391] Loss_D1: 0.2824 Loss_D2: 0.2462 Loss_G: 6.1705\n",
            "[22/25][363/391] Loss_D1: 0.5024 Loss_D2: 0.5025 Loss_G: 3.5725\n",
            "[22/25][364/391] Loss_D1: 0.5351 Loss_D2: 0.5826 Loss_G: 6.7835\n",
            "[22/25][365/391] Loss_D1: 0.6697 Loss_D2: 0.4609 Loss_G: 3.8153\n",
            "[22/25][366/391] Loss_D1: 0.5881 Loss_D2: 0.4303 Loss_G: 5.2089\n",
            "[22/25][367/391] Loss_D1: 0.3251 Loss_D2: 0.3681 Loss_G: 6.5080\n",
            "[22/25][368/391] Loss_D1: 0.4110 Loss_D2: 0.3384 Loss_G: 5.2442\n",
            "[22/25][369/391] Loss_D1: 0.4030 Loss_D2: 0.4627 Loss_G: 4.8474\n",
            "[22/25][370/391] Loss_D1: 0.4346 Loss_D2: 0.3884 Loss_G: 4.8695\n",
            "[22/25][371/391] Loss_D1: 0.4355 Loss_D2: 0.6724 Loss_G: 5.2710\n",
            "[22/25][372/391] Loss_D1: 0.3177 Loss_D2: 0.3631 Loss_G: 5.7789\n",
            "[22/25][373/391] Loss_D1: 0.3978 Loss_D2: 0.4259 Loss_G: 4.8859\n",
            "[22/25][374/391] Loss_D1: 0.3412 Loss_D2: 0.3502 Loss_G: 6.3712\n",
            "[22/25][375/391] Loss_D1: 0.3763 Loss_D2: 0.4349 Loss_G: 4.8907\n",
            "[22/25][376/391] Loss_D1: 0.3851 Loss_D2: 0.4204 Loss_G: 5.9685\n",
            "[22/25][377/391] Loss_D1: 0.3828 Loss_D2: 0.3597 Loss_G: 5.0453\n",
            "[22/25][378/391] Loss_D1: 0.3374 Loss_D2: 0.3464 Loss_G: 4.9020\n",
            "[22/25][379/391] Loss_D1: 0.3846 Loss_D2: 0.3778 Loss_G: 5.7975\n",
            "[22/25][380/391] Loss_D1: 0.4630 Loss_D2: 0.3721 Loss_G: 6.1778\n",
            "[22/25][381/391] Loss_D1: 0.4021 Loss_D2: 0.3711 Loss_G: 4.0988\n",
            "[22/25][382/391] Loss_D1: 0.3800 Loss_D2: 0.4440 Loss_G: 7.1441\n",
            "[22/25][383/391] Loss_D1: 0.4936 Loss_D2: 0.6232 Loss_G: 3.6719\n",
            "[22/25][384/391] Loss_D1: 0.4638 Loss_D2: 0.7840 Loss_G: 7.2360\n",
            "[22/25][385/391] Loss_D1: 0.3826 Loss_D2: 0.8485 Loss_G: 5.0056\n",
            "[22/25][386/391] Loss_D1: 0.5648 Loss_D2: 0.5846 Loss_G: 4.6933\n",
            "[22/25][387/391] Loss_D1: 0.5269 Loss_D2: 0.4169 Loss_G: 6.0656\n",
            "[22/25][388/391] Loss_D1: 0.3112 Loss_D2: 0.3904 Loss_G: 5.5606\n",
            "[22/25][389/391] Loss_D1: 0.3251 Loss_D2: 0.4184 Loss_G: 5.3341\n",
            "[22/25][390/391] Loss_D1: 0.5017 Loss_D2: 0.4311 Loss_G: 6.1232\n",
            "[23/25][0/391] Loss_D1: 0.2417 Loss_D2: 0.3575 Loss_G: 6.0320\n",
            "saving the output\n",
            "[23/25][1/391] Loss_D1: 0.3176 Loss_D2: 0.3594 Loss_G: 5.8191\n",
            "[23/25][2/391] Loss_D1: 0.3626 Loss_D2: 0.4600 Loss_G: 4.5281\n",
            "[23/25][3/391] Loss_D1: 0.4670 Loss_D2: 0.3929 Loss_G: 5.8307\n",
            "[23/25][4/391] Loss_D1: 0.4797 Loss_D2: 0.4556 Loss_G: 5.9467\n",
            "[23/25][5/391] Loss_D1: 0.4086 Loss_D2: 0.7562 Loss_G: 4.2212\n",
            "[23/25][6/391] Loss_D1: 0.3136 Loss_D2: 0.8890 Loss_G: 7.6943\n",
            "[23/25][7/391] Loss_D1: 0.4001 Loss_D2: 0.7510 Loss_G: 3.4525\n",
            "[23/25][8/391] Loss_D1: 0.3306 Loss_D2: 1.0962 Loss_G: 8.3920\n",
            "[23/25][9/391] Loss_D1: 0.3009 Loss_D2: 1.4857 Loss_G: 3.0437\n",
            "[23/25][10/391] Loss_D1: 0.4026 Loss_D2: 1.9371 Loss_G: 10.1248\n",
            "[23/25][11/391] Loss_D1: 0.4138 Loss_D2: 2.7057 Loss_G: 2.6262\n",
            "[23/25][12/391] Loss_D1: 0.2980 Loss_D2: 4.0848 Loss_G: 10.1333\n",
            "[23/25][13/391] Loss_D1: 0.4262 Loss_D2: 2.3786 Loss_G: 2.6535\n",
            "[23/25][14/391] Loss_D1: 0.4558 Loss_D2: 2.0503 Loss_G: 8.7404\n",
            "[23/25][15/391] Loss_D1: 0.4145 Loss_D2: 1.2791 Loss_G: 4.8948\n",
            "[23/25][16/391] Loss_D1: 0.3430 Loss_D2: 0.8184 Loss_G: 4.9452\n",
            "[23/25][17/391] Loss_D1: 0.4291 Loss_D2: 0.6575 Loss_G: 5.9500\n",
            "[23/25][18/391] Loss_D1: 0.3343 Loss_D2: 0.9845 Loss_G: 5.0397\n",
            "[23/25][19/391] Loss_D1: 0.3557 Loss_D2: 0.6333 Loss_G: 5.1256\n",
            "[23/25][20/391] Loss_D1: 0.5358 Loss_D2: 0.8831 Loss_G: 5.8084\n",
            "[23/25][21/391] Loss_D1: 0.3879 Loss_D2: 0.8115 Loss_G: 4.2455\n",
            "[23/25][22/391] Loss_D1: 0.3578 Loss_D2: 1.0684 Loss_G: 7.9790\n",
            "[23/25][23/391] Loss_D1: 0.4526 Loss_D2: 0.7341 Loss_G: 3.6869\n",
            "[23/25][24/391] Loss_D1: 0.4213 Loss_D2: 0.5515 Loss_G: 5.9665\n",
            "[23/25][25/391] Loss_D1: 0.3238 Loss_D2: 0.5400 Loss_G: 6.4750\n",
            "[23/25][26/391] Loss_D1: 0.4271 Loss_D2: 0.7071 Loss_G: 3.3167\n",
            "[23/25][27/391] Loss_D1: 0.5539 Loss_D2: 0.8719 Loss_G: 8.4746\n",
            "[23/25][28/391] Loss_D1: 0.5049 Loss_D2: 0.9882 Loss_G: 3.2262\n",
            "[23/25][29/391] Loss_D1: 0.4186 Loss_D2: 0.8266 Loss_G: 7.2573\n",
            "[23/25][30/391] Loss_D1: 0.4161 Loss_D2: 0.6322 Loss_G: 5.0252\n",
            "[23/25][31/391] Loss_D1: 0.3420 Loss_D2: 0.5899 Loss_G: 6.1779\n",
            "[23/25][32/391] Loss_D1: 0.5931 Loss_D2: 0.8578 Loss_G: 3.7253\n",
            "[23/25][33/391] Loss_D1: 0.3613 Loss_D2: 0.9763 Loss_G: 8.0605\n",
            "[23/25][34/391] Loss_D1: 0.3755 Loss_D2: 0.6182 Loss_G: 5.8715\n",
            "[23/25][35/391] Loss_D1: 0.5009 Loss_D2: 0.6114 Loss_G: 2.8922\n",
            "[23/25][36/391] Loss_D1: 0.5692 Loss_D2: 0.8861 Loss_G: 8.6458\n",
            "[23/25][37/391] Loss_D1: 0.3952 Loss_D2: 0.5510 Loss_G: 5.2792\n",
            "[23/25][38/391] Loss_D1: 0.2906 Loss_D2: 0.5175 Loss_G: 5.1819\n",
            "[23/25][39/391] Loss_D1: 0.2733 Loss_D2: 0.4034 Loss_G: 6.4014\n",
            "[23/25][40/391] Loss_D1: 0.2767 Loss_D2: 0.6341 Loss_G: 5.1116\n",
            "[23/25][41/391] Loss_D1: 0.2429 Loss_D2: 0.6021 Loss_G: 5.3349\n",
            "[23/25][42/391] Loss_D1: 0.2957 Loss_D2: 0.5731 Loss_G: 5.5485\n",
            "[23/25][43/391] Loss_D1: 0.2626 Loss_D2: 0.4923 Loss_G: 6.6813\n",
            "[23/25][44/391] Loss_D1: 0.3400 Loss_D2: 0.3808 Loss_G: 5.1676\n",
            "[23/25][45/391] Loss_D1: 0.3185 Loss_D2: 0.3898 Loss_G: 5.4029\n",
            "[23/25][46/391] Loss_D1: 0.3102 Loss_D2: 0.5785 Loss_G: 4.3808\n",
            "[23/25][47/391] Loss_D1: 0.4382 Loss_D2: 0.5216 Loss_G: 6.9060\n",
            "[23/25][48/391] Loss_D1: 0.4497 Loss_D2: 0.4211 Loss_G: 4.9202\n",
            "[23/25][49/391] Loss_D1: 0.3652 Loss_D2: 0.3699 Loss_G: 4.7818\n",
            "[23/25][50/391] Loss_D1: 0.4184 Loss_D2: 0.3909 Loss_G: 6.4107\n",
            "[23/25][51/391] Loss_D1: 0.3005 Loss_D2: 0.3181 Loss_G: 6.1234\n",
            "[23/25][52/391] Loss_D1: 0.3362 Loss_D2: 0.4259 Loss_G: 4.4045\n",
            "[23/25][53/391] Loss_D1: 0.3013 Loss_D2: 0.4199 Loss_G: 5.9735\n",
            "[23/25][54/391] Loss_D1: 0.4179 Loss_D2: 0.3873 Loss_G: 6.3043\n",
            "[23/25][55/391] Loss_D1: 0.2471 Loss_D2: 0.4178 Loss_G: 5.6071\n",
            "[23/25][56/391] Loss_D1: 0.3431 Loss_D2: 0.3209 Loss_G: 5.6517\n",
            "[23/25][57/391] Loss_D1: 0.3833 Loss_D2: 0.2810 Loss_G: 4.5880\n",
            "[23/25][58/391] Loss_D1: 0.4450 Loss_D2: 0.5169 Loss_G: 7.0663\n",
            "[23/25][59/391] Loss_D1: 0.2179 Loss_D2: 0.6694 Loss_G: 5.3547\n",
            "[23/25][60/391] Loss_D1: 0.4264 Loss_D2: 0.5426 Loss_G: 5.2031\n",
            "[23/25][61/391] Loss_D1: 0.5772 Loss_D2: 0.3725 Loss_G: 6.3504\n",
            "[23/25][62/391] Loss_D1: 0.4007 Loss_D2: 0.4893 Loss_G: 3.8660\n",
            "[23/25][63/391] Loss_D1: 0.2429 Loss_D2: 0.5916 Loss_G: 6.8456\n",
            "[23/25][64/391] Loss_D1: 0.4095 Loss_D2: 0.3049 Loss_G: 5.4224\n",
            "[23/25][65/391] Loss_D1: 0.4150 Loss_D2: 0.5073 Loss_G: 4.3381\n",
            "[23/25][66/391] Loss_D1: 0.3364 Loss_D2: 0.5584 Loss_G: 6.5885\n",
            "[23/25][67/391] Loss_D1: 0.2860 Loss_D2: 0.3185 Loss_G: 6.0651\n",
            "[23/25][68/391] Loss_D1: 0.3068 Loss_D2: 0.4688 Loss_G: 4.0229\n",
            "[23/25][69/391] Loss_D1: 0.2806 Loss_D2: 0.4581 Loss_G: 6.2061\n",
            "[23/25][70/391] Loss_D1: 0.3174 Loss_D2: 0.4046 Loss_G: 6.4290\n",
            "[23/25][71/391] Loss_D1: 0.4028 Loss_D2: 0.3190 Loss_G: 4.8196\n",
            "[23/25][72/391] Loss_D1: 0.2396 Loss_D2: 0.3165 Loss_G: 6.0429\n",
            "[23/25][73/391] Loss_D1: 0.2678 Loss_D2: 0.4718 Loss_G: 5.6786\n",
            "[23/25][74/391] Loss_D1: 0.4826 Loss_D2: 0.3319 Loss_G: 4.0132\n",
            "[23/25][75/391] Loss_D1: 0.6063 Loss_D2: 0.4497 Loss_G: 7.0947\n",
            "[23/25][76/391] Loss_D1: 0.2351 Loss_D2: 0.4909 Loss_G: 5.9512\n",
            "[23/25][77/391] Loss_D1: 0.3557 Loss_D2: 0.4439 Loss_G: 4.7847\n",
            "[23/25][78/391] Loss_D1: 0.4711 Loss_D2: 0.4066 Loss_G: 5.7511\n",
            "[23/25][79/391] Loss_D1: 0.2470 Loss_D2: 0.3533 Loss_G: 5.9580\n",
            "[23/25][80/391] Loss_D1: 0.2424 Loss_D2: 0.4350 Loss_G: 5.6057\n",
            "[23/25][81/391] Loss_D1: 0.2489 Loss_D2: 0.3078 Loss_G: 5.9363\n",
            "[23/25][82/391] Loss_D1: 0.3319 Loss_D2: 0.3503 Loss_G: 4.8303\n",
            "[23/25][83/391] Loss_D1: 0.2924 Loss_D2: 0.2970 Loss_G: 6.8507\n",
            "[23/25][84/391] Loss_D1: 0.3750 Loss_D2: 0.3299 Loss_G: 5.0019\n",
            "[23/25][85/391] Loss_D1: 0.3947 Loss_D2: 0.2834 Loss_G: 5.0669\n",
            "[23/25][86/391] Loss_D1: 0.2343 Loss_D2: 0.3813 Loss_G: 5.8665\n",
            "[23/25][87/391] Loss_D1: 0.4039 Loss_D2: 0.2764 Loss_G: 4.5489\n",
            "[23/25][88/391] Loss_D1: 0.4298 Loss_D2: 0.4376 Loss_G: 6.8757\n",
            "[23/25][89/391] Loss_D1: 0.5756 Loss_D2: 0.4535 Loss_G: 3.7058\n",
            "[23/25][90/391] Loss_D1: 0.5297 Loss_D2: 0.4295 Loss_G: 7.1656\n",
            "[23/25][91/391] Loss_D1: 0.5319 Loss_D2: 0.2857 Loss_G: 4.6793\n",
            "[23/25][92/391] Loss_D1: 0.4233 Loss_D2: 0.4603 Loss_G: 5.7326\n",
            "[23/25][93/391] Loss_D1: 0.3101 Loss_D2: 0.5169 Loss_G: 5.9452\n",
            "[23/25][94/391] Loss_D1: 0.3778 Loss_D2: 0.3864 Loss_G: 5.7649\n",
            "[23/25][95/391] Loss_D1: 0.2953 Loss_D2: 0.3110 Loss_G: 5.1377\n",
            "[23/25][96/391] Loss_D1: 0.3316 Loss_D2: 0.5450 Loss_G: 6.1401\n",
            "[23/25][97/391] Loss_D1: 0.4148 Loss_D2: 0.4506 Loss_G: 5.1642\n",
            "[23/25][98/391] Loss_D1: 0.3009 Loss_D2: 0.3045 Loss_G: 5.0283\n",
            "[23/25][99/391] Loss_D1: 0.4381 Loss_D2: 0.4403 Loss_G: 6.4409\n",
            "[23/25][100/391] Loss_D1: 0.3746 Loss_D2: 0.5727 Loss_G: 3.6803\n",
            "saving the output\n",
            "[23/25][101/391] Loss_D1: 0.6621 Loss_D2: 0.3801 Loss_G: 6.2805\n",
            "[23/25][102/391] Loss_D1: 0.2605 Loss_D2: 0.3363 Loss_G: 5.6514\n",
            "[23/25][103/391] Loss_D1: 0.5466 Loss_D2: 0.3367 Loss_G: 6.3682\n",
            "[23/25][104/391] Loss_D1: 0.4209 Loss_D2: 0.3180 Loss_G: 4.5613\n",
            "[23/25][105/391] Loss_D1: 0.3571 Loss_D2: 0.3510 Loss_G: 5.2695\n",
            "[23/25][106/391] Loss_D1: 0.3195 Loss_D2: 0.4353 Loss_G: 5.4094\n",
            "[23/25][107/391] Loss_D1: 0.5362 Loss_D2: 0.4355 Loss_G: 4.0607\n",
            "[23/25][108/391] Loss_D1: 0.2937 Loss_D2: 0.3760 Loss_G: 6.1663\n",
            "[23/25][109/391] Loss_D1: 0.3932 Loss_D2: 0.2944 Loss_G: 5.6078\n",
            "[23/25][110/391] Loss_D1: 0.3181 Loss_D2: 0.3494 Loss_G: 5.2289\n",
            "[23/25][111/391] Loss_D1: 0.2455 Loss_D2: 0.2842 Loss_G: 5.3991\n",
            "[23/25][112/391] Loss_D1: 0.2434 Loss_D2: 0.3331 Loss_G: 5.9434\n",
            "[23/25][113/391] Loss_D1: 0.2804 Loss_D2: 0.3271 Loss_G: 5.9463\n",
            "[23/25][114/391] Loss_D1: 0.2255 Loss_D2: 0.3817 Loss_G: 5.2918\n",
            "[23/25][115/391] Loss_D1: 0.2773 Loss_D2: 0.3627 Loss_G: 5.2397\n",
            "[23/25][116/391] Loss_D1: 0.3599 Loss_D2: 0.3725 Loss_G: 5.7170\n",
            "[23/25][117/391] Loss_D1: 0.2987 Loss_D2: 0.3204 Loss_G: 5.5990\n",
            "[23/25][118/391] Loss_D1: 0.2820 Loss_D2: 0.2687 Loss_G: 7.1304\n",
            "[23/25][119/391] Loss_D1: 0.4627 Loss_D2: 0.4003 Loss_G: 4.0609\n",
            "[23/25][120/391] Loss_D1: 0.5066 Loss_D2: 0.3535 Loss_G: 6.0791\n",
            "[23/25][121/391] Loss_D1: 0.5309 Loss_D2: 0.3972 Loss_G: 4.2499\n",
            "[23/25][122/391] Loss_D1: 0.3804 Loss_D2: 0.3163 Loss_G: 8.0540\n",
            "[23/25][123/391] Loss_D1: 0.2595 Loss_D2: 0.3447 Loss_G: 5.6614\n",
            "[23/25][124/391] Loss_D1: 0.3243 Loss_D2: 0.2583 Loss_G: 4.3883\n",
            "[23/25][125/391] Loss_D1: 0.6326 Loss_D2: 0.4576 Loss_G: 6.2548\n",
            "[23/25][126/391] Loss_D1: 0.4361 Loss_D2: 0.3866 Loss_G: 5.1103\n",
            "[23/25][127/391] Loss_D1: 0.6198 Loss_D2: 0.3269 Loss_G: 8.2673\n",
            "[23/25][128/391] Loss_D1: 0.6716 Loss_D2: 0.3999 Loss_G: 3.9451\n",
            "[23/25][129/391] Loss_D1: 0.4085 Loss_D2: 0.4868 Loss_G: 6.4884\n",
            "[23/25][130/391] Loss_D1: 0.3210 Loss_D2: 0.2667 Loss_G: 7.0317\n",
            "[23/25][131/391] Loss_D1: 0.3629 Loss_D2: 0.3130 Loss_G: 5.2592\n",
            "[23/25][132/391] Loss_D1: 0.3681 Loss_D2: 0.2694 Loss_G: 4.8529\n",
            "[23/25][133/391] Loss_D1: 0.4533 Loss_D2: 0.3413 Loss_G: 6.1318\n",
            "[23/25][134/391] Loss_D1: 0.7550 Loss_D2: 0.3614 Loss_G: 3.7814\n",
            "[23/25][135/391] Loss_D1: 0.8349 Loss_D2: 0.4092 Loss_G: 7.6254\n",
            "[23/25][136/391] Loss_D1: 0.6824 Loss_D2: 0.2844 Loss_G: 4.9969\n",
            "[23/25][137/391] Loss_D1: 0.4813 Loss_D2: 0.3221 Loss_G: 6.5633\n",
            "[23/25][138/391] Loss_D1: 0.6258 Loss_D2: 0.3365 Loss_G: 3.6359\n",
            "[23/25][139/391] Loss_D1: 0.7481 Loss_D2: 0.3434 Loss_G: 8.0111\n",
            "[23/25][140/391] Loss_D1: 1.0506 Loss_D2: 0.3809 Loss_G: 3.3858\n",
            "[23/25][141/391] Loss_D1: 0.8562 Loss_D2: 0.4451 Loss_G: 7.5215\n",
            "[23/25][142/391] Loss_D1: 0.7592 Loss_D2: 0.5222 Loss_G: 4.9388\n",
            "[23/25][143/391] Loss_D1: 0.9274 Loss_D2: 0.8033 Loss_G: 6.4580\n",
            "[23/25][144/391] Loss_D1: 0.5533 Loss_D2: 1.0183 Loss_G: 6.5681\n",
            "[23/25][145/391] Loss_D1: 0.6438 Loss_D2: 0.6654 Loss_G: 6.8394\n",
            "[23/25][146/391] Loss_D1: 0.9616 Loss_D2: 0.2736 Loss_G: 2.9269\n",
            "[23/25][147/391] Loss_D1: 1.2169 Loss_D2: 0.5449 Loss_G: 9.1968\n",
            "[23/25][148/391] Loss_D1: 1.0986 Loss_D2: 0.5230 Loss_G: 2.4579\n",
            "[23/25][149/391] Loss_D1: 1.6271 Loss_D2: 0.5080 Loss_G: 9.4559\n",
            "[23/25][150/391] Loss_D1: 1.1951 Loss_D2: 0.3716 Loss_G: 4.2500\n",
            "[23/25][151/391] Loss_D1: 1.0793 Loss_D2: 0.4990 Loss_G: 7.2443\n",
            "[23/25][152/391] Loss_D1: 1.3195 Loss_D2: 0.5200 Loss_G: 4.1614\n",
            "[23/25][153/391] Loss_D1: 1.6292 Loss_D2: 0.5762 Loss_G: 7.6599\n",
            "[23/25][154/391] Loss_D1: 1.2610 Loss_D2: 0.4827 Loss_G: 4.0081\n",
            "[23/25][155/391] Loss_D1: 1.0081 Loss_D2: 0.4362 Loss_G: 7.5238\n",
            "[23/25][156/391] Loss_D1: 0.7118 Loss_D2: 0.3843 Loss_G: 3.9544\n",
            "[23/25][157/391] Loss_D1: 0.5910 Loss_D2: 0.3655 Loss_G: 6.3416\n",
            "[23/25][158/391] Loss_D1: 0.5166 Loss_D2: 0.4097 Loss_G: 5.7273\n",
            "[23/25][159/391] Loss_D1: 0.3388 Loss_D2: 0.4164 Loss_G: 6.2748\n",
            "[23/25][160/391] Loss_D1: 0.8295 Loss_D2: 0.3241 Loss_G: 3.6752\n",
            "[23/25][161/391] Loss_D1: 0.8129 Loss_D2: 0.3722 Loss_G: 6.5610\n",
            "[23/25][162/391] Loss_D1: 0.3845 Loss_D2: 0.3790 Loss_G: 7.4689\n",
            "[23/25][163/391] Loss_D1: 0.5121 Loss_D2: 0.4930 Loss_G: 3.7044\n",
            "[23/25][164/391] Loss_D1: 0.6735 Loss_D2: 0.3859 Loss_G: 7.5529\n",
            "[23/25][165/391] Loss_D1: 0.5257 Loss_D2: 0.3501 Loss_G: 5.1396\n",
            "[23/25][166/391] Loss_D1: 0.3691 Loss_D2: 0.3157 Loss_G: 5.2679\n",
            "[23/25][167/391] Loss_D1: 0.4350 Loss_D2: 0.3693 Loss_G: 5.7922\n",
            "[23/25][168/391] Loss_D1: 0.4122 Loss_D2: 0.3777 Loss_G: 5.4570\n",
            "[23/25][169/391] Loss_D1: 0.4757 Loss_D2: 0.3957 Loss_G: 4.9734\n",
            "[23/25][170/391] Loss_D1: 0.3542 Loss_D2: 0.3558 Loss_G: 6.3379\n",
            "[23/25][171/391] Loss_D1: 0.3153 Loss_D2: 0.3223 Loss_G: 7.0853\n",
            "[23/25][172/391] Loss_D1: 0.3802 Loss_D2: 0.2909 Loss_G: 5.6718\n",
            "[23/25][173/391] Loss_D1: 0.2545 Loss_D2: 0.2994 Loss_G: 5.3122\n",
            "[23/25][174/391] Loss_D1: 0.3853 Loss_D2: 0.3629 Loss_G: 5.6329\n",
            "[23/25][175/391] Loss_D1: 0.5458 Loss_D2: 0.4429 Loss_G: 3.8908\n",
            "[23/25][176/391] Loss_D1: 0.4112 Loss_D2: 0.4180 Loss_G: 6.3466\n",
            "[23/25][177/391] Loss_D1: 0.2804 Loss_D2: 0.4380 Loss_G: 5.6986\n",
            "[23/25][178/391] Loss_D1: 0.2885 Loss_D2: 0.3353 Loss_G: 6.7905\n",
            "[23/25][179/391] Loss_D1: 0.1949 Loss_D2: 0.4021 Loss_G: 5.5361\n",
            "[23/25][180/391] Loss_D1: 0.2615 Loss_D2: 0.3891 Loss_G: 6.5749\n",
            "[23/25][181/391] Loss_D1: 0.3567 Loss_D2: 0.2756 Loss_G: 5.5554\n",
            "[23/25][182/391] Loss_D1: 0.4972 Loss_D2: 0.3107 Loss_G: 5.3994\n",
            "[23/25][183/391] Loss_D1: 0.3430 Loss_D2: 0.3826 Loss_G: 4.9354\n",
            "[23/25][184/391] Loss_D1: 0.3585 Loss_D2: 0.4724 Loss_G: 6.8753\n",
            "[23/25][185/391] Loss_D1: 0.3307 Loss_D2: 0.4389 Loss_G: 5.7013\n",
            "[23/25][186/391] Loss_D1: 0.2578 Loss_D2: 0.4161 Loss_G: 5.3226\n",
            "[23/25][187/391] Loss_D1: 0.3744 Loss_D2: 0.6992 Loss_G: 6.7608\n",
            "[23/25][188/391] Loss_D1: 0.4528 Loss_D2: 0.5305 Loss_G: 2.8965\n",
            "[23/25][189/391] Loss_D1: 0.4431 Loss_D2: 1.3762 Loss_G: 11.2713\n",
            "[23/25][190/391] Loss_D1: 0.4871 Loss_D2: 2.7399 Loss_G: 1.7232\n",
            "[23/25][191/391] Loss_D1: 0.6546 Loss_D2: 5.2686 Loss_G: 10.7848\n",
            "[23/25][192/391] Loss_D1: 0.7943 Loss_D2: 4.2060 Loss_G: 0.5309\n",
            "[23/25][193/391] Loss_D1: 1.6528 Loss_D2: 4.6400 Loss_G: 12.6095\n",
            "[23/25][194/391] Loss_D1: 1.8473 Loss_D2: 3.1873 Loss_G: 1.0280\n",
            "[23/25][195/391] Loss_D1: 1.1653 Loss_D2: 2.3652 Loss_G: 9.9405\n",
            "[23/25][196/391] Loss_D1: 1.2059 Loss_D2: 1.6331 Loss_G: 2.0609\n",
            "[23/25][197/391] Loss_D1: 1.6135 Loss_D2: 0.8033 Loss_G: 8.1941\n",
            "[23/25][198/391] Loss_D1: 0.9759 Loss_D2: 0.9565 Loss_G: 4.5973\n",
            "[23/25][199/391] Loss_D1: 0.9871 Loss_D2: 1.5084 Loss_G: 6.4581\n",
            "[23/25][200/391] Loss_D1: 1.4156 Loss_D2: 1.6589 Loss_G: 4.9831\n",
            "saving the output\n",
            "[23/25][201/391] Loss_D1: 1.1733 Loss_D2: 1.5753 Loss_G: 5.8918\n",
            "[23/25][202/391] Loss_D1: 0.9230 Loss_D2: 1.3534 Loss_G: 4.5884\n",
            "[23/25][203/391] Loss_D1: 1.4514 Loss_D2: 0.8674 Loss_G: 8.7042\n",
            "[23/25][204/391] Loss_D1: 2.9340 Loss_D2: 0.6181 Loss_G: 3.4103\n",
            "[23/25][205/391] Loss_D1: 1.2592 Loss_D2: 0.6717 Loss_G: 7.4389\n",
            "[23/25][206/391] Loss_D1: 1.1532 Loss_D2: 0.6624 Loss_G: 2.4654\n",
            "[23/25][207/391] Loss_D1: 1.4323 Loss_D2: 0.7867 Loss_G: 8.3279\n",
            "[23/25][208/391] Loss_D1: 0.9136 Loss_D2: 0.8194 Loss_G: 3.0606\n",
            "[23/25][209/391] Loss_D1: 0.6317 Loss_D2: 0.7414 Loss_G: 6.3144\n",
            "[23/25][210/391] Loss_D1: 0.8615 Loss_D2: 0.7553 Loss_G: 5.7596\n",
            "[23/25][211/391] Loss_D1: 1.8373 Loss_D2: 1.1656 Loss_G: 2.5671\n",
            "[23/25][212/391] Loss_D1: 0.9352 Loss_D2: 1.3913 Loss_G: 10.5987\n",
            "[23/25][213/391] Loss_D1: 0.9187 Loss_D2: 1.1912 Loss_G: 2.7262\n",
            "[23/25][214/391] Loss_D1: 0.6208 Loss_D2: 1.4745 Loss_G: 8.4673\n",
            "[23/25][215/391] Loss_D1: 0.6705 Loss_D2: 1.0654 Loss_G: 5.4851\n",
            "[23/25][216/391] Loss_D1: 0.3840 Loss_D2: 0.7358 Loss_G: 5.9981\n",
            "[23/25][217/391] Loss_D1: 0.5555 Loss_D2: 0.6959 Loss_G: 5.6539\n",
            "[23/25][218/391] Loss_D1: 0.5186 Loss_D2: 0.5992 Loss_G: 5.0818\n",
            "[23/25][219/391] Loss_D1: 0.7540 Loss_D2: 0.5403 Loss_G: 6.9223\n",
            "[23/25][220/391] Loss_D1: 1.7656 Loss_D2: 0.5775 Loss_G: 3.0810\n",
            "[23/25][221/391] Loss_D1: 1.5317 Loss_D2: 0.6409 Loss_G: 6.4524\n",
            "[23/25][222/391] Loss_D1: 0.6968 Loss_D2: 0.7639 Loss_G: 7.2500\n",
            "[23/25][223/391] Loss_D1: 0.3956 Loss_D2: 0.6996 Loss_G: 4.1214\n",
            "[23/25][224/391] Loss_D1: 0.5026 Loss_D2: 0.6017 Loss_G: 6.6948\n",
            "[23/25][225/391] Loss_D1: 0.7245 Loss_D2: 0.5194 Loss_G: 5.2469\n",
            "[23/25][226/391] Loss_D1: 0.4549 Loss_D2: 0.6592 Loss_G: 6.0540\n",
            "[23/25][227/391] Loss_D1: 0.4905 Loss_D2: 0.7210 Loss_G: 4.1350\n",
            "[23/25][228/391] Loss_D1: 0.5645 Loss_D2: 0.5929 Loss_G: 6.7261\n",
            "[23/25][229/391] Loss_D1: 0.3720 Loss_D2: 0.6077 Loss_G: 5.7567\n",
            "[23/25][230/391] Loss_D1: 0.2854 Loss_D2: 0.6006 Loss_G: 4.7558\n",
            "[23/25][231/391] Loss_D1: 0.3934 Loss_D2: 0.5188 Loss_G: 5.5695\n",
            "[23/25][232/391] Loss_D1: 0.5587 Loss_D2: 0.3392 Loss_G: 7.5801\n",
            "[23/25][233/391] Loss_D1: 0.5091 Loss_D2: 0.6544 Loss_G: 4.0583\n",
            "[23/25][234/391] Loss_D1: 0.3744 Loss_D2: 0.7430 Loss_G: 5.5115\n",
            "[23/25][235/391] Loss_D1: 0.4381 Loss_D2: 0.4017 Loss_G: 6.1873\n",
            "[23/25][236/391] Loss_D1: 0.3781 Loss_D2: 0.4744 Loss_G: 5.1859\n",
            "[23/25][237/391] Loss_D1: 0.3748 Loss_D2: 0.3951 Loss_G: 5.4687\n",
            "[23/25][238/391] Loss_D1: 0.6540 Loss_D2: 0.4376 Loss_G: 3.1348\n",
            "[23/25][239/391] Loss_D1: 0.9225 Loss_D2: 0.7966 Loss_G: 9.4187\n",
            "[23/25][240/391] Loss_D1: 0.5148 Loss_D2: 0.7412 Loss_G: 3.8858\n",
            "[23/25][241/391] Loss_D1: 0.7296 Loss_D2: 0.5999 Loss_G: 5.9120\n",
            "[23/25][242/391] Loss_D1: 0.5005 Loss_D2: 0.5526 Loss_G: 5.6045\n",
            "[23/25][243/391] Loss_D1: 0.2976 Loss_D2: 0.6310 Loss_G: 5.7681\n",
            "[23/25][244/391] Loss_D1: 0.5223 Loss_D2: 0.5630 Loss_G: 4.1479\n",
            "[23/25][245/391] Loss_D1: 0.4231 Loss_D2: 0.5215 Loss_G: 5.6168\n",
            "[23/25][246/391] Loss_D1: 0.4738 Loss_D2: 0.5552 Loss_G: 4.2313\n",
            "[23/25][247/391] Loss_D1: 0.6334 Loss_D2: 0.4367 Loss_G: 6.6277\n",
            "[23/25][248/391] Loss_D1: 0.4725 Loss_D2: 0.4261 Loss_G: 5.6886\n",
            "[23/25][249/391] Loss_D1: 0.4729 Loss_D2: 0.4977 Loss_G: 3.3975\n",
            "[23/25][250/391] Loss_D1: 0.4531 Loss_D2: 0.6217 Loss_G: 7.0341\n",
            "[23/25][251/391] Loss_D1: 0.5758 Loss_D2: 0.3239 Loss_G: 4.7259\n",
            "[23/25][252/391] Loss_D1: 0.8742 Loss_D2: 0.2430 Loss_G: 7.3591\n",
            "[23/25][253/391] Loss_D1: 0.9883 Loss_D2: 0.4380 Loss_G: 3.0685\n",
            "[23/25][254/391] Loss_D1: 0.9959 Loss_D2: 0.3696 Loss_G: 8.3017\n",
            "[23/25][255/391] Loss_D1: 0.7503 Loss_D2: 0.3461 Loss_G: 3.9985\n",
            "[23/25][256/391] Loss_D1: 0.9778 Loss_D2: 0.4895 Loss_G: 6.0251\n",
            "[23/25][257/391] Loss_D1: 1.2207 Loss_D2: 0.8838 Loss_G: 5.4266\n",
            "[23/25][258/391] Loss_D1: 0.7045 Loss_D2: 0.9165 Loss_G: 5.0812\n",
            "[23/25][259/391] Loss_D1: 0.6200 Loss_D2: 0.8874 Loss_G: 6.8229\n",
            "[23/25][260/391] Loss_D1: 0.3406 Loss_D2: 0.7382 Loss_G: 4.6248\n",
            "[23/25][261/391] Loss_D1: 0.4890 Loss_D2: 0.5176 Loss_G: 6.1616\n",
            "[23/25][262/391] Loss_D1: 0.3357 Loss_D2: 0.5216 Loss_G: 6.0473\n",
            "[23/25][263/391] Loss_D1: 0.3384 Loss_D2: 0.3497 Loss_G: 5.3862\n",
            "[23/25][264/391] Loss_D1: 0.3850 Loss_D2: 0.4695 Loss_G: 5.2419\n",
            "[23/25][265/391] Loss_D1: 0.4293 Loss_D2: 0.3982 Loss_G: 5.9248\n",
            "[23/25][266/391] Loss_D1: 0.6147 Loss_D2: 0.4612 Loss_G: 4.1284\n",
            "[23/25][267/391] Loss_D1: 0.5120 Loss_D2: 0.5413 Loss_G: 5.6040\n",
            "[23/25][268/391] Loss_D1: 0.5057 Loss_D2: 0.7828 Loss_G: 6.3794\n",
            "[23/25][269/391] Loss_D1: 0.3706 Loss_D2: 0.4977 Loss_G: 4.6850\n",
            "[23/25][270/391] Loss_D1: 0.3370 Loss_D2: 0.2674 Loss_G: 5.2994\n",
            "[23/25][271/391] Loss_D1: 0.3879 Loss_D2: 0.4272 Loss_G: 6.9877\n",
            "[23/25][272/391] Loss_D1: 0.3678 Loss_D2: 0.3569 Loss_G: 5.8630\n",
            "[23/25][273/391] Loss_D1: 0.4773 Loss_D2: 0.3099 Loss_G: 4.3571\n",
            "[23/25][274/391] Loss_D1: 0.4890 Loss_D2: 0.4739 Loss_G: 6.1677\n",
            "[23/25][275/391] Loss_D1: 0.2569 Loss_D2: 0.4859 Loss_G: 7.2198\n",
            "[23/25][276/391] Loss_D1: 0.5303 Loss_D2: 0.4671 Loss_G: 3.8630\n",
            "[23/25][277/391] Loss_D1: 0.5326 Loss_D2: 0.4175 Loss_G: 6.2001\n",
            "[23/25][278/391] Loss_D1: 0.4644 Loss_D2: 0.4979 Loss_G: 5.4434\n",
            "[23/25][279/391] Loss_D1: 0.2726 Loss_D2: 0.4262 Loss_G: 6.4667\n",
            "[23/25][280/391] Loss_D1: 0.6992 Loss_D2: 0.4297 Loss_G: 3.0385\n",
            "[23/25][281/391] Loss_D1: 0.6866 Loss_D2: 0.4892 Loss_G: 7.7226\n",
            "[23/25][282/391] Loss_D1: 0.4153 Loss_D2: 0.3461 Loss_G: 5.6285\n",
            "[23/25][283/391] Loss_D1: 0.3293 Loss_D2: 0.3856 Loss_G: 4.7427\n",
            "[23/25][284/391] Loss_D1: 0.4834 Loss_D2: 0.4846 Loss_G: 4.8829\n",
            "[23/25][285/391] Loss_D1: 0.6633 Loss_D2: 0.4389 Loss_G: 6.8142\n",
            "[23/25][286/391] Loss_D1: 0.6264 Loss_D2: 0.3934 Loss_G: 4.4006\n",
            "[23/25][287/391] Loss_D1: 0.4561 Loss_D2: 0.3859 Loss_G: 6.4158\n",
            "[23/25][288/391] Loss_D1: 0.4573 Loss_D2: 0.4633 Loss_G: 5.6029\n",
            "[23/25][289/391] Loss_D1: 0.5698 Loss_D2: 0.4320 Loss_G: 3.7120\n",
            "[23/25][290/391] Loss_D1: 0.5958 Loss_D2: 0.4578 Loss_G: 6.3621\n",
            "[23/25][291/391] Loss_D1: 0.5239 Loss_D2: 0.4101 Loss_G: 5.3176\n",
            "[23/25][292/391] Loss_D1: 0.3726 Loss_D2: 0.2702 Loss_G: 6.0000\n",
            "[23/25][293/391] Loss_D1: 0.3375 Loss_D2: 0.3687 Loss_G: 5.3385\n",
            "[23/25][294/391] Loss_D1: 0.3522 Loss_D2: 0.4825 Loss_G: 4.0180\n",
            "[23/25][295/391] Loss_D1: 0.3620 Loss_D2: 0.4534 Loss_G: 6.2496\n",
            "[23/25][296/391] Loss_D1: 0.3794 Loss_D2: 0.3991 Loss_G: 6.2120\n",
            "[23/25][297/391] Loss_D1: 0.4489 Loss_D2: 0.4928 Loss_G: 4.5969\n",
            "[23/25][298/391] Loss_D1: 0.3535 Loss_D2: 0.4219 Loss_G: 5.7286\n",
            "[23/25][299/391] Loss_D1: 0.3287 Loss_D2: 0.4698 Loss_G: 6.1601\n",
            "[23/25][300/391] Loss_D1: 0.2435 Loss_D2: 0.3643 Loss_G: 5.7172\n",
            "saving the output\n",
            "[23/25][301/391] Loss_D1: 0.2804 Loss_D2: 0.4295 Loss_G: 5.1158\n",
            "[23/25][302/391] Loss_D1: 0.3051 Loss_D2: 0.3968 Loss_G: 5.5619\n",
            "[23/25][303/391] Loss_D1: 0.3132 Loss_D2: 0.5076 Loss_G: 5.5810\n",
            "[23/25][304/391] Loss_D1: 0.3936 Loss_D2: 0.4404 Loss_G: 4.4331\n",
            "[23/25][305/391] Loss_D1: 0.4867 Loss_D2: 0.4227 Loss_G: 6.0492\n",
            "[23/25][306/391] Loss_D1: 0.3499 Loss_D2: 0.3922 Loss_G: 6.2025\n",
            "[23/25][307/391] Loss_D1: 0.6464 Loss_D2: 0.4286 Loss_G: 3.1652\n",
            "[23/25][308/391] Loss_D1: 0.5388 Loss_D2: 0.4484 Loss_G: 7.0808\n",
            "[23/25][309/391] Loss_D1: 0.2690 Loss_D2: 0.3665 Loss_G: 6.3277\n",
            "[23/25][310/391] Loss_D1: 0.2938 Loss_D2: 0.4805 Loss_G: 4.2300\n",
            "[23/25][311/391] Loss_D1: 0.3230 Loss_D2: 0.5204 Loss_G: 6.4570\n",
            "[23/25][312/391] Loss_D1: 0.4323 Loss_D2: 0.6603 Loss_G: 4.3330\n",
            "[23/25][313/391] Loss_D1: 0.2999 Loss_D2: 0.6603 Loss_G: 7.3844\n",
            "[23/25][314/391] Loss_D1: 0.5013 Loss_D2: 0.6128 Loss_G: 3.0928\n",
            "[23/25][315/391] Loss_D1: 0.5264 Loss_D2: 0.5654 Loss_G: 7.5922\n",
            "[23/25][316/391] Loss_D1: 0.3766 Loss_D2: 0.5354 Loss_G: 4.8221\n",
            "[23/25][317/391] Loss_D1: 0.4163 Loss_D2: 0.3952 Loss_G: 5.1562\n",
            "[23/25][318/391] Loss_D1: 0.4336 Loss_D2: 0.4722 Loss_G: 5.3085\n",
            "[23/25][319/391] Loss_D1: 0.2235 Loss_D2: 0.4264 Loss_G: 5.6870\n",
            "[23/25][320/391] Loss_D1: 0.3965 Loss_D2: 0.3971 Loss_G: 5.1057\n",
            "[23/25][321/391] Loss_D1: 0.2623 Loss_D2: 0.3497 Loss_G: 6.0069\n",
            "[23/25][322/391] Loss_D1: 0.2595 Loss_D2: 0.2881 Loss_G: 5.8310\n",
            "[23/25][323/391] Loss_D1: 0.3347 Loss_D2: 0.3266 Loss_G: 5.2418\n",
            "[23/25][324/391] Loss_D1: 0.2931 Loss_D2: 0.4484 Loss_G: 5.1365\n",
            "[23/25][325/391] Loss_D1: 0.3793 Loss_D2: 0.3464 Loss_G: 5.3914\n",
            "[23/25][326/391] Loss_D1: 0.2714 Loss_D2: 0.4179 Loss_G: 5.7537\n",
            "[23/25][327/391] Loss_D1: 0.2443 Loss_D2: 0.3119 Loss_G: 5.8438\n",
            "[23/25][328/391] Loss_D1: 0.3186 Loss_D2: 0.3414 Loss_G: 5.2285\n",
            "[23/25][329/391] Loss_D1: 0.4236 Loss_D2: 0.3166 Loss_G: 5.2052\n",
            "[23/25][330/391] Loss_D1: 0.2910 Loss_D2: 0.4474 Loss_G: 5.2516\n",
            "[23/25][331/391] Loss_D1: 0.2106 Loss_D2: 0.4405 Loss_G: 6.5526\n",
            "[23/25][332/391] Loss_D1: 0.3661 Loss_D2: 0.4109 Loss_G: 5.5470\n",
            "[23/25][333/391] Loss_D1: 0.2319 Loss_D2: 0.4256 Loss_G: 4.3206\n",
            "[23/25][334/391] Loss_D1: 0.3862 Loss_D2: 0.3854 Loss_G: 6.5049\n",
            "[23/25][335/391] Loss_D1: 0.4634 Loss_D2: 0.4283 Loss_G: 5.0593\n",
            "[23/25][336/391] Loss_D1: 0.2953 Loss_D2: 0.3997 Loss_G: 5.7088\n",
            "[23/25][337/391] Loss_D1: 0.1999 Loss_D2: 0.2120 Loss_G: 6.3471\n",
            "[23/25][338/391] Loss_D1: 0.3214 Loss_D2: 0.3323 Loss_G: 5.6662\n",
            "[23/25][339/391] Loss_D1: 0.7077 Loss_D2: 0.2789 Loss_G: 3.5991\n",
            "[23/25][340/391] Loss_D1: 0.7763 Loss_D2: 0.4416 Loss_G: 6.8351\n",
            "[23/25][341/391] Loss_D1: 0.3445 Loss_D2: 0.3112 Loss_G: 6.6287\n",
            "[23/25][342/391] Loss_D1: 0.5007 Loss_D2: 0.5728 Loss_G: 3.0746\n",
            "[23/25][343/391] Loss_D1: 0.5734 Loss_D2: 0.6030 Loss_G: 8.3214\n",
            "[23/25][344/391] Loss_D1: 0.7176 Loss_D2: 0.3057 Loss_G: 5.1058\n",
            "[23/25][345/391] Loss_D1: 0.2933 Loss_D2: 0.3396 Loss_G: 4.4900\n",
            "[23/25][346/391] Loss_D1: 0.4273 Loss_D2: 0.4029 Loss_G: 7.1064\n",
            "[23/25][347/391] Loss_D1: 0.3795 Loss_D2: 0.4385 Loss_G: 5.3650\n",
            "[23/25][348/391] Loss_D1: 0.4356 Loss_D2: 0.5393 Loss_G: 4.4247\n",
            "[23/25][349/391] Loss_D1: 0.4862 Loss_D2: 0.4238 Loss_G: 6.0871\n",
            "[23/25][350/391] Loss_D1: 0.3303 Loss_D2: 0.2996 Loss_G: 5.6971\n",
            "[23/25][351/391] Loss_D1: 0.4234 Loss_D2: 0.3416 Loss_G: 4.6798\n",
            "[23/25][352/391] Loss_D1: 0.5182 Loss_D2: 0.3955 Loss_G: 7.4768\n",
            "[23/25][353/391] Loss_D1: 0.5285 Loss_D2: 0.3451 Loss_G: 4.3846\n",
            "[23/25][354/391] Loss_D1: 0.5108 Loss_D2: 0.3859 Loss_G: 5.0638\n",
            "[23/25][355/391] Loss_D1: 0.4556 Loss_D2: 0.4975 Loss_G: 6.6190\n",
            "[23/25][356/391] Loss_D1: 0.5596 Loss_D2: 0.5638 Loss_G: 3.6722\n",
            "[23/25][357/391] Loss_D1: 0.7040 Loss_D2: 0.3820 Loss_G: 7.3429\n",
            "[23/25][358/391] Loss_D1: 0.5834 Loss_D2: 0.3466 Loss_G: 6.0815\n",
            "[23/25][359/391] Loss_D1: 0.4239 Loss_D2: 0.4602 Loss_G: 4.8035\n",
            "[23/25][360/391] Loss_D1: 0.3205 Loss_D2: 0.4089 Loss_G: 6.0072\n",
            "[23/25][361/391] Loss_D1: 0.2540 Loss_D2: 0.4401 Loss_G: 6.5227\n",
            "[23/25][362/391] Loss_D1: 0.5316 Loss_D2: 0.3291 Loss_G: 3.9662\n",
            "[23/25][363/391] Loss_D1: 0.3964 Loss_D2: 0.3455 Loss_G: 6.2510\n",
            "[23/25][364/391] Loss_D1: 0.3565 Loss_D2: 0.4884 Loss_G: 5.4401\n",
            "[23/25][365/391] Loss_D1: 0.2890 Loss_D2: 0.4430 Loss_G: 5.3629\n",
            "[23/25][366/391] Loss_D1: 0.4983 Loss_D2: 0.3031 Loss_G: 5.5513\n",
            "[23/25][367/391] Loss_D1: 0.4883 Loss_D2: 0.2392 Loss_G: 4.0171\n",
            "[23/25][368/391] Loss_D1: 0.6523 Loss_D2: 0.4406 Loss_G: 6.5492\n",
            "[23/25][369/391] Loss_D1: 0.7374 Loss_D2: 0.4991 Loss_G: 4.4165\n",
            "[23/25][370/391] Loss_D1: 0.7445 Loss_D2: 0.2587 Loss_G: 7.5019\n",
            "[23/25][371/391] Loss_D1: 0.6443 Loss_D2: 0.3729 Loss_G: 3.2522\n",
            "[23/25][372/391] Loss_D1: 0.5065 Loss_D2: 0.4108 Loss_G: 7.3871\n",
            "[23/25][373/391] Loss_D1: 0.4558 Loss_D2: 0.3131 Loss_G: 5.2209\n",
            "[23/25][374/391] Loss_D1: 0.4379 Loss_D2: 0.2416 Loss_G: 5.4262\n",
            "[23/25][375/391] Loss_D1: 0.4719 Loss_D2: 0.4129 Loss_G: 5.5910\n",
            "[23/25][376/391] Loss_D1: 0.5332 Loss_D2: 0.4317 Loss_G: 5.8759\n",
            "[23/25][377/391] Loss_D1: 0.4152 Loss_D2: 0.4193 Loss_G: 4.8177\n",
            "[23/25][378/391] Loss_D1: 0.2859 Loss_D2: 0.2581 Loss_G: 6.4888\n",
            "[23/25][379/391] Loss_D1: 0.3872 Loss_D2: 0.3913 Loss_G: 5.1938\n",
            "[23/25][380/391] Loss_D1: 0.4495 Loss_D2: 0.3346 Loss_G: 5.1939\n",
            "[23/25][381/391] Loss_D1: 0.2327 Loss_D2: 0.4201 Loss_G: 6.5561\n",
            "[23/25][382/391] Loss_D1: 0.3941 Loss_D2: 0.3860 Loss_G: 4.5759\n",
            "[23/25][383/391] Loss_D1: 0.4069 Loss_D2: 0.4478 Loss_G: 5.7093\n",
            "[23/25][384/391] Loss_D1: 0.3443 Loss_D2: 0.3791 Loss_G: 5.6002\n",
            "[23/25][385/391] Loss_D1: 0.3096 Loss_D2: 0.3098 Loss_G: 5.9526\n",
            "[23/25][386/391] Loss_D1: 0.3025 Loss_D2: 0.3197 Loss_G: 5.6265\n",
            "[23/25][387/391] Loss_D1: 0.5111 Loss_D2: 0.4205 Loss_G: 4.8964\n",
            "[23/25][388/391] Loss_D1: 0.4491 Loss_D2: 0.5220 Loss_G: 5.8129\n",
            "[23/25][389/391] Loss_D1: 0.4055 Loss_D2: 0.4282 Loss_G: 4.2217\n",
            "[23/25][390/391] Loss_D1: 0.3848 Loss_D2: 0.5247 Loss_G: 8.5658\n",
            "[24/25][0/391] Loss_D1: 0.3828 Loss_D2: 0.9250 Loss_G: 4.1912\n",
            "saving the output\n",
            "[24/25][1/391] Loss_D1: 0.3023 Loss_D2: 1.2604 Loss_G: 8.7766\n",
            "[24/25][2/391] Loss_D1: 0.2826 Loss_D2: 0.7846 Loss_G: 4.1547\n",
            "[24/25][3/391] Loss_D1: 0.3742 Loss_D2: 0.4276 Loss_G: 6.3533\n",
            "[24/25][4/391] Loss_D1: 0.3990 Loss_D2: 0.3780 Loss_G: 5.4695\n",
            "[24/25][5/391] Loss_D1: 0.2413 Loss_D2: 0.3445 Loss_G: 6.3883\n",
            "[24/25][6/391] Loss_D1: 0.2658 Loss_D2: 0.2961 Loss_G: 6.1672\n",
            "[24/25][7/391] Loss_D1: 0.7910 Loss_D2: 0.4452 Loss_G: 2.7369\n",
            "[24/25][8/391] Loss_D1: 1.0469 Loss_D2: 0.4098 Loss_G: 8.4389\n",
            "[24/25][9/391] Loss_D1: 0.4698 Loss_D2: 0.5242 Loss_G: 4.3326\n",
            "[24/25][10/391] Loss_D1: 0.7013 Loss_D2: 0.3815 Loss_G: 7.9994\n",
            "[24/25][11/391] Loss_D1: 1.2049 Loss_D2: 0.5305 Loss_G: 4.5555\n",
            "[24/25][12/391] Loss_D1: 1.5559 Loss_D2: 0.5875 Loss_G: 8.0318\n",
            "[24/25][13/391] Loss_D1: 2.0000 Loss_D2: 0.5282 Loss_G: 3.4385\n",
            "[24/25][14/391] Loss_D1: 1.4744 Loss_D2: 0.4519 Loss_G: 8.1699\n",
            "[24/25][15/391] Loss_D1: 0.6528 Loss_D2: 0.5114 Loss_G: 5.2156\n",
            "[24/25][16/391] Loss_D1: 0.4562 Loss_D2: 0.4241 Loss_G: 4.3468\n",
            "[24/25][17/391] Loss_D1: 0.5570 Loss_D2: 0.3481 Loss_G: 7.4310\n",
            "[24/25][18/391] Loss_D1: 0.5420 Loss_D2: 0.5281 Loss_G: 3.9242\n",
            "[24/25][19/391] Loss_D1: 0.5284 Loss_D2: 0.5623 Loss_G: 6.6931\n",
            "[24/25][20/391] Loss_D1: 0.6379 Loss_D2: 0.3816 Loss_G: 4.7727\n",
            "[24/25][21/391] Loss_D1: 0.4658 Loss_D2: 0.3889 Loss_G: 5.9741\n",
            "[24/25][22/391] Loss_D1: 0.4537 Loss_D2: 0.3906 Loss_G: 5.7718\n",
            "[24/25][23/391] Loss_D1: 0.3711 Loss_D2: 0.3500 Loss_G: 5.7759\n",
            "[24/25][24/391] Loss_D1: 0.3468 Loss_D2: 0.3441 Loss_G: 6.5931\n",
            "[24/25][25/391] Loss_D1: 0.3662 Loss_D2: 0.4380 Loss_G: 4.8135\n",
            "[24/25][26/391] Loss_D1: 0.5027 Loss_D2: 0.4726 Loss_G: 6.0407\n",
            "[24/25][27/391] Loss_D1: 0.6001 Loss_D2: 0.3214 Loss_G: 6.0808\n",
            "[24/25][28/391] Loss_D1: 0.8142 Loss_D2: 0.5855 Loss_G: 4.1986\n",
            "[24/25][29/391] Loss_D1: 0.8101 Loss_D2: 0.3955 Loss_G: 6.6324\n",
            "[24/25][30/391] Loss_D1: 0.3074 Loss_D2: 0.3169 Loss_G: 7.2469\n",
            "[24/25][31/391] Loss_D1: 0.2970 Loss_D2: 0.4646 Loss_G: 4.0640\n",
            "[24/25][32/391] Loss_D1: 0.3365 Loss_D2: 0.5145 Loss_G: 6.5726\n",
            "[24/25][33/391] Loss_D1: 0.3894 Loss_D2: 0.3907 Loss_G: 5.5637\n",
            "[24/25][34/391] Loss_D1: 0.3048 Loss_D2: 0.3713 Loss_G: 4.6535\n",
            "[24/25][35/391] Loss_D1: 0.3852 Loss_D2: 0.6348 Loss_G: 6.5997\n",
            "[24/25][36/391] Loss_D1: 0.3542 Loss_D2: 0.3067 Loss_G: 5.5592\n",
            "[24/25][37/391] Loss_D1: 0.5100 Loss_D2: 0.4935 Loss_G: 4.1116\n",
            "[24/25][38/391] Loss_D1: 0.7392 Loss_D2: 0.3847 Loss_G: 8.7883\n",
            "[24/25][39/391] Loss_D1: 0.6908 Loss_D2: 0.5633 Loss_G: 3.1244\n",
            "[24/25][40/391] Loss_D1: 0.4252 Loss_D2: 0.6810 Loss_G: 8.0129\n",
            "[24/25][41/391] Loss_D1: 0.4696 Loss_D2: 0.8921 Loss_G: 2.8436\n",
            "[24/25][42/391] Loss_D1: 0.5244 Loss_D2: 1.1834 Loss_G: 9.3380\n",
            "[24/25][43/391] Loss_D1: 0.5668 Loss_D2: 0.8404 Loss_G: 1.6629\n",
            "[24/25][44/391] Loss_D1: 0.4679 Loss_D2: 1.5382 Loss_G: 10.3020\n",
            "[24/25][45/391] Loss_D1: 0.2437 Loss_D2: 1.4628 Loss_G: 4.3389\n",
            "[24/25][46/391] Loss_D1: 0.4208 Loss_D2: 1.1090 Loss_G: 6.4933\n",
            "[24/25][47/391] Loss_D1: 0.5901 Loss_D2: 0.9730 Loss_G: 4.9089\n",
            "[24/25][48/391] Loss_D1: 0.3327 Loss_D2: 0.6993 Loss_G: 5.6443\n",
            "[24/25][49/391] Loss_D1: 0.4225 Loss_D2: 0.5015 Loss_G: 5.7453\n",
            "[24/25][50/391] Loss_D1: 0.3169 Loss_D2: 0.5539 Loss_G: 4.9725\n",
            "[24/25][51/391] Loss_D1: 0.2991 Loss_D2: 0.8426 Loss_G: 4.2725\n",
            "[24/25][52/391] Loss_D1: 0.5031 Loss_D2: 0.7316 Loss_G: 5.9851\n",
            "[24/25][53/391] Loss_D1: 0.5059 Loss_D2: 0.4719 Loss_G: 5.6653\n",
            "[24/25][54/391] Loss_D1: 0.4247 Loss_D2: 0.5819 Loss_G: 4.6789\n",
            "[24/25][55/391] Loss_D1: 0.3457 Loss_D2: 0.6429 Loss_G: 5.5612\n",
            "[24/25][56/391] Loss_D1: 0.2681 Loss_D2: 0.5755 Loss_G: 7.0144\n",
            "[24/25][57/391] Loss_D1: 0.3536 Loss_D2: 0.5416 Loss_G: 4.6285\n",
            "[24/25][58/391] Loss_D1: 0.4190 Loss_D2: 0.4682 Loss_G: 4.6670\n",
            "[24/25][59/391] Loss_D1: 0.6081 Loss_D2: 0.3316 Loss_G: 7.5525\n",
            "[24/25][60/391] Loss_D1: 0.7685 Loss_D2: 0.3707 Loss_G: 3.0173\n",
            "[24/25][61/391] Loss_D1: 0.8417 Loss_D2: 0.5912 Loss_G: 10.6119\n",
            "[24/25][62/391] Loss_D1: 0.3505 Loss_D2: 0.8222 Loss_G: 4.4375\n",
            "[24/25][63/391] Loss_D1: 0.2987 Loss_D2: 1.2027 Loss_G: 8.0709\n",
            "[24/25][64/391] Loss_D1: 0.4551 Loss_D2: 0.6214 Loss_G: 3.6210\n",
            "[24/25][65/391] Loss_D1: 0.4201 Loss_D2: 0.7341 Loss_G: 6.0210\n",
            "[24/25][66/391] Loss_D1: 0.6905 Loss_D2: 0.8057 Loss_G: 6.2776\n",
            "[24/25][67/391] Loss_D1: 0.3413 Loss_D2: 0.7594 Loss_G: 7.5576\n",
            "[24/25][68/391] Loss_D1: 0.6986 Loss_D2: 0.7129 Loss_G: 2.7863\n",
            "[24/25][69/391] Loss_D1: 0.7683 Loss_D2: 0.5438 Loss_G: 8.2166\n",
            "[24/25][70/391] Loss_D1: 0.8692 Loss_D2: 0.5752 Loss_G: 4.0684\n",
            "[24/25][71/391] Loss_D1: 1.5108 Loss_D2: 0.7215 Loss_G: 8.7698\n",
            "[24/25][72/391] Loss_D1: 1.7395 Loss_D2: 0.5500 Loss_G: 2.6517\n",
            "[24/25][73/391] Loss_D1: 1.7259 Loss_D2: 0.6438 Loss_G: 10.5592\n",
            "[24/25][74/391] Loss_D1: 2.2068 Loss_D2: 0.7697 Loss_G: 1.6442\n",
            "[24/25][75/391] Loss_D1: 1.7477 Loss_D2: 0.5229 Loss_G: 12.8251\n",
            "[24/25][76/391] Loss_D1: 3.4512 Loss_D2: 0.6093 Loss_G: 1.7052\n",
            "[24/25][77/391] Loss_D1: 3.0206 Loss_D2: 0.7553 Loss_G: 10.4786\n",
            "[24/25][78/391] Loss_D1: 2.0178 Loss_D2: 0.7179 Loss_G: 1.6262\n",
            "[24/25][79/391] Loss_D1: 4.0652 Loss_D2: 0.5668 Loss_G: 10.2980\n",
            "[24/25][80/391] Loss_D1: 1.5106 Loss_D2: 0.9268 Loss_G: 2.6623\n",
            "[24/25][81/391] Loss_D1: 0.9648 Loss_D2: 0.8281 Loss_G: 7.3089\n",
            "[24/25][82/391] Loss_D1: 0.6568 Loss_D2: 0.4555 Loss_G: 5.2139\n",
            "[24/25][83/391] Loss_D1: 0.7341 Loss_D2: 0.5634 Loss_G: 3.9052\n",
            "[24/25][84/391] Loss_D1: 0.8511 Loss_D2: 0.6544 Loss_G: 7.5310\n",
            "[24/25][85/391] Loss_D1: 1.2864 Loss_D2: 0.6828 Loss_G: 2.1983\n",
            "[24/25][86/391] Loss_D1: 1.4600 Loss_D2: 0.6428 Loss_G: 9.6882\n",
            "[24/25][87/391] Loss_D1: 1.5986 Loss_D2: 0.5050 Loss_G: 3.9620\n",
            "[24/25][88/391] Loss_D1: 0.9012 Loss_D2: 0.5705 Loss_G: 4.5562\n",
            "[24/25][89/391] Loss_D1: 0.5392 Loss_D2: 1.1762 Loss_G: 8.6088\n",
            "[24/25][90/391] Loss_D1: 0.8843 Loss_D2: 0.8177 Loss_G: 1.5550\n",
            "[24/25][91/391] Loss_D1: 1.7271 Loss_D2: 1.0971 Loss_G: 10.8171\n",
            "[24/25][92/391] Loss_D1: 0.9508 Loss_D2: 1.1995 Loss_G: 2.2665\n",
            "[24/25][93/391] Loss_D1: 1.2868 Loss_D2: 0.7754 Loss_G: 9.1890\n",
            "[24/25][94/391] Loss_D1: 1.2327 Loss_D2: 0.4322 Loss_G: 4.4884\n",
            "[24/25][95/391] Loss_D1: 0.7017 Loss_D2: 0.4780 Loss_G: 4.4694\n",
            "[24/25][96/391] Loss_D1: 0.8145 Loss_D2: 0.6750 Loss_G: 6.4616\n",
            "[24/25][97/391] Loss_D1: 0.5054 Loss_D2: 0.5187 Loss_G: 4.5933\n",
            "[24/25][98/391] Loss_D1: 0.5929 Loss_D2: 0.5454 Loss_G: 5.2253\n",
            "[24/25][99/391] Loss_D1: 0.5593 Loss_D2: 0.5798 Loss_G: 5.3730\n",
            "[24/25][100/391] Loss_D1: 0.6592 Loss_D2: 0.7191 Loss_G: 4.4176\n",
            "saving the output\n",
            "[24/25][101/391] Loss_D1: 0.7160 Loss_D2: 0.6921 Loss_G: 6.1824\n",
            "[24/25][102/391] Loss_D1: 0.5728 Loss_D2: 0.6457 Loss_G: 5.9960\n",
            "[24/25][103/391] Loss_D1: 0.7198 Loss_D2: 0.6110 Loss_G: 3.6951\n",
            "[24/25][104/391] Loss_D1: 0.4697 Loss_D2: 0.6344 Loss_G: 7.4853\n",
            "[24/25][105/391] Loss_D1: 0.4062 Loss_D2: 0.4359 Loss_G: 5.2598\n",
            "[24/25][106/391] Loss_D1: 0.4956 Loss_D2: 0.5845 Loss_G: 5.4999\n",
            "[24/25][107/391] Loss_D1: 0.4637 Loss_D2: 0.5993 Loss_G: 5.0152\n",
            "[24/25][108/391] Loss_D1: 0.2910 Loss_D2: 0.3759 Loss_G: 6.1390\n",
            "[24/25][109/391] Loss_D1: 0.4171 Loss_D2: 0.3878 Loss_G: 6.1174\n",
            "[24/25][110/391] Loss_D1: 0.4814 Loss_D2: 0.5963 Loss_G: 4.4725\n",
            "[24/25][111/391] Loss_D1: 0.5164 Loss_D2: 0.4114 Loss_G: 5.5727\n",
            "[24/25][112/391] Loss_D1: 0.2722 Loss_D2: 0.4791 Loss_G: 7.4796\n",
            "[24/25][113/391] Loss_D1: 0.6835 Loss_D2: 0.4762 Loss_G: 3.6288\n",
            "[24/25][114/391] Loss_D1: 0.5289 Loss_D2: 0.3685 Loss_G: 6.1149\n",
            "[24/25][115/391] Loss_D1: 0.4787 Loss_D2: 0.4086 Loss_G: 4.9366\n",
            "[24/25][116/391] Loss_D1: 0.3661 Loss_D2: 0.3777 Loss_G: 6.7911\n",
            "[24/25][117/391] Loss_D1: 0.3004 Loss_D2: 0.4729 Loss_G: 5.1949\n",
            "[24/25][118/391] Loss_D1: 0.3333 Loss_D2: 0.2851 Loss_G: 5.7409\n",
            "[24/25][119/391] Loss_D1: 0.4086 Loss_D2: 0.5728 Loss_G: 6.1487\n",
            "[24/25][120/391] Loss_D1: 0.4835 Loss_D2: 0.4361 Loss_G: 5.0781\n",
            "[24/25][121/391] Loss_D1: 0.5245 Loss_D2: 0.5266 Loss_G: 6.0556\n",
            "[24/25][122/391] Loss_D1: 0.4942 Loss_D2: 0.6094 Loss_G: 3.0292\n",
            "[24/25][123/391] Loss_D1: 0.4791 Loss_D2: 0.7873 Loss_G: 8.3856\n",
            "[24/25][124/391] Loss_D1: 0.4328 Loss_D2: 0.8574 Loss_G: 3.7505\n",
            "[24/25][125/391] Loss_D1: 0.4991 Loss_D2: 0.5609 Loss_G: 6.5223\n",
            "[24/25][126/391] Loss_D1: 0.4856 Loss_D2: 0.3968 Loss_G: 4.8911\n",
            "[24/25][127/391] Loss_D1: 0.3317 Loss_D2: 0.4695 Loss_G: 4.8231\n",
            "[24/25][128/391] Loss_D1: 0.3804 Loss_D2: 0.3774 Loss_G: 5.4861\n",
            "[24/25][129/391] Loss_D1: 0.2909 Loss_D2: 0.3387 Loss_G: 6.4789\n",
            "[24/25][130/391] Loss_D1: 0.3810 Loss_D2: 0.5025 Loss_G: 4.6579\n",
            "[24/25][131/391] Loss_D1: 0.3829 Loss_D2: 0.4459 Loss_G: 4.5007\n",
            "[24/25][132/391] Loss_D1: 0.4992 Loss_D2: 0.5583 Loss_G: 5.1732\n",
            "[24/25][133/391] Loss_D1: 0.3299 Loss_D2: 0.4851 Loss_G: 6.0210\n",
            "[24/25][134/391] Loss_D1: 0.3883 Loss_D2: 0.3351 Loss_G: 5.0679\n",
            "[24/25][135/391] Loss_D1: 0.4653 Loss_D2: 0.4236 Loss_G: 4.8329\n",
            "[24/25][136/391] Loss_D1: 0.3602 Loss_D2: 0.5618 Loss_G: 4.2408\n",
            "[24/25][137/391] Loss_D1: 0.4882 Loss_D2: 0.5347 Loss_G: 6.7998\n",
            "[24/25][138/391] Loss_D1: 0.5197 Loss_D2: 0.4678 Loss_G: 4.9918\n",
            "[24/25][139/391] Loss_D1: 0.3587 Loss_D2: 0.4350 Loss_G: 5.1382\n",
            "[24/25][140/391] Loss_D1: 0.3274 Loss_D2: 0.3895 Loss_G: 5.6246\n",
            "[24/25][141/391] Loss_D1: 0.2544 Loss_D2: 0.4044 Loss_G: 5.3264\n",
            "[24/25][142/391] Loss_D1: 0.3692 Loss_D2: 0.4946 Loss_G: 5.6798\n",
            "[24/25][143/391] Loss_D1: 0.5366 Loss_D2: 0.5335 Loss_G: 4.4740\n",
            "[24/25][144/391] Loss_D1: 0.5659 Loss_D2: 0.5755 Loss_G: 7.4653\n",
            "[24/25][145/391] Loss_D1: 0.5342 Loss_D2: 0.3541 Loss_G: 5.9101\n",
            "[24/25][146/391] Loss_D1: 0.6073 Loss_D2: 0.4121 Loss_G: 3.1316\n",
            "[24/25][147/391] Loss_D1: 1.1079 Loss_D2: 0.3739 Loss_G: 7.3129\n",
            "[24/25][148/391] Loss_D1: 0.4417 Loss_D2: 0.3890 Loss_G: 5.4085\n",
            "[24/25][149/391] Loss_D1: 0.5161 Loss_D2: 0.4244 Loss_G: 2.9623\n",
            "[24/25][150/391] Loss_D1: 1.1977 Loss_D2: 0.4589 Loss_G: 9.3807\n",
            "[24/25][151/391] Loss_D1: 1.2577 Loss_D2: 0.3487 Loss_G: 3.9402\n",
            "[24/25][152/391] Loss_D1: 0.8583 Loss_D2: 0.5607 Loss_G: 4.4635\n",
            "[24/25][153/391] Loss_D1: 0.4383 Loss_D2: 0.5685 Loss_G: 6.9372\n",
            "[24/25][154/391] Loss_D1: 0.7058 Loss_D2: 0.3341 Loss_G: 4.6032\n",
            "[24/25][155/391] Loss_D1: 0.5885 Loss_D2: 0.4330 Loss_G: 4.7050\n",
            "[24/25][156/391] Loss_D1: 0.4003 Loss_D2: 0.5253 Loss_G: 6.9928\n",
            "[24/25][157/391] Loss_D1: 0.5139 Loss_D2: 0.3412 Loss_G: 4.6654\n",
            "[24/25][158/391] Loss_D1: 0.5761 Loss_D2: 0.3325 Loss_G: 6.5426\n",
            "[24/25][159/391] Loss_D1: 0.4692 Loss_D2: 0.4659 Loss_G: 4.8048\n",
            "[24/25][160/391] Loss_D1: 0.3604 Loss_D2: 0.4589 Loss_G: 5.4674\n",
            "[24/25][161/391] Loss_D1: 0.4813 Loss_D2: 0.5082 Loss_G: 4.3967\n",
            "[24/25][162/391] Loss_D1: 0.4630 Loss_D2: 0.3055 Loss_G: 6.5434\n",
            "[24/25][163/391] Loss_D1: 0.2796 Loss_D2: 0.3540 Loss_G: 5.9036\n",
            "[24/25][164/391] Loss_D1: 0.4087 Loss_D2: 0.3555 Loss_G: 4.5789\n",
            "[24/25][165/391] Loss_D1: 0.5834 Loss_D2: 0.4455 Loss_G: 5.4841\n",
            "[24/25][166/391] Loss_D1: 0.2780 Loss_D2: 0.4307 Loss_G: 5.9948\n",
            "[24/25][167/391] Loss_D1: 0.3020 Loss_D2: 0.4503 Loss_G: 5.1057\n",
            "[24/25][168/391] Loss_D1: 0.2236 Loss_D2: 0.3947 Loss_G: 6.7215\n",
            "[24/25][169/391] Loss_D1: 0.3094 Loss_D2: 0.6707 Loss_G: 4.3924\n",
            "[24/25][170/391] Loss_D1: 0.2657 Loss_D2: 0.5543 Loss_G: 7.3738\n",
            "[24/25][171/391] Loss_D1: 0.3708 Loss_D2: 0.4080 Loss_G: 4.4816\n",
            "[24/25][172/391] Loss_D1: 0.4798 Loss_D2: 0.5046 Loss_G: 5.0202\n",
            "[24/25][173/391] Loss_D1: 0.3033 Loss_D2: 0.4292 Loss_G: 6.3898\n",
            "[24/25][174/391] Loss_D1: 0.3746 Loss_D2: 0.4863 Loss_G: 3.7794\n",
            "[24/25][175/391] Loss_D1: 0.5247 Loss_D2: 0.3787 Loss_G: 6.8114\n",
            "[24/25][176/391] Loss_D1: 0.2654 Loss_D2: 0.3511 Loss_G: 6.8249\n",
            "[24/25][177/391] Loss_D1: 0.5530 Loss_D2: 0.5095 Loss_G: 3.8623\n",
            "[24/25][178/391] Loss_D1: 0.3095 Loss_D2: 0.4733 Loss_G: 7.3723\n",
            "[24/25][179/391] Loss_D1: 0.2499 Loss_D2: 0.3847 Loss_G: 6.2348\n",
            "[24/25][180/391] Loss_D1: 0.3602 Loss_D2: 0.3744 Loss_G: 4.5802\n",
            "[24/25][181/391] Loss_D1: 0.3684 Loss_D2: 0.2656 Loss_G: 6.8532\n",
            "[24/25][182/391] Loss_D1: 0.4514 Loss_D2: 0.3715 Loss_G: 4.9729\n",
            "[24/25][183/391] Loss_D1: 0.6129 Loss_D2: 0.4440 Loss_G: 4.7725\n",
            "[24/25][184/391] Loss_D1: 0.4717 Loss_D2: 0.4404 Loss_G: 5.8926\n",
            "[24/25][185/391] Loss_D1: 0.3567 Loss_D2: 0.3613 Loss_G: 6.4675\n",
            "[24/25][186/391] Loss_D1: 0.3805 Loss_D2: 0.3779 Loss_G: 4.2919\n",
            "[24/25][187/391] Loss_D1: 0.3959 Loss_D2: 0.4416 Loss_G: 6.3156\n",
            "[24/25][188/391] Loss_D1: 0.3357 Loss_D2: 0.4801 Loss_G: 5.4181\n",
            "[24/25][189/391] Loss_D1: 0.3668 Loss_D2: 0.4681 Loss_G: 5.2661\n",
            "[24/25][190/391] Loss_D1: 0.3327 Loss_D2: 0.3208 Loss_G: 5.5952\n",
            "[24/25][191/391] Loss_D1: 0.3151 Loss_D2: 0.3261 Loss_G: 5.0987\n",
            "[24/25][192/391] Loss_D1: 0.3334 Loss_D2: 0.5790 Loss_G: 5.8997\n",
            "[24/25][193/391] Loss_D1: 0.2722 Loss_D2: 0.3877 Loss_G: 5.6380\n",
            "[24/25][194/391] Loss_D1: 0.2773 Loss_D2: 0.3685 Loss_G: 5.9668\n",
            "[24/25][195/391] Loss_D1: 0.3419 Loss_D2: 0.2984 Loss_G: 5.7378\n",
            "[24/25][196/391] Loss_D1: 0.2230 Loss_D2: 0.3220 Loss_G: 5.3008\n",
            "[24/25][197/391] Loss_D1: 0.3593 Loss_D2: 0.2814 Loss_G: 5.9238\n",
            "[24/25][198/391] Loss_D1: 0.3801 Loss_D2: 0.4329 Loss_G: 5.2669\n",
            "[24/25][199/391] Loss_D1: 0.2806 Loss_D2: 0.2910 Loss_G: 5.6729\n",
            "[24/25][200/391] Loss_D1: 0.3986 Loss_D2: 0.2973 Loss_G: 6.2228\n",
            "saving the output\n",
            "[24/25][201/391] Loss_D1: 0.6195 Loss_D2: 0.4300 Loss_G: 4.5980\n",
            "[24/25][202/391] Loss_D1: 0.2627 Loss_D2: 0.3212 Loss_G: 5.4411\n",
            "[24/25][203/391] Loss_D1: 0.3663 Loss_D2: 0.3911 Loss_G: 5.3266\n",
            "[24/25][204/391] Loss_D1: 0.3852 Loss_D2: 0.4240 Loss_G: 5.2091\n",
            "[24/25][205/391] Loss_D1: 0.3042 Loss_D2: 0.2928 Loss_G: 5.7358\n",
            "[24/25][206/391] Loss_D1: 0.3292 Loss_D2: 0.4027 Loss_G: 6.1253\n",
            "[24/25][207/391] Loss_D1: 0.3450 Loss_D2: 0.3711 Loss_G: 4.9527\n",
            "[24/25][208/391] Loss_D1: 0.2290 Loss_D2: 0.3866 Loss_G: 5.5086\n",
            "[24/25][209/391] Loss_D1: 0.2757 Loss_D2: 0.3816 Loss_G: 6.3874\n",
            "[24/25][210/391] Loss_D1: 0.2151 Loss_D2: 0.4384 Loss_G: 5.6212\n",
            "[24/25][211/391] Loss_D1: 0.3508 Loss_D2: 0.3613 Loss_G: 5.0731\n",
            "[24/25][212/391] Loss_D1: 0.3767 Loss_D2: 0.4705 Loss_G: 5.1841\n",
            "[24/25][213/391] Loss_D1: 0.4600 Loss_D2: 0.3827 Loss_G: 4.4357\n",
            "[24/25][214/391] Loss_D1: 0.5175 Loss_D2: 0.3549 Loss_G: 6.1641\n",
            "[24/25][215/391] Loss_D1: 0.4214 Loss_D2: 0.3701 Loss_G: 5.6951\n",
            "[24/25][216/391] Loss_D1: 0.2949 Loss_D2: 0.2600 Loss_G: 5.3716\n",
            "[24/25][217/391] Loss_D1: 0.1548 Loss_D2: 0.3907 Loss_G: 5.1782\n",
            "[24/25][218/391] Loss_D1: 0.5103 Loss_D2: 0.3781 Loss_G: 6.6160\n",
            "[24/25][219/391] Loss_D1: 0.3334 Loss_D2: 0.2646 Loss_G: 5.6791\n",
            "[24/25][220/391] Loss_D1: 0.3976 Loss_D2: 0.3375 Loss_G: 3.8412\n",
            "[24/25][221/391] Loss_D1: 0.5282 Loss_D2: 0.3409 Loss_G: 6.8842\n",
            "[24/25][222/391] Loss_D1: 0.3335 Loss_D2: 0.2663 Loss_G: 6.1425\n",
            "[24/25][223/391] Loss_D1: 0.3235 Loss_D2: 0.3302 Loss_G: 4.6068\n",
            "[24/25][224/391] Loss_D1: 0.3319 Loss_D2: 0.2889 Loss_G: 6.6087\n",
            "[24/25][225/391] Loss_D1: 0.2842 Loss_D2: 0.2304 Loss_G: 6.9105\n",
            "[24/25][226/391] Loss_D1: 0.4015 Loss_D2: 0.4832 Loss_G: 4.2008\n",
            "[24/25][227/391] Loss_D1: 0.2916 Loss_D2: 0.4183 Loss_G: 5.4863\n",
            "[24/25][228/391] Loss_D1: 0.2765 Loss_D2: 0.3506 Loss_G: 6.7451\n",
            "[24/25][229/391] Loss_D1: 0.4511 Loss_D2: 0.3103 Loss_G: 4.5838\n",
            "[24/25][230/391] Loss_D1: 0.5088 Loss_D2: 0.4209 Loss_G: 5.5311\n",
            "[24/25][231/391] Loss_D1: 0.4717 Loss_D2: 0.4350 Loss_G: 5.7580\n",
            "[24/25][232/391] Loss_D1: 0.4057 Loss_D2: 0.6191 Loss_G: 3.3802\n",
            "[24/25][233/391] Loss_D1: 0.4423 Loss_D2: 0.4859 Loss_G: 7.8640\n",
            "[24/25][234/391] Loss_D1: 0.4419 Loss_D2: 0.3494 Loss_G: 5.2223\n",
            "[24/25][235/391] Loss_D1: 0.4893 Loss_D2: 0.3024 Loss_G: 4.0819\n",
            "[24/25][236/391] Loss_D1: 0.5562 Loss_D2: 0.3959 Loss_G: 7.0956\n",
            "[24/25][237/391] Loss_D1: 0.3417 Loss_D2: 0.3221 Loss_G: 5.8286\n",
            "[24/25][238/391] Loss_D1: 0.2374 Loss_D2: 0.2729 Loss_G: 5.6192\n",
            "[24/25][239/391] Loss_D1: 0.3685 Loss_D2: 0.3835 Loss_G: 6.0557\n",
            "[24/25][240/391] Loss_D1: 0.2426 Loss_D2: 0.4605 Loss_G: 5.1496\n",
            "[24/25][241/391] Loss_D1: 0.2281 Loss_D2: 0.4054 Loss_G: 5.5207\n",
            "[24/25][242/391] Loss_D1: 0.3116 Loss_D2: 0.4124 Loss_G: 4.6293\n",
            "[24/25][243/391] Loss_D1: 0.3740 Loss_D2: 0.3871 Loss_G: 6.1202\n",
            "[24/25][244/391] Loss_D1: 0.2398 Loss_D2: 0.3368 Loss_G: 5.9536\n",
            "[24/25][245/391] Loss_D1: 0.3206 Loss_D2: 0.3630 Loss_G: 6.3064\n",
            "[24/25][246/391] Loss_D1: 0.2591 Loss_D2: 0.3756 Loss_G: 5.0066\n",
            "[24/25][247/391] Loss_D1: 0.3916 Loss_D2: 0.3269 Loss_G: 4.5237\n",
            "[24/25][248/391] Loss_D1: 0.4124 Loss_D2: 0.2742 Loss_G: 6.4356\n",
            "[24/25][249/391] Loss_D1: 0.2804 Loss_D2: 0.4356 Loss_G: 5.0969\n",
            "[24/25][250/391] Loss_D1: 0.3591 Loss_D2: 0.5049 Loss_G: 5.9918\n",
            "[24/25][251/391] Loss_D1: 0.2188 Loss_D2: 0.5333 Loss_G: 4.6061\n",
            "[24/25][252/391] Loss_D1: 0.2680 Loss_D2: 0.5363 Loss_G: 8.0594\n",
            "[24/25][253/391] Loss_D1: 0.3168 Loss_D2: 0.3521 Loss_G: 5.4952\n",
            "[24/25][254/391] Loss_D1: 0.2786 Loss_D2: 0.3833 Loss_G: 5.1522\n",
            "[24/25][255/391] Loss_D1: 0.3372 Loss_D2: 0.3598 Loss_G: 5.4199\n",
            "[24/25][256/391] Loss_D1: 0.3909 Loss_D2: 0.4710 Loss_G: 5.6335\n",
            "[24/25][257/391] Loss_D1: 0.4593 Loss_D2: 0.4317 Loss_G: 5.3536\n",
            "[24/25][258/391] Loss_D1: 0.3205 Loss_D2: 0.4080 Loss_G: 5.4712\n",
            "[24/25][259/391] Loss_D1: 0.2679 Loss_D2: 0.4286 Loss_G: 6.3371\n",
            "[24/25][260/391] Loss_D1: 0.5772 Loss_D2: 0.2867 Loss_G: 4.1377\n",
            "[24/25][261/391] Loss_D1: 0.5205 Loss_D2: 0.4231 Loss_G: 5.2344\n",
            "[24/25][262/391] Loss_D1: 0.2698 Loss_D2: 0.5107 Loss_G: 8.5693\n",
            "[24/25][263/391] Loss_D1: 0.6941 Loss_D2: 0.6159 Loss_G: 1.6246\n",
            "[24/25][264/391] Loss_D1: 1.0433 Loss_D2: 0.9012 Loss_G: 10.1706\n",
            "[24/25][265/391] Loss_D1: 0.5195 Loss_D2: 0.8520 Loss_G: 2.3816\n",
            "[24/25][266/391] Loss_D1: 0.6994 Loss_D2: 1.3031 Loss_G: 10.3575\n",
            "[24/25][267/391] Loss_D1: 0.6978 Loss_D2: 1.5988 Loss_G: 1.5761\n",
            "[24/25][268/391] Loss_D1: 0.6997 Loss_D2: 3.4200 Loss_G: 10.9182\n",
            "[24/25][269/391] Loss_D1: 0.6359 Loss_D2: 2.4159 Loss_G: 1.4790\n",
            "[24/25][270/391] Loss_D1: 0.9387 Loss_D2: 3.2966 Loss_G: 14.1108\n",
            "[24/25][271/391] Loss_D1: 1.0957 Loss_D2: 4.2241 Loss_G: 1.7071\n",
            "[24/25][272/391] Loss_D1: 0.8588 Loss_D2: 4.4572 Loss_G: 11.6939\n",
            "[24/25][273/391] Loss_D1: 0.4837 Loss_D2: 3.1887 Loss_G: 2.7367\n",
            "[24/25][274/391] Loss_D1: 0.4002 Loss_D2: 1.9370 Loss_G: 7.2347\n",
            "[24/25][275/391] Loss_D1: 0.5037 Loss_D2: 1.7985 Loss_G: 2.7516\n",
            "[24/25][276/391] Loss_D1: 0.8436 Loss_D2: 1.2211 Loss_G: 8.4496\n",
            "[24/25][277/391] Loss_D1: 1.2698 Loss_D2: 1.3267 Loss_G: 2.1203\n",
            "[24/25][278/391] Loss_D1: 1.2088 Loss_D2: 1.5909 Loss_G: 8.6965\n",
            "[24/25][279/391] Loss_D1: 0.7805 Loss_D2: 1.4414 Loss_G: 2.8885\n",
            "[24/25][280/391] Loss_D1: 0.5415 Loss_D2: 1.0447 Loss_G: 7.8751\n",
            "[24/25][281/391] Loss_D1: 0.3582 Loss_D2: 0.7947 Loss_G: 5.4919\n",
            "[24/25][282/391] Loss_D1: 0.4589 Loss_D2: 0.5921 Loss_G: 4.6788\n",
            "[24/25][283/391] Loss_D1: 0.8718 Loss_D2: 0.7975 Loss_G: 6.3303\n",
            "[24/25][284/391] Loss_D1: 0.8588 Loss_D2: 0.5584 Loss_G: 3.6811\n",
            "[24/25][285/391] Loss_D1: 0.8200 Loss_D2: 0.5200 Loss_G: 7.7046\n",
            "[24/25][286/391] Loss_D1: 0.7983 Loss_D2: 0.6602 Loss_G: 3.6725\n",
            "[24/25][287/391] Loss_D1: 0.8679 Loss_D2: 0.6437 Loss_G: 7.5284\n",
            "[24/25][288/391] Loss_D1: 0.7868 Loss_D2: 0.5679 Loss_G: 3.4547\n",
            "[24/25][289/391] Loss_D1: 0.6308 Loss_D2: 0.7527 Loss_G: 7.7678\n",
            "[24/25][290/391] Loss_D1: 0.4785 Loss_D2: 0.7695 Loss_G: 3.8912\n",
            "[24/25][291/391] Loss_D1: 0.4447 Loss_D2: 1.3200 Loss_G: 7.5395\n",
            "[24/25][292/391] Loss_D1: 0.6629 Loss_D2: 1.5906 Loss_G: 4.0602\n",
            "[24/25][293/391] Loss_D1: 0.6124 Loss_D2: 1.5599 Loss_G: 6.9516\n",
            "[24/25][294/391] Loss_D1: 0.4461 Loss_D2: 0.8730 Loss_G: 5.1672\n",
            "[24/25][295/391] Loss_D1: 0.4123 Loss_D2: 0.6219 Loss_G: 4.8423\n",
            "[24/25][296/391] Loss_D1: 0.4328 Loss_D2: 0.5953 Loss_G: 6.4281\n",
            "[24/25][297/391] Loss_D1: 0.3544 Loss_D2: 0.6528 Loss_G: 5.1698\n",
            "[24/25][298/391] Loss_D1: 0.4035 Loss_D2: 0.6694 Loss_G: 5.3777\n",
            "[24/25][299/391] Loss_D1: 0.4648 Loss_D2: 0.6031 Loss_G: 4.7567\n",
            "[24/25][300/391] Loss_D1: 0.4314 Loss_D2: 0.5099 Loss_G: 6.0954\n",
            "saving the output\n",
            "[24/25][301/391] Loss_D1: 0.3868 Loss_D2: 0.7132 Loss_G: 4.5622\n",
            "[24/25][302/391] Loss_D1: 0.5507 Loss_D2: 0.5820 Loss_G: 5.9932\n",
            "[24/25][303/391] Loss_D1: 0.4732 Loss_D2: 0.4997 Loss_G: 4.7380\n",
            "[24/25][304/391] Loss_D1: 0.3850 Loss_D2: 0.6452 Loss_G: 5.5507\n",
            "[24/25][305/391] Loss_D1: 0.1842 Loss_D2: 0.3841 Loss_G: 7.0317\n",
            "[24/25][306/391] Loss_D1: 0.3814 Loss_D2: 0.5575 Loss_G: 4.8138\n",
            "[24/25][307/391] Loss_D1: 0.5126 Loss_D2: 0.6505 Loss_G: 5.3277\n",
            "[24/25][308/391] Loss_D1: 0.5930 Loss_D2: 0.4156 Loss_G: 6.7175\n",
            "[24/25][309/391] Loss_D1: 0.3757 Loss_D2: 0.2802 Loss_G: 5.4339\n",
            "[24/25][310/391] Loss_D1: 0.3227 Loss_D2: 0.4412 Loss_G: 5.7107\n",
            "[24/25][311/391] Loss_D1: 0.3871 Loss_D2: 0.5881 Loss_G: 5.3841\n",
            "[24/25][312/391] Loss_D1: 0.4921 Loss_D2: 0.4538 Loss_G: 5.4279\n",
            "[24/25][313/391] Loss_D1: 0.4993 Loss_D2: 0.5170 Loss_G: 5.6004\n",
            "[24/25][314/391] Loss_D1: 0.4259 Loss_D2: 0.7478 Loss_G: 5.5238\n",
            "[24/25][315/391] Loss_D1: 0.3849 Loss_D2: 0.5965 Loss_G: 4.6999\n",
            "[24/25][316/391] Loss_D1: 0.3290 Loss_D2: 0.5524 Loss_G: 5.8275\n",
            "[24/25][317/391] Loss_D1: 0.3232 Loss_D2: 0.4955 Loss_G: 5.8622\n",
            "[24/25][318/391] Loss_D1: 0.2455 Loss_D2: 0.3560 Loss_G: 6.2256\n",
            "[24/25][319/391] Loss_D1: 0.4225 Loss_D2: 0.6020 Loss_G: 4.4629\n",
            "[24/25][320/391] Loss_D1: 0.2714 Loss_D2: 0.3781 Loss_G: 6.3435\n",
            "[24/25][321/391] Loss_D1: 0.3451 Loss_D2: 0.4458 Loss_G: 4.9158\n",
            "[24/25][322/391] Loss_D1: 0.4101 Loss_D2: 0.5519 Loss_G: 6.8387\n",
            "[24/25][323/391] Loss_D1: 0.4575 Loss_D2: 0.4689 Loss_G: 4.5623\n",
            "[24/25][324/391] Loss_D1: 0.4168 Loss_D2: 0.3897 Loss_G: 4.4101\n",
            "[24/25][325/391] Loss_D1: 0.5152 Loss_D2: 0.5674 Loss_G: 7.5891\n",
            "[24/25][326/391] Loss_D1: 0.3662 Loss_D2: 0.4540 Loss_G: 5.3736\n",
            "[24/25][327/391] Loss_D1: 0.3823 Loss_D2: 0.5156 Loss_G: 4.1359\n",
            "[24/25][328/391] Loss_D1: 0.7521 Loss_D2: 0.3730 Loss_G: 7.4892\n",
            "[24/25][329/391] Loss_D1: 0.5460 Loss_D2: 0.4564 Loss_G: 4.3726\n",
            "[24/25][330/391] Loss_D1: 0.3900 Loss_D2: 0.5277 Loss_G: 5.2277\n",
            "[24/25][331/391] Loss_D1: 0.3681 Loss_D2: 0.3766 Loss_G: 6.0064\n",
            "[24/25][332/391] Loss_D1: 0.6348 Loss_D2: 0.4567 Loss_G: 3.8330\n",
            "[24/25][333/391] Loss_D1: 0.4137 Loss_D2: 0.4307 Loss_G: 7.1799\n",
            "[24/25][334/391] Loss_D1: 0.2754 Loss_D2: 0.4617 Loss_G: 5.6689\n",
            "[24/25][335/391] Loss_D1: 0.4358 Loss_D2: 0.6393 Loss_G: 3.0672\n",
            "[24/25][336/391] Loss_D1: 0.5409 Loss_D2: 0.6810 Loss_G: 8.2417\n",
            "[24/25][337/391] Loss_D1: 0.5749 Loss_D2: 0.4186 Loss_G: 4.7018\n",
            "[24/25][338/391] Loss_D1: 0.4237 Loss_D2: 0.3593 Loss_G: 5.2329\n",
            "[24/25][339/391] Loss_D1: 0.3285 Loss_D2: 0.4785 Loss_G: 6.0602\n",
            "[24/25][340/391] Loss_D1: 0.2317 Loss_D2: 0.3878 Loss_G: 5.5765\n",
            "[24/25][341/391] Loss_D1: 0.2798 Loss_D2: 0.2516 Loss_G: 5.1931\n",
            "[24/25][342/391] Loss_D1: 0.3348 Loss_D2: 0.4485 Loss_G: 6.3168\n",
            "[24/25][343/391] Loss_D1: 0.2797 Loss_D2: 0.4189 Loss_G: 5.2807\n",
            "[24/25][344/391] Loss_D1: 0.3424 Loss_D2: 0.4457 Loss_G: 4.7278\n",
            "[24/25][345/391] Loss_D1: 0.3396 Loss_D2: 0.4026 Loss_G: 6.9370\n",
            "[24/25][346/391] Loss_D1: 0.3631 Loss_D2: 0.4043 Loss_G: 5.5220\n",
            "[24/25][347/391] Loss_D1: 0.3853 Loss_D2: 0.3322 Loss_G: 4.8251\n",
            "[24/25][348/391] Loss_D1: 0.2462 Loss_D2: 0.3708 Loss_G: 6.4536\n",
            "[24/25][349/391] Loss_D1: 0.3700 Loss_D2: 0.3541 Loss_G: 5.5672\n",
            "[24/25][350/391] Loss_D1: 0.2573 Loss_D2: 0.4439 Loss_G: 4.8678\n",
            "[24/25][351/391] Loss_D1: 0.3568 Loss_D2: 0.4973 Loss_G: 5.8915\n",
            "[24/25][352/391] Loss_D1: 0.2483 Loss_D2: 0.5020 Loss_G: 7.1836\n",
            "[24/25][353/391] Loss_D1: 0.3403 Loss_D2: 0.4555 Loss_G: 4.9367\n",
            "[24/25][354/391] Loss_D1: 0.4009 Loss_D2: 0.4580 Loss_G: 4.9177\n",
            "[24/25][355/391] Loss_D1: 0.2798 Loss_D2: 0.5184 Loss_G: 7.4849\n",
            "[24/25][356/391] Loss_D1: 0.3328 Loss_D2: 0.4262 Loss_G: 4.3156\n",
            "[24/25][357/391] Loss_D1: 0.3605 Loss_D2: 0.3993 Loss_G: 5.9708\n",
            "[24/25][358/391] Loss_D1: 0.2464 Loss_D2: 0.4153 Loss_G: 6.1178\n",
            "[24/25][359/391] Loss_D1: 0.2170 Loss_D2: 0.3356 Loss_G: 5.9507\n",
            "[24/25][360/391] Loss_D1: 0.4648 Loss_D2: 0.4302 Loss_G: 3.6829\n",
            "[24/25][361/391] Loss_D1: 0.5815 Loss_D2: 0.3752 Loss_G: 6.9409\n",
            "[24/25][362/391] Loss_D1: 0.2763 Loss_D2: 0.3778 Loss_G: 6.1523\n",
            "[24/25][363/391] Loss_D1: 0.4232 Loss_D2: 0.3306 Loss_G: 4.2853\n",
            "[24/25][364/391] Loss_D1: 0.5388 Loss_D2: 0.3087 Loss_G: 7.3268\n",
            "[24/25][365/391] Loss_D1: 0.6011 Loss_D2: 0.3955 Loss_G: 4.3372\n",
            "[24/25][366/391] Loss_D1: 0.5039 Loss_D2: 0.3790 Loss_G: 5.7057\n",
            "[24/25][367/391] Loss_D1: 0.4274 Loss_D2: 0.3104 Loss_G: 6.2193\n",
            "[24/25][368/391] Loss_D1: 0.4315 Loss_D2: 0.3670 Loss_G: 4.0533\n",
            "[24/25][369/391] Loss_D1: 0.3105 Loss_D2: 0.5562 Loss_G: 8.1335\n",
            "[24/25][370/391] Loss_D1: 0.6460 Loss_D2: 0.4630 Loss_G: 3.2908\n",
            "[24/25][371/391] Loss_D1: 0.8545 Loss_D2: 0.3449 Loss_G: 7.0247\n",
            "[24/25][372/391] Loss_D1: 0.8852 Loss_D2: 0.4672 Loss_G: 5.3475\n",
            "[24/25][373/391] Loss_D1: 0.7392 Loss_D2: 0.5255 Loss_G: 6.5235\n",
            "[24/25][374/391] Loss_D1: 0.9425 Loss_D2: 0.3035 Loss_G: 4.0080\n",
            "[24/25][375/391] Loss_D1: 0.5279 Loss_D2: 0.4744 Loss_G: 5.9759\n",
            "[24/25][376/391] Loss_D1: 0.7595 Loss_D2: 0.5736 Loss_G: 6.9612\n",
            "[24/25][377/391] Loss_D1: 1.2080 Loss_D2: 0.7715 Loss_G: 6.7077\n",
            "[24/25][378/391] Loss_D1: 0.5778 Loss_D2: 0.8118 Loss_G: 4.7265\n",
            "[24/25][379/391] Loss_D1: 0.3419 Loss_D2: 0.4227 Loss_G: 6.0530\n",
            "[24/25][380/391] Loss_D1: 0.3046 Loss_D2: 0.4055 Loss_G: 6.3640\n",
            "[24/25][381/391] Loss_D1: 0.3209 Loss_D2: 0.5129 Loss_G: 4.3635\n",
            "[24/25][382/391] Loss_D1: 0.6046 Loss_D2: 0.5182 Loss_G: 6.9680\n",
            "[24/25][383/391] Loss_D1: 0.4569 Loss_D2: 0.4365 Loss_G: 4.4021\n",
            "[24/25][384/391] Loss_D1: 0.3997 Loss_D2: 0.5486 Loss_G: 6.5641\n",
            "[24/25][385/391] Loss_D1: 0.4277 Loss_D2: 0.4081 Loss_G: 5.9506\n",
            "[24/25][386/391] Loss_D1: 0.4203 Loss_D2: 0.3634 Loss_G: 5.1671\n",
            "[24/25][387/391] Loss_D1: 0.2720 Loss_D2: 0.3338 Loss_G: 7.6605\n",
            "[24/25][388/391] Loss_D1: 0.6854 Loss_D2: 0.5344 Loss_G: 2.9226\n",
            "[24/25][389/391] Loss_D1: 0.7217 Loss_D2: 0.3337 Loss_G: 8.0742\n",
            "[24/25][390/391] Loss_D1: 0.4110 Loss_D2: 0.4132 Loss_G: 5.9169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzPXIq_bmTD3",
        "outputId": "a4ad6624-3f5c-4068-ff0f-87a450fcd198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "eval_images = []\n",
        "for i in range(600):\n",
        "    noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "    with torch.no_grad():\n",
        "        fake = netG0(noise).detach().cpu()\n",
        "    eval_images += fake\n",
        "\n",
        "print(len(eval_images))\n",
        "print(eval_images[0].shape)\n",
        "print(inception_score(eval_images, cuda=True, batch_size=32, resize=True, splits=10))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48000\n",
            "torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "./inception-score-pytorch/inception_score.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x).data.cpu().numpy()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4.7815574018604, 0.052156231289132625)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}